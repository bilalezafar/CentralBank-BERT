{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfe6f02",
   "metadata": {},
   "source": [
    "# Statistics for CBDC Sentence Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c100c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_17016\\3833511645.py:28: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[\"date\"]    = pd.to_datetime(df[\"date\"], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“’ Wrote Excel workbook â†’ results\\cbdc_descriptives.xlsx\n",
      "âœ… Saved: results\\top_authors.svg\n",
      "âœ… Descriptives complete. Tables & figures saved in: results\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Descriptive Statistics for CBDC Sentence Dataset\n",
    "# ============================================================\n",
    "# Input schema expected: url, cbdc_sentence, title, description, date,\n",
    "#                        author, affiliation, position, country\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ========== TASK 0. Imports & Paths =========================================\n",
    "import os, re, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "IN_FILE  = \"cbdc-dataset-final.csv\"\n",
    "OUT_DIR  = \"results\"                     # <-- per request\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ========== TASK 1. Load & Basic Hygiene ====================================\n",
    "df = pd.read_csv(IN_FILE)\n",
    "\n",
    "# Standardize text columns\n",
    "for col in [\"cbdc_sentence\",\"title\",\"description\",\"author\",\"affiliation\",\"position\",\"country\",\"url\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip().replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "        df[col] = df[col].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "# Dates & calendar cuts\n",
    "df[\"date\"]    = pd.to_datetime(df[\"date\"], errors=\"coerce\", infer_datetime_format=True)\n",
    "df[\"year\"]    = df[\"date\"].dt.year\n",
    "df[\"month\"]   = df[\"date\"].dt.month\n",
    "df[\"quarter\"] = df[\"date\"].dt.to_period(\"Q\").astype(str)\n",
    "\n",
    "# Sentence length features\n",
    "df[\"tokens\"] = df[\"cbdc_sentence\"].fillna(\"\").str.split().str.len()\n",
    "df[\"chars\"]  = df[\"cbdc_sentence\"].fillna(\"\").str.len()\n",
    "\n",
    "# ========== TASK 2. Overview & Missingness ==================================\n",
    "overview = pd.DataFrame({\n",
    "    \"rows (sentences)\":            [len(df)],\n",
    "    \"unique URLs (speeches)\":      [df[\"url\"].nunique()],\n",
    "    \"date coverage (minâ†’max)\":     [f\"{df['date'].min():%Y-%m-%d} â†’ {df['date'].max():%Y-%m-%d}\"],\n",
    "    \"unique authors\":              [df[\"author\"].nunique(dropna=True)],\n",
    "    \"unique affiliations\":         [df[\"affiliation\"].nunique(dropna=True)],\n",
    "    \"unique positions\":            [df[\"position\"].nunique(dropna=True)],\n",
    "    \"unique countries\":            [df[\"country\"].nunique(dropna=True)],\n",
    "    \"median tokens / sentence\":    [int(df[\"tokens\"].median())],\n",
    "    \"p90 tokens / sentence\":       [int(df[\"tokens\"].quantile(0.90))],\n",
    "}).T.reset_index().rename(columns={\"index\": \"metric\", 0: \"value\"})\n",
    "\n",
    "missing = (df[[\"title\",\"description\",\"date\",\"author\",\"affiliation\",\"position\",\"country\"]]\n",
    "           .isna().mean().mul(100).round(1)\n",
    "           .rename(\"missing_percent\").reset_index()\n",
    "           .rename(columns={\"index\":\"column\"}))\n",
    "\n",
    "# ========== TASK 3. Time Series (Year / Quarter) ============================\n",
    "by_year = (df.groupby(\"year\")\n",
    "             .agg(sentences=(\"url\",\"count\"),\n",
    "                  speeches=(\"url\",\"nunique\"))\n",
    "             .reset_index()\n",
    "             .sort_values(\"year\"))\n",
    "\n",
    "by_quarter = (df.groupby(\"quarter\")\n",
    "                .agg(sentences=(\"url\",\"count\"),\n",
    "                     speeches=(\"url\",\"nunique\"))\n",
    "                .reset_index()\n",
    "                .sort_values(\"quarter\"))\n",
    "\n",
    "# ========== TASK 4. Entity Frequencies ======================================\n",
    "def freq_table(series, k=20, dropna=True, name=None):\n",
    "    s = series if not dropna else series.dropna()\n",
    "    counts = s.value_counts()\n",
    "    out = (pd.DataFrame({\"count\": counts, \"share_%\": (counts / counts.sum() * 100).round(2)})\n",
    "           .reset_index()\n",
    "           .rename(columns={\"index\": name or series.name}))\n",
    "    return out.head(k)\n",
    "\n",
    "top_authors = (df.groupby(\"author\")\n",
    "                 .agg(sentences=(\"url\",\"count\"),\n",
    "                      speeches=(\"url\",\"nunique\"),\n",
    "                      first_year=(\"year\",\"min\"),\n",
    "                      last_year=(\"year\",\"max\"),\n",
    "                      avg_tokens=(\"tokens\",\"mean\"))\n",
    "                 .reset_index()\n",
    "                 .sort_values([\"sentences\",\"speeches\"], ascending=False)\n",
    "                 .head(25))\n",
    "top_authors[\"avg_tokens\"] = top_authors[\"avg_tokens\"].round(1)\n",
    "\n",
    "top_affiliations = (df.groupby(\"affiliation\")\n",
    "                     .agg(sentences=(\"url\",\"count\"),\n",
    "                          speeches=(\"url\",\"nunique\"),\n",
    "                          countries=(\"country\", pd.Series.nunique))\n",
    "                     .reset_index()\n",
    "                     .sort_values([\"sentences\",\"speeches\"], ascending=False)\n",
    "                     .head(25))\n",
    "\n",
    "top_positions = freq_table(df[\"position\"], k=25, name=\"position\", dropna=True)\n",
    "top_countries = freq_table(df[\"country\"], k=25, name=\"country\", dropna=True)\n",
    "\n",
    "# ========== TASK 5. Affiliation Ã— Country Crosstab (SAFE) ===================\n",
    "# Avoid KeyError from mis-using row labels as column names in sort_values\n",
    "tmp = df.copy()\n",
    "tmp[\"country\"] = tmp[\"country\"].fillna(\"Unknown\")\n",
    "ct = pd.crosstab(tmp[\"affiliation\"], tmp[\"country\"])\n",
    "\n",
    "# Sort rows by totals; sort columns by totals\n",
    "ct[\"__row_total__\"] = ct.sum(axis=1)\n",
    "ct = ct.sort_values(\"__row_total__\", ascending=False)\n",
    "col_order = ct.drop(columns=\"__row_total__\").sum(axis=0).sort_values(ascending=False).index\n",
    "ct = ct.loc[:, list(col_order) + [\"__row_total__\"]]\n",
    "\n",
    "N = 20\n",
    "aff_country_top = ct.head(N).reset_index().rename(columns={\"affiliation\":\"Affiliation\",\"__row_total__\":\"Total\"})\n",
    "\n",
    "# ========== TASK 6. Sentence-Length Distributions ===========================\n",
    "def describe_length(col):\n",
    "    return (df[col].describe(percentiles=[.1,.25,.5,.75,.9,.95,.99])\n",
    "              .round(2).to_frame(name=col).reset_index()\n",
    "              .rename(columns={\"index\":\"stat\"}))\n",
    "\n",
    "len_tokens = describe_length(\"tokens\")\n",
    "len_chars  = describe_length(\"chars\")\n",
    "\n",
    "# ========== TASK 7. N-gram Summaries (Uni/Bi/Tri) ===========================\n",
    "STOP = {\n",
    "    \"a\",\"an\",\"the\",\"and\",\"or\",\"for\",\"of\",\"to\",\"in\",\"on\",\"at\",\"with\",\"by\",\"from\",\"as\",\"that\",\"this\",\n",
    "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"it\",\"its\",\"into\",\"than\",\"then\",\"so\",\"such\",\"which\",\n",
    "    \"will\",\"would\",\"should\",\"could\",\"may\",\"might\",\"can\",\"cannot\",\"not\",\"no\",\"do\",\"does\",\"did\",\"doing\",\n",
    "    \"we\",\"our\",\"us\",\"you\",\"your\",\"they\",\"their\",\"them\",\"i\",\"he\",\"she\",\"his\",\"her\",\"him\",\"mr\",\"mrs\",\"ms\",\n",
    "    \"central\",\"bank\",\"digital\",\"currency\",\"cbdc\",\"banks\",\"monetary\",\"policy\",\"speech\",\"hong\",\"kong\",\n",
    "    \"china\",\"people\",\"banking\",\"system\",\"economy\",\"financial\"\n",
    "}\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return re.findall(r\"[A-Za-z']+\", str(s).lower())\n",
    "\n",
    "def top_ngrams(series, n=1, top=50):\n",
    "    counter = Counter()\n",
    "    for s in series.dropna():\n",
    "        toks = [t for t in tokenize(s) if t not in STOP]\n",
    "        if n == 1:\n",
    "            counter.update(toks)\n",
    "        else:\n",
    "            if len(toks) >= n:\n",
    "                grams = zip(*[toks[i:] for i in range(n)])\n",
    "                counter.update((\" \".join(g),) for g in grams)\n",
    "    dfc = (pd.DataFrame(counter.most_common(top), columns=[f\"{n}-gram\",\"count\"])\n",
    "             .assign(**{\"share_%\": lambda d: (d[\"count\"]/d[\"count\"].sum()*100).round(2)}))\n",
    "    return dfc\n",
    "\n",
    "top_unigrams = top_ngrams(df[\"cbdc_sentence\"], n=1, top=50)\n",
    "top_bigrams  = top_ngrams(df[\"cbdc_sentence\"], n=2, top=50)\n",
    "top_trigrams = top_ngrams(df[\"cbdc_sentence\"], n=3, top=50)\n",
    "\n",
    "# ========== TASK 8. Country Ã— Year Matrix ===================================\n",
    "country_year = (pd.crosstab(df[\"country\"], df[\"year\"])\n",
    "                .loc[lambda t: t.sum(axis=1).sort_values(ascending=False).index]\n",
    "                .reset_index())\n",
    "\n",
    "# ========== TASK 9. Persist Tables (CSV + Excel) ============================\n",
    "tables = {\n",
    "    \"00_overview.csv\":                 overview,\n",
    "    \"01_missingness.csv\":              missing,\n",
    "    \"02_by_year.csv\":                  by_year,\n",
    "    \"03_by_quarter.csv\":               by_quarter,\n",
    "    \"04_top_authors.csv\":              top_authors,\n",
    "    \"05_top_affiliations.csv\":         top_affiliations,\n",
    "    \"06_top_positions.csv\":            top_positions,\n",
    "    \"07_top_countries.csv\":            top_countries,\n",
    "    \"08_affiliation_x_country.csv\":    aff_country_top,\n",
    "    \"09_len_tokens.csv\":               len_tokens,\n",
    "    \"10_len_chars.csv\":                len_chars,\n",
    "    \"11_top_unigrams.csv\":             top_unigrams,\n",
    "    \"12_top_bigrams.csv\":              top_bigrams,\n",
    "    \"13_top_trigrams.csv\":             top_trigrams,\n",
    "    \"14_country_year_matrix.csv\":      country_year,\n",
    "}\n",
    "\n",
    "for fname, frame in tables.items():\n",
    "    frame.to_csv(os.path.join(OUT_DIR, fname), index=False)\n",
    "\n",
    "# Also bundle as a single Excel workbook (if openpyxl available)\n",
    "try:\n",
    "    import openpyxl\n",
    "    with pd.ExcelWriter(os.path.join(OUT_DIR, \"cbdc_descriptives.xlsx\"), engine=\"openpyxl\") as xl:\n",
    "        for sheet, frame in {\n",
    "            \"Overview\": overview,\n",
    "            \"Missingness\": missing,\n",
    "            \"ByYear\": by_year,\n",
    "            \"ByQuarter\": by_quarter,\n",
    "            \"TopAuthors\": top_authors,\n",
    "            \"TopAffiliations\": top_affiliations,\n",
    "            \"TopPositions\": top_positions,\n",
    "            \"TopCountries\": top_countries,\n",
    "            \"AffiliationsÃ—Country\": aff_country_top,\n",
    "            \"LenTokens\": len_tokens,\n",
    "            \"LenChars\": len_chars,\n",
    "            \"Unigrams\": top_unigrams,\n",
    "            \"Bigrams\": top_bigrams,\n",
    "            \"Trigrams\": top_trigrams,\n",
    "            \"CountryÃ—Year\": country_year,\n",
    "        }.items():\n",
    "            frame.to_excel(xl, sheet_name=sheet, index=False)\n",
    "    print(f\"ðŸ“’ Wrote Excel workbook â†’ {os.path.join(OUT_DIR, 'cbdc_descriptives.xlsx')}\")\n",
    "except Exception as e:\n",
    "    print(\"Excel export skipped:\", e)\n",
    "\n",
    "# ============================\n",
    "# Top Authors (horizontal bar)\n",
    "# ============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUT_DIR = \"results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- build an author list that splits multi-author rows ---\n",
    "# assumes your full dataset is already loaded in `df`\n",
    "auth_col = \"author\"\n",
    "text_id  = \"url\"   # counts based on sentences; keep as 'url' if you want speeches use nunique later\n",
    "\n",
    "# split on semicolon, strip whitespace, drop empties, and explode\n",
    "auth_series = (df[auth_col]\n",
    "               .dropna()\n",
    "               .astype(str)\n",
    "               .str.split(\";\")\n",
    "               .explode()\n",
    "               .str.strip())\n",
    "auth_series = auth_series[auth_series.ne(\"\")]\n",
    "\n",
    "# count sentences per author\n",
    "topk = 15  # change to 20/25 if you want a longer bar chart\n",
    "author_counts = (auth_series\n",
    "                 .to_frame(name=\"author\")\n",
    "                 .assign(sentences=1)\n",
    "                 .groupby(\"author\", as_index=False)[\"sentences\"].sum()\n",
    "                 .sort_values(\"sentences\", ascending=False)\n",
    "                 .head(topk))\n",
    "# --- plot (horizontal bar) ---\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "plt.barh(author_counts[\"author\"][::-1], author_counts[\"sentences\"][::-1])\n",
    "plt.xlabel(\"Sentences\")\n",
    "plt.title(\"Top Authors by CBDC Mentions\")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = os.path.join(OUT_DIR, \"top_authors.svg\")\n",
    "plt.savefig(out_path, format=\"svg\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"âœ… Saved: {out_path}\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Top Authors (horizontal bar) by speeches\n",
    "# ============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUT_DIR = \"results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- build an author list that splits multi-author rows ---\n",
    "# assumes your full dataset is already loaded in `df`\n",
    "auth_col = \"author\"\n",
    "text_id  = \"url\"   # counts based on sentences; keep as 'url' if you want speeches use nunique later\n",
    "\n",
    "# split on semicolon, strip whitespace, drop empties, and explode\n",
    "auth_series = (df[auth_col]\n",
    "               .dropna()\n",
    "               .astype(str)\n",
    "               .str.split(\";\")\n",
    "               .explode()\n",
    "               .str.strip())\n",
    "auth_series = auth_series[auth_series.ne(\"\")]\n",
    "\n",
    "#unique speeches per author\n",
    "tmp = (df.assign(author_list=df[\"author\"].fillna(\"\").astype(str).str.split(\";\"))\n",
    "          .explode(\"author_list\"))\n",
    "author_counts = (tmp.assign(author_list=tmp[\"author_list\"].str.strip())\n",
    "                      .query(\"author_list != ''\")\n",
    "                      .groupby(\"author_list\")[\"url\"].nunique()\n",
    "                      .reset_index(name=\"speeches\")\n",
    "                      .sort_values(\"speeches\", ascending=False)\n",
    "                      .head(topk)\n",
    "                 .rename(columns={\"author_list\":\"author\", \"speeches\":\"sentences\"}))\n",
    "\n",
    "# --- plot (horizontal bar) ---\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "plt.barh(author_counts[\"author\"][::-1], author_counts[\"sentences\"][::-1])\n",
    "plt.xlabel(\"Speeches\")\n",
    "plt.title(\"Top Authors by CBDC Mentions\")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = os.path.join(OUT_DIR, \"top_authors_by_speeches.svg\")\n",
    "plt.savefig(out_path, format=\"svg\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"âœ… Saved: {out_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# ========== TASK 10. Figures (SVG) ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Time series (sentences & speeches per year)\n",
    "fig = plt.figure()\n",
    "plt.plot(by_year[\"year\"], by_year[\"sentences\"], label=\"Sentences\")\n",
    "plt.plot(by_year[\"year\"], by_year[\"speeches\"],  label=\"Speeches\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Count\"); plt.title(\"CBDC Mentions Over Time\"); plt.legend()\n",
    "plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"timeseries_year.svg\")); plt.close(fig)\n",
    "\n",
    "# Top affiliations (bar, top 15)\n",
    "ta15 = top_affiliations.head(15)\n",
    "fig = plt.figure()\n",
    "plt.barh(ta15[\"affiliation\"][::-1], ta15[\"sentences\"][::-1])\n",
    "plt.xlabel(\"Sentences\"); plt.title(\"Top Affiliations by CBDC Mentions\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"top_affiliations.svg\")); plt.close(fig)\n",
    "\n",
    "# Top countries (bar, top 15)\n",
    "tc15 = top_countries.head(15)\n",
    "fig = plt.figure()\n",
    "plt.barh(tc15[\"country\"][::-1], tc15[\"count\"][::-1])\n",
    "plt.xlabel(\"Sentences\"); plt.title(\"Top Countries by CBDC Mentions\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"top_countries.svg\")); plt.close(fig)\n",
    "\n",
    "# Affiliation Ã— Country heatmap (top N rows; columns already sorted by totals)\n",
    "heat = ct.drop(columns=\"__row_total__\").head(N)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.imshow(heat.values, aspect=\"auto\")\n",
    "plt.yticks(range(heat.shape[0]), heat.index)\n",
    "plt.xticks(range(heat.shape[1]), heat.columns, rotation=45, ha=\"right\")\n",
    "plt.title(\"Affiliation Ã— Country (Top 20 affiliations by CBDC mentions)\")\n",
    "plt.colorbar(label=\"Sentences\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"affiliation_country_heatmap.svg\"))\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"âœ… Descriptives complete. Tables & figures saved in: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af7efc",
   "metadata": {},
   "source": [
    "# CBDC Sentence Classification Pipeline\n",
    "\n",
    "Applied four **domain-specialized BERT models** to classify BIS speech sentences on Central Bank Digital Currencies (CBDCs).\n",
    "The script runs in **Google Colab with GPU**, loads the input dataset, performs inference, and saves enriched results.\n",
    "\n",
    "## Models Used\n",
    "1. **CBDC-Type** â†’ `Retail CBDC`, `Wholesale CBDC`, `General/Unspecified`\n",
    "2. **CBDC-Stance** â†’ `Pro-CBDC`, `Wait-and-See`, `Anti-CBDC`\n",
    "3. **CBDC-Sentiment** â†’ `Positive`, `Neutral`, `Negative`\n",
    "4. **CBDC-Discourse** â†’ `Feature`, `Risk-Benefit`, `Process`\n",
    "\n",
    "Each model returns the **predicted label** and **confidence score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e045702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "HF Token): Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
      "Loaded 5376 sentences from: /content/drive/MyDrive/cbdc-analysis/cbdc-dataset-final.csv\n",
      "Using device: GPU\n",
      "Loading: Type -> bilalzafar/CBDC-Type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Stance -> bilalzafar/CBDC-Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Sentiment -> bilalzafar/CBDC-Sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Discourse -> bilalzafar/CBDC-Discourse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions for: Type\n",
      "Running predictions for: Stance\n",
      "Running predictions for: Sentiment\n",
      "Running predictions for: Discourse\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"print(f\\\"\\\\nSaved predictions to: {out_path}\\\")\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"cbdc_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"central bank digital currency: who can bank at the central bank?\",\n          \"and if a private-sector digital currency uses the technology to substitute for a third-party clearer, the central bank counterpart would do the opposite.\",\n          \"barrdear, j and kumhof, m (2016), \\\"the macroeconomics of central bank-issued digital currency\\\", bank of england working paper, forthcoming.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type_Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Wholesale CBDC\",\n          \"General/Unspecified\",\n          \"Retail CBDC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0501860603599214,\n        \"min\": 0.867669939994812,\n        \"max\": 0.998955249786377,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9972527623176575,\n          0.8904648423194885,\n          0.9979693293571472\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Stance_Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Wait-and-See\",\n          \"Anti-CBDC\",\n          \"Pro-CBDC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Stance_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03529067178971648,\n        \"min\": 0.902238667011261,\n        \"max\": 0.984924852848053,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.984924852848053,\n          0.902238667011261,\n          0.9796251058578491\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment_Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"neutral\",\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08391003511959477,\n        \"min\": 0.7376696467399597,\n        \"max\": 0.9889969825744629,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9889969825744629,\n          0.9107151031494141,\n          0.9795494675636292\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Discourse_Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Feature\",\n          \"Risk-Benefit\",\n          \"Process\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Discourse_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12501647359223872,\n        \"min\": 0.5994727611541748,\n        \"max\": 0.9992420673370361,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9991600513458252,\n          0.5994727611541748,\n          0.9992358684539795\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ad9286e3-cc2c-45c2-873b-0a1d758f8922\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbdc_sentence</th>\n",
       "      <th>Type_Label</th>\n",
       "      <th>Type_Score</th>\n",
       "      <th>Stance_Label</th>\n",
       "      <th>Stance_Score</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Discourse_Label</th>\n",
       "      <th>Discourse_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a \"central bank digital currency\" (cbdc) would...</td>\n",
       "      <td>Wholesale CBDC</td>\n",
       "      <td>0.996188</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.974996</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.977445</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.999242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and if a private-sector digital currency uses ...</td>\n",
       "      <td>General/Unspecified</td>\n",
       "      <td>0.890465</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.902239</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.910715</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.599473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and, as i tried to explain earlier, a cbdc tha...</td>\n",
       "      <td>Retail CBDC</td>\n",
       "      <td>0.998955</td>\n",
       "      <td>Anti-CBDC</td>\n",
       "      <td>0.948519</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.971240</td>\n",
       "      <td>Risk-Benefit</td>\n",
       "      <td>0.998474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>as far as its economic effects are concerned, ...</td>\n",
       "      <td>Retail CBDC</td>\n",
       "      <td>0.997053</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.983435</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.979650</td>\n",
       "      <td>Risk-Benefit</td>\n",
       "      <td>0.985957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as long as it's possible to hold something wit...</td>\n",
       "      <td>General/Unspecified</td>\n",
       "      <td>0.867670</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.917042</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.840400</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.998640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>barrdear, j and kumhof, m (2016), \"the macroec...</td>\n",
       "      <td>General/Unspecified</td>\n",
       "      <td>0.997969</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.979625</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.979549</td>\n",
       "      <td>Process</td>\n",
       "      <td>0.999236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ben broadbent: central banks and digital curre...</td>\n",
       "      <td>General/Unspecified</td>\n",
       "      <td>0.997384</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.975705</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.981172</td>\n",
       "      <td>Process</td>\n",
       "      <td>0.999203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>but as you widen that access, and the more clo...</td>\n",
       "      <td>Retail CBDC</td>\n",
       "      <td>0.998112</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.913568</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.987450</td>\n",
       "      <td>Risk-Benefit</td>\n",
       "      <td>0.938904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>central bank digital currency: who can bank at...</td>\n",
       "      <td>Retail CBDC</td>\n",
       "      <td>0.997253</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.984925</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.988997</td>\n",
       "      <td>Process</td>\n",
       "      <td>0.999160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>conversely, others see the distributed ledger ...</td>\n",
       "      <td>General/Unspecified</td>\n",
       "      <td>0.996235</td>\n",
       "      <td>Pro-CBDC</td>\n",
       "      <td>0.904902</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.737670</td>\n",
       "      <td>Risk-Benefit</td>\n",
       "      <td>0.994623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad9286e3-cc2c-45c2-873b-0a1d758f8922')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ad9286e3-cc2c-45c2-873b-0a1d758f8922 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ad9286e3-cc2c-45c2-873b-0a1d758f8922');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-8e75418a-9787-4740-9462-f3be2e3f0674\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8e75418a-9787-4740-9462-f3be2e3f0674')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-8e75418a-9787-4740-9462-f3be2e3f0674 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                       cbdc_sentence           Type_Label  \\\n",
       "0  a \"central bank digital currency\" (cbdc) would...       Wholesale CBDC   \n",
       "1  and if a private-sector digital currency uses ...  General/Unspecified   \n",
       "2  and, as i tried to explain earlier, a cbdc tha...          Retail CBDC   \n",
       "3  as far as its economic effects are concerned, ...          Retail CBDC   \n",
       "4  as long as it's possible to hold something wit...  General/Unspecified   \n",
       "5  barrdear, j and kumhof, m (2016), \"the macroec...  General/Unspecified   \n",
       "6  ben broadbent: central banks and digital curre...  General/Unspecified   \n",
       "7  but as you widen that access, and the more clo...          Retail CBDC   \n",
       "8  central bank digital currency: who can bank at...          Retail CBDC   \n",
       "9  conversely, others see the distributed ledger ...  General/Unspecified   \n",
       "\n",
       "   Type_Score  Stance_Label  Stance_Score Sentiment_Label  Sentiment_Score  \\\n",
       "0    0.996188  Wait-and-See      0.974996         neutral         0.977445   \n",
       "1    0.890465  Wait-and-See      0.902239         neutral         0.910715   \n",
       "2    0.998955     Anti-CBDC      0.948519        negative         0.971240   \n",
       "3    0.997053  Wait-and-See      0.983435         neutral         0.979650   \n",
       "4    0.867670  Wait-and-See      0.917042         neutral         0.840400   \n",
       "5    0.997969  Wait-and-See      0.979625         neutral         0.979549   \n",
       "6    0.997384  Wait-and-See      0.975705         neutral         0.981172   \n",
       "7    0.998112  Wait-and-See      0.913568        negative         0.987450   \n",
       "8    0.997253  Wait-and-See      0.984925         neutral         0.988997   \n",
       "9    0.996235      Pro-CBDC      0.904902        positive         0.737670   \n",
       "\n",
       "  Discourse_Label  Discourse_Score  \n",
       "0         Feature         0.999242  \n",
       "1         Feature         0.599473  \n",
       "2    Risk-Benefit         0.998474  \n",
       "3    Risk-Benefit         0.985957  \n",
       "4         Feature         0.998640  \n",
       "5         Process         0.999236  \n",
       "6         Process         0.999203  \n",
       "7    Risk-Benefit         0.938904  \n",
       "8         Process         0.999160  \n",
       "9    Risk-Benefit         0.994623  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved predictions to: /content/drive/MyDrive/cbdc-analysis/cbdc-dataset-predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Colab setup & dependencies\n",
    "# =========================\n",
    "# !pip -q install -U \"transformers>=4.41\" \"huggingface_hub>=0.23\" accelerate pandas\n",
    "\n",
    "import os, getpass, datetime as dt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline\n",
    "\n",
    "# -------------------------\n",
    "# 1) Mount Google Drive\n",
    "# -------------------------\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# -------------------------\n",
    "# 2) Paths & filenames\n",
    "# -------------------------\n",
    "DATA_DIR   = \"/content/drive/MyDrive/cbdc-analysis\"\n",
    "CSV_FILE   = \"cbdc-dataset-final.csv\"      # your file\n",
    "SENT_COL   = \"cbdc_sentence\"               # column containing sentences\n",
    "OUT_FILE   = f\"cbdc-dataset-predictions.csv\"\n",
    "\n",
    "in_path  = os.path.join(DATA_DIR, CSV_FILE)\n",
    "out_path = os.path.join(DATA_DIR, OUT_FILE)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Hugging Face login\n",
    "# -------------------------\n",
    "HF_TOKEN = getpass.getpass(\"HF Token): \")\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Load data (robust to comma or tab)\n",
    "# -------------------------\n",
    "def load_df(path):\n",
    "    try:\n",
    "        # Let pandas sniff the delimiter\n",
    "        df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "    except Exception:\n",
    "        # Common fallback: tab-separated\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "df = load_df(in_path)\n",
    "assert SENT_COL in df.columns, f\"Column '{SENT_COL}' not found. Available: {list(df.columns)}\"\n",
    "texts = df[SENT_COL].astype(str).fillna(\"\").tolist()\n",
    "print(f\"Loaded {len(texts)} sentences from: {in_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Models to run\n",
    "# -------------------------\n",
    "MODEL_IDS = {\n",
    "    \"Type\":      \"bilalzafar/CBDC-Type\",       # Retail / Wholesale / General\n",
    "    \"Stance\":    \"bilalzafar/CBDC-Stance\",     # Pro / Wait-and-See / Anti\n",
    "    \"Sentiment\": \"bilalzafar/CBDC-Sentiment\",  # negative / neutral / positive\n",
    "    \"Discourse\": \"bilalzafar/CBDC-Discourse\",  # Feature / Risk-Benefit / Process\n",
    "}\n",
    "\n",
    "# Prefer GPU if available\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Using device:\", \"GPU\" if DEVICE == 0 else \"CPU\")\n",
    "\n",
    "# Optional: half precision on GPU for speed\n",
    "model_kwargs = {\"torch_dtype\": torch.float16} if DEVICE == 0 else {}\n",
    "\n",
    "# -------------------------\n",
    "# 6) Build pipelines\n",
    "# -------------------------\n",
    "pipes = {}\n",
    "for name, repo in MODEL_IDS.items():\n",
    "    print(f\"Loading: {name} -> {repo}\")\n",
    "    pipes[name] = pipeline(\n",
    "        task=\"text-classification\",\n",
    "        model=repo,\n",
    "        device=DEVICE,\n",
    "        model_kwargs=model_kwargs\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# 7) Batched inference helper\n",
    "# -------------------------\n",
    "def predict_top(pipe, batch_texts, batch_size=64, max_length=320):\n",
    "    \"\"\"\n",
    "    Returns two lists (labels, scores) for a list of texts using a HF pipeline.\n",
    "    \"\"\"\n",
    "    labels, scores = [], []\n",
    "    for i in range(0, len(batch_texts), batch_size):\n",
    "        chunk = batch_texts[i:i+batch_size]\n",
    "        outputs = pipe(chunk, truncation=True, max_length=max_length, batch_size=batch_size)\n",
    "        # outputs is a list of dicts like {'label': 'Retail CBDC', 'score': 0.998}\n",
    "        for o in outputs:\n",
    "            labels.append(o[\"label\"])\n",
    "            scores.append(float(o[\"score\"]))\n",
    "    return labels, scores\n",
    "\n",
    "# -------------------------\n",
    "# 8) Run all four models\n",
    "# -------------------------\n",
    "results = {}\n",
    "for name, p in pipes.items():\n",
    "    print(f\"Running predictions for: {name}\")\n",
    "    lbls, scs = predict_top(p, texts, batch_size=64, max_length=320)\n",
    "    results[f\"{name}_Label\"] = lbls\n",
    "    results[f\"{name}_Score\"] = scs\n",
    "\n",
    "# -------------------------\n",
    "# 9) Merge & save\n",
    "# -------------------------\n",
    "pred_df = pd.DataFrame(results)\n",
    "out_df = pd.concat([df.reset_index(drop=True), pred_df], axis=1)\n",
    "\n",
    "# Preview a few rows\n",
    "display(out_df[[SENT_COL,\n",
    "                \"Type_Label\",\"Type_Score\",\n",
    "                \"Stance_Label\",\"Stance_Score\",\n",
    "                \"Sentiment_Label\",\"Sentiment_Score\",\n",
    "                \"Discourse_Label\",\"Discourse_Score\"]].head(10))\n",
    "\n",
    "# Save to Drive\n",
    "out_df.to_csv(out_path, index=False)\n",
    "print(f\"\\nSaved predictions to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfe510",
   "metadata": {},
   "source": [
    "# CBDC Predictions: Analysis & Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb95ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV: prediction_results\\type_label_counts.csv\n",
      "Saved CSV: prediction_results\\type_label_proportions.csv\n",
      "Saved SVG: prediction_results\\type_label_distribution_bar.svg\n",
      "Saved CSV: prediction_results\\stance_label_counts.csv\n",
      "Saved CSV: prediction_results\\stance_label_proportions.csv\n",
      "Saved SVG: prediction_results\\stance_label_distribution_bar.svg\n",
      "Saved CSV: prediction_results\\sentiment_label_counts.csv\n",
      "Saved CSV: prediction_results\\sentiment_label_proportions.csv\n",
      "Saved SVG: prediction_results\\sentiment_label_distribution_bar.svg\n",
      "Saved CSV: prediction_results\\discourse_label_counts.csv\n",
      "Saved CSV: prediction_results\\discourse_label_proportions.csv\n",
      "Saved SVG: prediction_results\\discourse_label_distribution_bar.svg\n",
      "Saved SVG: prediction_results\\type_score_hist.svg\n",
      "Saved SVG: prediction_results\\stance_score_hist.svg\n",
      "Saved SVG: prediction_results\\sentiment_score_hist.svg\n",
      "Saved SVG: prediction_results\\discourse_score_hist.svg\n",
      "Saved CSV: prediction_results\\low_confidence_type_thr_60.csv\n",
      "Saved CSV: prediction_results\\low_confidence_stance_thr_60.csv\n",
      "Saved CSV: prediction_results\\low_confidence_sentiment_thr_60.csv\n",
      "Saved CSV: prediction_results\\low_confidence_discourse_thr_60.csv\n",
      "Saved CSV: prediction_results\\low_confidence_summary.csv\n",
      "Saved CSV: prediction_results\\exemplars_top10_per_label_type.csv\n",
      "Saved CSV: prediction_results\\exemplars_top10_per_label_stance.csv\n",
      "Saved CSV: prediction_results\\exemplars_top10_per_label_sentiment.csv\n",
      "Saved CSV: prediction_results\\exemplars_top10_per_label_discourse.csv\n",
      "Saved CSV: prediction_results\\crosstab_type_by_discourse.csv\n",
      "Saved SVG: prediction_results\\heatmap_type_by_discourse.svg\n",
      "Saved CSV: prediction_results\\crosstab_stance_by_sentiment.csv\n",
      "Saved SVG: prediction_results\\heatmap_stance_by_sentiment.svg\n",
      "Saved CSV: prediction_results\\crosstab_type_by_stance.csv\n",
      "Saved SVG: prediction_results\\heatmap_type_by_stance.svg\n",
      "Saved CSV: prediction_results\\yearly_counts_by_type.csv\n",
      "Saved SVG: prediction_results\\yearly_counts_by_type_stacked.svg\n",
      "Saved CSV: prediction_results\\yearly_counts_by_discourse.csv\n",
      "Saved SVG: prediction_results\\yearly_counts_by_discourse_stacked.svg\n",
      "Saved CSV: prediction_results\\top_countries_overall.csv\n",
      "Saved SVG: prediction_results\\top_countries_overall_bar.svg\n",
      "Saved CSV: prediction_results\\country_by_type_topN.csv\n",
      "Saved SVG: prediction_results\\country_by_type_topN_stacked.svg\n",
      "Saved CSV: prediction_results\\top_authors_overall.csv\n",
      "Saved SVG: prediction_results\\top_authors_overall_bar.svg\n",
      "Saved CSV: prediction_results\\ngrams_top_by_type.csv\n",
      "Saved CSV: prediction_results\\ngrams_top_by_discourse.csv\n",
      "Saved CSV: prediction_results\\ngrams_top_by_stance.csv\n",
      "Saved CSV: prediction_results\\ngrams_top_by_sentiment.csv\n",
      "Saved CSV: prediction_results\\summary_label_counts_shares.csv\n",
      "Saved CSV: prediction_results\\sample_rows_for_appendix_head200.csv\n",
      "\n",
      "Artifacts written to: F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\n",
      "Files:\n",
      " - country_by_type_topN.csv\n",
      " - country_by_type_topN_stacked.svg\n",
      " - crosstab_stance_by_sentiment.csv\n",
      " - crosstab_type_by_discourse.csv\n",
      " - crosstab_type_by_stance.csv\n",
      " - discourse_label_counts.csv\n",
      " - discourse_label_distribution_bar.svg\n",
      " - discourse_label_proportions.csv\n",
      " - discourse_score_hist.svg\n",
      " - exemplars_top10_per_label_discourse.csv\n",
      " - exemplars_top10_per_label_sentiment.csv\n",
      " - exemplars_top10_per_label_stance.csv\n",
      " - exemplars_top10_per_label_type.csv\n",
      " - heatmap_stance_by_sentiment.svg\n",
      " - heatmap_type_by_discourse.svg\n",
      " - heatmap_type_by_stance.svg\n",
      " - low_confidence_discourse_thr_60.csv\n",
      " - low_confidence_sentiment_thr_60.csv\n",
      " - low_confidence_stance_thr_60.csv\n",
      " - low_confidence_summary.csv\n",
      " - low_confidence_type_thr_60.csv\n",
      " - ngrams_top_by_discourse.csv\n",
      " - ngrams_top_by_sentiment.csv\n",
      " - ngrams_top_by_stance.csv\n",
      " - ngrams_top_by_type.csv\n",
      " - sample_rows_for_appendix_head200.csv\n",
      " - sentiment_label_counts.csv\n",
      " - sentiment_label_distribution_bar.svg\n",
      " - sentiment_label_proportions.csv\n",
      " - sentiment_score_hist.svg\n",
      " - stance_label_counts.csv\n",
      " - stance_label_distribution_bar.svg\n",
      " - stance_label_proportions.csv\n",
      " - stance_score_hist.svg\n",
      " - summary_label_counts_shares.csv\n",
      " - top_authors_overall.csv\n",
      " - top_authors_overall_bar.svg\n",
      " - top_countries_overall.csv\n",
      " - top_countries_overall_bar.svg\n",
      " - type_label_counts.csv\n",
      " - type_label_distribution_bar.svg\n",
      " - type_label_proportions.csv\n",
      " - type_score_hist.svg\n",
      " - yearly_counts_by_discourse.csv\n",
      " - yearly_counts_by_discourse_stacked.svg\n",
      " - yearly_counts_by_type.csv\n",
      " - yearly_counts_by_type_stacked.svg\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CBDC Predictions: Analysis & Exports\n",
    "# =========================\n",
    "# Requirements: pandas, numpy, matplotlib, scikit-learn (for n-grams)\n",
    "# pip install -U pandas numpy matplotlib scikit-learn\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "INPUT_FILE   = \"cbdc-dataset-predictions.csv\" \n",
    "OUTPUT_DIR   = Path(\"prediction_results\")\n",
    "SENT_COL     = \"cbdc_sentence\"\n",
    "DATE_COL     = \"date\"\n",
    "COUNTRY_COL  = \"country\"\n",
    "AUTHOR_COL   = \"author\"\n",
    "AFFIL_COL    = \"affiliation\"\n",
    "TOP_N        = 20             # top items in bar charts / tables\n",
    "LOW_CONF_THR = 0.60           # flag low-confidence predictions\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def save_csv(df, name):\n",
    "    path = OUTPUT_DIR / f\"{name}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved CSV: {path}\")\n",
    "\n",
    "def save_fig(fig, name):\n",
    "    path = OUTPUT_DIR / f\"{name}.svg\"\n",
    "    fig.savefig(path, format=\"svg\", bbox_inches=\"tight\", dpi=150)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved SVG: {path}\")\n",
    "\n",
    "def parse_dates(series):\n",
    "    # robust parse without deprecated infer_datetime_format\n",
    "    d1 = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    mask = d1.isna()\n",
    "    if mask.any():\n",
    "        d2 = pd.to_datetime(series[mask], errors=\"coerce\", dayfirst=True)\n",
    "        d1.loc[mask] = d2\n",
    "    return d1\n",
    "\n",
    "def value_counts_df(series, normalize=False, top=None):\n",
    "    \"\"\"\n",
    "    Return value counts as a DataFrame with consistent column names.\n",
    "    \"\"\"\n",
    "    vc = series.value_counts(normalize=normalize, dropna=False)\n",
    "    if top:\n",
    "        vc = vc.head(top)\n",
    "    colname = \"count\" if not normalize else \"proportion\"\n",
    "    return vc.rename_axis(series.name or \"value\").reset_index(name=colname)\n",
    "\n",
    "def barplot_counts(series, title, fname, top=TOP_N):\n",
    "    counts = series.value_counts().head(top)\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    counts.sort_values().plot(kind=\"barh\", ax=ax)\n",
    "    ax.set_xlabel(\"Count\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_title(title)\n",
    "    for i, v in enumerate(counts.sort_values().values):\n",
    "        ax.text(v, i, f\" {v}\", va=\"center\")\n",
    "    save_fig(fig, fname)\n",
    "\n",
    "def heatmap_table(ct, title, fname):\n",
    "    fig, ax = plt.subplots(figsize=(1.1*ct.shape[1]+4, 1.1*ct.shape[0]+3))\n",
    "    im = ax.imshow(ct.values, aspect=\"auto\")\n",
    "    ax.set_xticks(range(ct.shape[1])); ax.set_xticklabels(ct.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(ct.shape[0])); ax.set_yticklabels(ct.index)\n",
    "    ax.set_title(title)\n",
    "    for i in range(ct.shape[0]):\n",
    "        for j in range(ct.shape[1]):\n",
    "            ax.text(j, i, f\"{ct.values[i, j]:,.0f}\", ha=\"center\", va=\"center\", fontsize=9, color=\"white\" if im.cmap(im.norm(ct.values[i,j]))[0:3] < (0.5,0.5,0.5) else \"black\")\n",
    "    fig.colorbar(im, ax=ax, label=\"Count\")\n",
    "    save_fig(fig, fname)\n",
    "\n",
    "def score_hist(scores, title, fname, bins=25):\n",
    "    fig, ax = plt.subplots(figsize=(7,4))\n",
    "    ax.hist(scores.dropna(), bins=bins)\n",
    "    ax.set_xlabel(\"Confidence score\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(title)\n",
    "    save_fig(fig, fname)\n",
    "\n",
    "def stacked_bar(pivot, title, fname):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    pivot.plot(kind=\"bar\", stacked=True, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(pivot.index.name or \"\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend(title=pivot.columns.name or \"\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    save_fig(fig, fname)\n",
    "\n",
    "def top_ngrams_per_label(df, text_col, label_col, ngram_range=(1,2), top_k=20):\n",
    "    # use a domain stoplist to avoid trivial tokens\n",
    "    stop_words = set(CountVectorizer(stop_words=\"english\").get_stop_words() or [])\n",
    "    stop_words |= {\"cbdc\",\"cbdcs\",\"central\",\"bank\",\"banks\",\"digital\",\"currency\",\"currencies\",\n",
    "                   \"centralbank\",\"central-bank\",\"centralbanks\",\"euro\",\"dollar\",\"banking\"}\n",
    "    rows = []\n",
    "    for label, d in df.groupby(label_col):\n",
    "        corpus = d[text_col].dropna().astype(str).tolist()\n",
    "        if not corpus:\n",
    "            continue\n",
    "        vec = CountVectorizer(stop_words=list(stop_words), ngram_range=ngram_range, min_df=2)\n",
    "        X = vec.fit_transform(corpus)\n",
    "        sums = np.asarray(X.sum(axis=0)).ravel()\n",
    "        terms = np.array(vec.get_feature_names_out())\n",
    "        order = np.argsort(-sums)[:top_k]\n",
    "        for t, c in zip(terms[order], sums[order]):\n",
    "            rows.append({label_col: label, \"ngram\": t, \"count\": int(c)})\n",
    "    out = pd.DataFrame(rows).sort_values([label_col, \"count\"], ascending=[True, False])\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Load data\n",
    "# -------------------------\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "required_cols = [\n",
    "    SENT_COL,\n",
    "    \"Type_Label\",\"Type_Score\",\n",
    "    \"Stance_Label\",\"Stance_Score\",\n",
    "    \"Sentiment_Label\",\"Sentiment_Score\",\n",
    "    \"Discourse_Label\",\"Discourse_Score\"\n",
    "]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "# Optional metadata\n",
    "for col in [DATE_COL, COUNTRY_COL, AUTHOR_COL, AFFIL_COL]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Parse dates & add time features\n",
    "dates = parse_dates(df[DATE_COL])\n",
    "df[\"year\"]   = dates.dt.year\n",
    "df[\"month\"]  = dates.dt.month\n",
    "df[\"quarter\"] = dates.dt.quarter\n",
    "\n",
    "# Strip label whitespace just in case\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    df[f\"{p}_Label\"] = df[f\"{p}_Label\"].astype(str).str.strip()\n",
    "\n",
    "# -------------------------\n",
    "# 1) Global distributions\n",
    "# -------------------------\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    counts = value_counts_df(df[f\"{p}_Label\"])\n",
    "    props  = value_counts_df(df[f\"{p}_Label\"], normalize=True)\n",
    "    save_csv(counts, f\"{p.lower()}_label_counts\")\n",
    "    save_csv(props,  f\"{p.lower()}_label_proportions\")\n",
    "    barplot_counts(df[f\"{p}_Label\"], f\"{p} â€” label distribution\", f\"{p.lower()}_label_distribution_bar\")\n",
    "\n",
    "# Score histograms\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    score_hist(df[f\"{p}_Score\"], f\"{p} â€” score distribution\", f\"{p.lower()}_score_hist\")\n",
    "\n",
    "# Low-confidence triage (per model)\n",
    "low_conf_tables = []\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    low = df[df[f\"{p}_Score\"] < LOW_CONF_THR].copy()\n",
    "    low = low[[SENT_COL, f\"{p}_Label\", f\"{p}_Score\", \"url\", \"title\", DATE_COL, COUNTRY_COL, AUTHOR_COL]]\n",
    "    save_csv(low, f\"low_confidence_{p.lower()}_thr_{int(LOW_CONF_THR*100)}\")\n",
    "    low_conf_tables.append({\"model\": p, \"low_conf_count\": len(low), \"share\": len(low)/len(df)})\n",
    "\n",
    "save_csv(pd.DataFrame(low_conf_tables), \"low_confidence_summary\")\n",
    "\n",
    "# High-confidence exemplars per class (top 10 each)\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    exemplars = []\n",
    "    for label in df[f\"{p}_Label\"].dropna().unique():\n",
    "        block = (\n",
    "            df[df[f\"{p}_Label\"] == label]\n",
    "            .sort_values(f\"{p}_Score\", ascending=False)\n",
    "            .head(10)\n",
    "            [[SENT_COL, f\"{p}_Label\", f\"{p}_Score\", \"url\", \"title\", DATE_COL, COUNTRY_COL, AUTHOR_COL]]\n",
    "        )\n",
    "        exemplars.append(block.assign(_label_group=label))\n",
    "    if exemplars:\n",
    "        out = pd.concat(exemplars, ignore_index=True)\n",
    "        save_csv(out, f\"exemplars_top10_per_label_{p.lower()}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Cross-model consistency\n",
    "# -------------------------\n",
    "# Type vs Discourse (e.g., which discourse categories dominate Retail vs Wholesale vs General)\n",
    "ct_type_disc = pd.crosstab(df[\"Type_Label\"], df[\"Discourse_Label\"])\n",
    "save_csv(ct_type_disc.reset_index(), \"crosstab_type_by_discourse\")\n",
    "heatmap_table(ct_type_disc, \"Type Ã— Discourse (counts)\", \"heatmap_type_by_discourse\")\n",
    "\n",
    "# Stance vs Sentiment (correlation of stance and tone)\n",
    "ct_stance_sent = pd.crosstab(df[\"Stance_Label\"], df[\"Sentiment_Label\"])\n",
    "save_csv(ct_stance_sent.reset_index(), \"crosstab_stance_by_sentiment\")\n",
    "heatmap_table(ct_stance_sent, \"Stance Ã— Sentiment (counts)\", \"heatmap_stance_by_sentiment\")\n",
    "\n",
    "# Type vs Stance\n",
    "ct_type_stance = pd.crosstab(df[\"Type_Label\"], df[\"Stance_Label\"])\n",
    "save_csv(ct_type_stance.reset_index(), \"crosstab_type_by_stance\")\n",
    "heatmap_table(ct_type_stance, \"Type Ã— Stance (counts)\", \"heatmap_type_by_stance\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Time trends\n",
    "# -------------------------\n",
    "# By year: Type distribution\n",
    "by_year_type = df.pivot_table(index=\"year\", columns=\"Type_Label\", values=SENT_COL, aggfunc=\"count\").fillna(0).astype(int)\n",
    "by_year_type = by_year_type.sort_index()\n",
    "save_csv(by_year_type.reset_index(), \"yearly_counts_by_type\")\n",
    "stacked_bar(by_year_type, \"Yearly counts by Type\", \"yearly_counts_by_type_stacked\")\n",
    "\n",
    "# By year: Discourse distribution\n",
    "by_year_disc = df.pivot_table(index=\"year\", columns=\"Discourse_Label\", values=SENT_COL, aggfunc=\"count\").fillna(0).astype(int)\n",
    "by_year_disc = by_year_disc.sort_index()\n",
    "save_csv(by_year_disc.reset_index(), \"yearly_counts_by_discourse\")\n",
    "stacked_bar(by_year_disc, \"Yearly counts by Discourse\", \"yearly_counts_by_discourse_stacked\")\n",
    "\n",
    "# -------------------------\n",
    "# 4) Geography & authorship\n",
    "# -------------------------\n",
    "# Top countries overall and by Type\n",
    "top_countries = (\n",
    "    df[COUNTRY_COL].dropna().astype(str).str.strip().value_counts().head(TOP_N).rename_axis(COUNTRY_COL).reset_index(name=\"count\")\n",
    ")\n",
    "save_csv(top_countries, \"top_countries_overall\")\n",
    "barplot_counts(df[COUNTRY_COL].dropna().astype(str).str.strip(), \"Top countries (overall)\", \"top_countries_overall_bar\")\n",
    "\n",
    "type_country = (\n",
    "    df.groupby([COUNTRY_COL, \"Type_Label\"], dropna=True)[SENT_COL].count().reset_index(name=\"count\")\n",
    ")\n",
    "# keep only top countries for a readable pivot/plot\n",
    "type_country = type_country[type_country[COUNTRY_COL].isin(top_countries[COUNTRY_COL])]\n",
    "pivot_tc = type_country.pivot(index=COUNTRY_COL, columns=\"Type_Label\", values=\"count\").fillna(0).astype(int)\n",
    "save_csv(pivot_tc.reset_index(), \"country_by_type_topN\")\n",
    "stacked_bar(pivot_tc, \"Country Ã— Type (Top countries)\", \"country_by_type_topN_stacked\")\n",
    "\n",
    "# Top authors\n",
    "if AUTHOR_COL in df.columns:\n",
    "    top_authors = (\n",
    "        df[AUTHOR_COL].dropna().astype(str).str.strip().value_counts().head(TOP_N).rename_axis(AUTHOR_COL).reset_index(name=\"count\")\n",
    "    )\n",
    "    save_csv(top_authors, \"top_authors_overall\")\n",
    "    barplot_counts(df[AUTHOR_COL].dropna().astype(str).str.strip(), \"Top authors (overall)\", \"top_authors_overall_bar\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Language signals (n-grams) per label\n",
    "# -------------------------\n",
    "# Type n-grams\n",
    "type_ngrams = top_ngrams_per_label(df, SENT_COL, \"Type_Label\", ngram_range=(1,2), top_k=25)\n",
    "save_csv(type_ngrams, \"ngrams_top_by_type\")\n",
    "\n",
    "# Discourse n-grams\n",
    "disc_ngrams = top_ngrams_per_label(df, SENT_COL, \"Discourse_Label\", ngram_range=(1,2), top_k=25)\n",
    "save_csv(disc_ngrams, \"ngrams_top_by_discourse\")\n",
    "\n",
    "# Stance n-grams\n",
    "stance_ngrams = top_ngrams_per_label(df, SENT_COL, \"Stance_Label\", ngram_range=(1,2), top_k=25)\n",
    "save_csv(stance_ngrams, \"ngrams_top_by_stance\")\n",
    "\n",
    "# Sentiment n-grams\n",
    "sent_ngrams = top_ngrams_per_label(df, SENT_COL, \"Sentiment_Label\", ngram_range=(1,2), top_k=25)\n",
    "save_csv(sent_ngrams, \"ngrams_top_by_sentiment\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Quick sanity tables for paper appendix\n",
    "# -------------------------\n",
    "summary_rows = []\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    total = len(df)\n",
    "    counts = df[f\"{p}_Label\"].value_counts()\n",
    "    for label, cnt in counts.items():\n",
    "        summary_rows.append({\"model\": p, \"label\": label, \"count\": int(cnt), \"share\": cnt/total})\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values([\"model\",\"count\"], ascending=[True, False])\n",
    "save_csv(summary_df, \"summary_label_counts_shares\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) List of columns kept for reproducibility\n",
    "# -------------------------\n",
    "keep_cols = [\n",
    "    \"url\",\"title\",DATE_COL,COUNTRY_COL,AUTHOR_COL,AFFIL_COL,\n",
    "    SENT_COL,\n",
    "    \"Type_Label\",\"Type_Score\",\n",
    "    \"Stance_Label\",\"Stance_Score\",\n",
    "    \"Sentiment_Label\",\"Sentiment_Score\",\n",
    "    \"Discourse_Label\",\"Discourse_Score\",\n",
    "    \"year\",\"quarter\",\"month\"\n",
    "]\n",
    "keep_cols = [c for c in keep_cols if c in df.columns]\n",
    "meta_export = df[keep_cols]\n",
    "save_csv(meta_export.head(200), \"sample_rows_for_appendix_head200\")\n",
    "\n",
    "# Done\n",
    "print(\"\\nArtifacts written to:\", OUTPUT_DIR.resolve())\n",
    "print(\"Files:\", *sorted(os.listdir(OUTPUT_DIR)), sep=\"\\n - \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2fe66",
   "metadata": {},
   "source": [
    "# Comprehensive stats for CBDC predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84a11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comprehensive descriptive stats to:\n",
      "  F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\cbdc_descriptive_stats.csv\n",
      "Rows: 2,098\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Comprehensive stats for CBDC predictions\n",
    "# ============================================\n",
    "# Input  : cbdc-dataset-predictions.csv \n",
    "# Output : prediction_results/cbdc_descriptive_stats.csv\n",
    "# --------------------------------------------\n",
    "import os, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Optional metrics (graceful fallback if not installed)\n",
    "try:\n",
    "    from scipy.stats import chi2_contingency, spearmanr\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import mutual_info_score\n",
    "    HAVE_SKLEARN = True\n",
    "except Exception:\n",
    "    HAVE_SKLEARN = False\n",
    "\n",
    "INPUT_FILE  = \"cbdc-dataset-predictions.csv\"\n",
    "OUT_DIR     = Path(\"prediction_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CSV     = OUT_DIR / \"cbdc_descriptive_stats.csv\"\n",
    "\n",
    "# Columns\n",
    "SENT_COL    = \"cbdc_sentence\"\n",
    "DATE_COL    = \"date\"\n",
    "COUNTRY_COL = \"country\"\n",
    "AUTHOR_COL  = \"author\"\n",
    "AFFIL_COL   = \"affiliation\"\n",
    "\n",
    "MODELS = [\n",
    "    (\"Type\",      \"Type_Label\",      \"Type_Score\"),\n",
    "    (\"Stance\",    \"Stance_Label\",    \"Stance_Score\"),\n",
    "    (\"Sentiment\", \"Sentiment_Label\", \"Sentiment_Score\"),\n",
    "    (\"Discourse\", \"Discourse_Label\", \"Discourse_Score\"),\n",
    "]\n",
    "THRESHOLDS = [0.50, 0.60, 0.70, 0.80, 0.90]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def parse_dates(series):\n",
    "    d1 = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    mask = d1.isna()\n",
    "    if mask.any():\n",
    "        d2 = pd.to_datetime(series[mask], errors=\"coerce\", dayfirst=True)\n",
    "        d1.loc[mask] = d2\n",
    "    return d1\n",
    "\n",
    "def entropy(p):\n",
    "    p = np.array(p, dtype=float)\n",
    "    p = p[p > 0]\n",
    "    if p.size == 0: return np.nan\n",
    "    return float(-(p * np.log2(p)).sum())\n",
    "\n",
    "def hhi(p):\n",
    "    p = np.array(p, dtype=float)\n",
    "    return float((p**2).sum())\n",
    "\n",
    "def cramers_v(ct):\n",
    "    if not HAVE_SCIPY: return np.nan\n",
    "    chi2, _, _, _ = chi2_contingency(ct)\n",
    "    n = ct.values.sum()\n",
    "    r, k = ct.shape\n",
    "    denom = min(k-1, r-1)\n",
    "    return math.sqrt((chi2 / n) / denom) if (n > 0 and denom > 0) else np.nan\n",
    "\n",
    "def mutual_info(a, b):\n",
    "    if not HAVE_SKLEARN: return np.nan\n",
    "    return float(mutual_info_score(a, b))\n",
    "\n",
    "def add_row(rows, section, metric, value,\n",
    "            model=None, label=None,\n",
    "            subgroup_type=None, subgroup=None,\n",
    "            n=None, denom=None, notes=None):\n",
    "    rows.append({\n",
    "        \"section\": section,\n",
    "        \"metric\": metric,\n",
    "        \"value\": value,\n",
    "        \"model\": model,\n",
    "        \"label\": label,\n",
    "        \"subgroup_type\": subgroup_type,\n",
    "        \"subgroup\": subgroup,\n",
    "        \"n\": n,\n",
    "        \"denominator\": denom,\n",
    "        \"notes\": notes\n",
    "    })\n",
    "\n",
    "# ---------- load data ----------\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "for col in [SENT_COL]:\n",
    "    assert col in df.columns, f\"Missing required column: {col}\"\n",
    "for col in [DATE_COL, COUNTRY_COL, AUTHOR_COL, AFFIL_COL]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Prepare text/labels/scores\n",
    "for m, lab, sc in MODELS:\n",
    "    if lab in df.columns:\n",
    "        df[lab] = df[lab].astype(str).str.strip()\n",
    "    if sc in df.columns:\n",
    "        df[sc] = pd.to_numeric(df[sc], errors=\"coerce\")\n",
    "\n",
    "# Dates & time-features\n",
    "dates = parse_dates(df[DATE_COL])\n",
    "df[\"year\"]    = dates.dt.year\n",
    "df[\"quarter\"] = dates.dt.quarter\n",
    "df[\"month\"]   = dates.dt.month\n",
    "\n",
    "# Text lengths\n",
    "df[\"char_len\"] = df[SENT_COL].astype(str).str.len()\n",
    "df[\"word_len\"] = df[SENT_COL].astype(str).str.split().str.len()\n",
    "\n",
    "# Duplicates on sentence text\n",
    "df[\"_dup_flag\"] = df[SENT_COL].duplicated(keep=False)\n",
    "df[\"_dup_exact\"] = df[SENT_COL].duplicated(keep=\"first\")\n",
    "\n",
    "# ---------- compile stats ----------\n",
    "rows = []\n",
    "\n",
    "# Dataset-level\n",
    "N = len(df)\n",
    "add_row(rows, \"dataset\", \"rows_total\", N)\n",
    "add_row(rows, \"dataset\", \"unique_sentences\", int(df[SENT_COL].nunique(dropna=True)), n=N)\n",
    "add_row(rows, \"dataset\", \"duplicate_sentence_rows\", int(df[\"_dup_flag\"].sum()), n=N)\n",
    "add_row(rows, \"dataset\", \"unique_urls\", int(df[\"url\"].nunique(dropna=True)) if \"url\" in df.columns else np.nan)\n",
    "add_row(rows, \"dataset\", \"unique_authors\", int(df[AUTHOR_COL].nunique(dropna=True)))\n",
    "add_row(rows, \"dataset\", \"unique_countries\", int(df[COUNTRY_COL].nunique(dropna=True)))\n",
    "add_row(rows, \"dataset\", \"date_min\", str(pd.to_datetime(dates).min()))\n",
    "add_row(rows, \"dataset\", \"date_max\", str(pd.to_datetime(dates).max()))\n",
    "add_row(rows, \"text\", \"char_len_mean\", float(df[\"char_len\"].mean()), n=N)\n",
    "add_row(rows, \"text\", \"char_len_median\", float(df[\"char_len\"].median()), n=N)\n",
    "add_row(rows, \"text\", \"char_len_std\", float(df[\"char_len\"].std(ddof=1)), n=N)\n",
    "add_row(rows, \"text\", \"word_len_mean\", float(df[\"word_len\"].mean()), n=N)\n",
    "add_row(rows, \"text\", \"word_len_median\", float(df[\"word_len\"].median()), n=N)\n",
    "add_row(rows, \"text\", \"word_len_std\", float(df[\"word_len\"].std(ddof=1)), n=N)\n",
    "\n",
    "# Per-model: label distributions, score stats, uncertainty, length effects\n",
    "for m, lab, sc in MODELS:\n",
    "    if lab not in df.columns: \n",
    "        add_row(rows, \"warning\", f\"missing_{m}_columns\", 1, model=m, notes=f\"{lab} not found\")\n",
    "        continue\n",
    "\n",
    "    # label counts/shares\n",
    "    lab_counts = df[lab].value_counts(dropna=False)\n",
    "    lab_shares = lab_counts / lab_counts.sum()\n",
    "    for lbl, cnt in lab_counts.items():\n",
    "        add_row(rows, \"labels\", \"count\", int(cnt), model=m, label=str(lbl), n=N)\n",
    "        add_row(rows, \"labels\", \"share\", float(lab_shares[lbl]), model=m, label=str(lbl), n=int(cnt), denom=N)\n",
    "\n",
    "    # concentration/entropy\n",
    "    p = lab_shares.values\n",
    "    add_row(rows, \"labels\", \"entropy_bits\", entropy(p), model=m)\n",
    "    add_row(rows, \"labels\", \"hhi\", hhi(p), model=m)\n",
    "    add_row(rows, \"labels\", \"num_classes\", int(lab_counts.shape[0]), model=m)\n",
    "    add_row(rows, \"labels\", \"majority_label\", lab_counts.idxmax(), model=m)\n",
    "    add_row(rows, \"labels\", \"majority_share\", float(lab_shares.max()), model=m)\n",
    "\n",
    "    # score summary overall\n",
    "    if sc in df.columns:\n",
    "        s = df[sc].dropna()\n",
    "        if len(s):\n",
    "            q = s.quantile([0.1,0.25,0.5,0.75,0.9])\n",
    "            add_row(rows, \"scores\", \"mean\", float(s.mean()), model=m, n=len(s))\n",
    "            add_row(rows, \"scores\", \"std\", float(s.std(ddof=1)), model=m, n=len(s))\n",
    "            add_row(rows, \"scores\", \"min\", float(s.min()), model=m)\n",
    "            add_row(rows, \"scores\", \"p10\", float(q.loc[0.10]), model=m)\n",
    "            add_row(rows, \"scores\", \"p25\", float(q.loc[0.25]), model=m)\n",
    "            add_row(rows, \"scores\", \"median\", float(q.loc[0.50]), model=m)\n",
    "            add_row(rows, \"scores\", \"p75\", float(q.loc[0.75]), model=m)\n",
    "            add_row(rows, \"scores\", \"p90\", float(q.loc[0.90]), model=m)\n",
    "            add_row(rows, \"scores\", \"max\", float(s.max()), model=m)\n",
    "\n",
    "            # score by label\n",
    "            for lbl in lab_counts.index:\n",
    "                ss = df.loc[df[lab] == lbl, sc].dropna()\n",
    "                if len(ss):\n",
    "                    add_row(rows, \"scores_by_label\", \"mean\", float(ss.mean()), model=m, label=str(lbl), n=len(ss))\n",
    "                    add_row(rows, \"scores_by_label\", \"median\", float(ss.median()), model=m, label=str(lbl), n=len(ss))\n",
    "                    add_row(rows, \"scores_by_label\", \"std\", float(ss.std(ddof=1)), model=m, label=str(lbl), n=len(ss))\n",
    "\n",
    "            # uncertainty buckets\n",
    "            for thr in THRESHOLDS:\n",
    "                low = (df[sc] < thr).sum()\n",
    "                add_row(rows, \"uncertainty\", f\"low_conf_count@{thr:.2f}\", int(low), model=m, n=N)\n",
    "                add_row(rows, \"uncertainty\", f\"low_conf_share@{thr:.2f}\", float(low / N), model=m, n=low, denom=N)\n",
    "\n",
    "            # length vs score\n",
    "            if HAVE_SCIPY:\n",
    "                rho, pval = spearmanr(df[\"word_len\"], df[sc], nan_policy=\"omit\")\n",
    "                add_row(rows, \"scores_vs_length\", \"spearman_rho_wordlen\", float(rho), model=m, n=int(df[[sc,\"word_len\"]].dropna().shape[0]), notes=f\"p={pval:.3g}\")\n",
    "                rho, pval = spearmanr(df[\"char_len\"], df[sc], nan_policy=\"omit\")\n",
    "                add_row(rows, \"scores_vs_length\", \"spearman_rho_charlen\", float(rho), model=m, n=int(df[[sc,\"char_len\"]].dropna().shape[0]), notes=f\"p={pval:.3g}\")\n",
    "\n",
    "    # time: by year label counts & shares\n",
    "    for y, g in df.groupby(\"year\", dropna=True):\n",
    "        if pd.isna(y): continue\n",
    "        yy = int(y)\n",
    "        total_y = g.shape[0]\n",
    "        c = g[lab].value_counts()\n",
    "        for lbl, cnt in c.items():\n",
    "            add_row(rows, \"time_year\", \"count\", int(cnt), model=m, label=str(lbl), subgroup_type=\"year\", subgroup=yy, n=total_y)\n",
    "            add_row(rows, \"time_year\", \"share\", float(cnt/total_y), model=m, label=str(lbl), subgroup_type=\"year\", subgroup=yy, n=int(cnt), denom=total_y)\n",
    "\n",
    "    # YoY change in shares (per label)\n",
    "    shares_by_year = (\n",
    "        df.groupby(\"year\")[lab]\n",
    "          .value_counts(normalize=True)\n",
    "          .rename(\"share\").reset_index()\n",
    "          .pivot(index=\"year\", columns=lab, values=\"share\")\n",
    "          .sort_index()\n",
    "    )\n",
    "    if shares_by_year.shape[0] >= 2:\n",
    "        yoy = shares_by_year.diff().dropna()\n",
    "        for yy, row in yoy.iterrows():\n",
    "            for lbl, delta in row.dropna().items():\n",
    "                add_row(rows, \"time_year\", \"yoy_share_delta\", float(delta), model=m, label=str(lbl), subgroup_type=\"year\", subgroup=int(yy))\n",
    "\n",
    "# Geography: per-country counts and shares (overall)\n",
    "if COUNTRY_COL in df.columns:\n",
    "    total = df.shape[0]\n",
    "    cn_counts = df[COUNTRY_COL].astype(str).str.strip().value_counts(dropna=True)\n",
    "    for c, cnt in cn_counts.items():\n",
    "        add_row(rows, \"geography\", \"count_overall\", int(cnt), subgroup_type=\"country\", subgroup=str(c), n=total)\n",
    "        add_row(rows, \"geography\", \"share_overall\", float(cnt/total), subgroup_type=\"country\", subgroup=str(c), n=int(cnt), denom=total)\n",
    "\n",
    "    # country Ã— model label shares (within-country)\n",
    "    for m, lab, _ in MODELS:\n",
    "        if lab not in df.columns: continue\n",
    "        grp = df.groupby([COUNTRY_COL, lab], dropna=True)[SENT_COL].count().rename(\"count\").reset_index()\n",
    "        for (c, lbl), sub in grp.groupby([COUNTRY_COL, lab]):\n",
    "            c_total = int(df.loc[df[COUNTRY_COL]==c, SENT_COL].shape[0])\n",
    "            cnt = int(sub[\"count\"].sum())\n",
    "            add_row(rows, \"geography_by_model\", \"count\", cnt, model=m, label=str(lbl), subgroup_type=\"country\", subgroup=str(c), n=c_total)\n",
    "            add_row(rows, \"geography_by_model\", \"within_country_share\", float(cnt / c_total if c_total else np.nan),\n",
    "                    model=m, label=str(lbl), subgroup_type=\"country\", subgroup=str(c), n=cnt, denom=c_total)\n",
    "\n",
    "# Authorship: per-author counts (overall)\n",
    "if AUTHOR_COL in df.columns:\n",
    "    au_counts = df[AUTHOR_COL].astype(str).str.strip().value_counts(dropna=True)\n",
    "    total = df.shape[0]\n",
    "    for a, cnt in au_counts.items():\n",
    "        add_row(rows, \"authorship\", \"count_overall\", int(cnt), subgroup_type=\"author\", subgroup=str(a), n=total)\n",
    "        add_row(rows, \"authorship\", \"share_overall\", float(cnt/total), subgroup_type=\"author\", subgroup=str(a), n=int(cnt), denom=total)\n",
    "\n",
    "# Cross-model crosstabs + association metrics\n",
    "PAIRWISE = [\n",
    "    (\"Type\",\"Discourse\"),\n",
    "    (\"Type\",\"Stance\"),\n",
    "    (\"Stance\",\"Sentiment\"),\n",
    "    (\"Discourse\",\"Sentiment\"),\n",
    "]\n",
    "for A, B in PAIRWISE:\n",
    "    labA = next(l for m,l,s in MODELS if m==A)\n",
    "    labB = next(l for m,l,s in MODELS if m==B)\n",
    "    if labA not in df.columns or labB not in df.columns: continue\n",
    "\n",
    "    ct = pd.crosstab(df[labA], df[labB])\n",
    "    total = ct.values.sum()\n",
    "\n",
    "    # cell counts and shares\n",
    "    for i in ct.index:\n",
    "        for j in ct.columns:\n",
    "            cnt = int(ct.loc[i, j])\n",
    "            add_row(rows, \"cross_model\", \"count\", cnt, model=f\"{A}Ã—{B}\", label=f\"{i} | {j}\", n=total)\n",
    "            add_row(rows, \"cross_model\", \"total_share\", float(cnt/total if total else np.nan), model=f\"{A}Ã—{B}\", label=f\"{i} | {j}\", n=cnt, denom=total)\n",
    "\n",
    "    # association measures\n",
    "    add_row(rows, \"cross_model_assoc\", \"cramers_v\", float(cramers_v(ct)), model=f\"{A}Ã—{B}\")\n",
    "    add_row(rows, \"cross_model_assoc\", \"mutual_info\", float(mutual_info(df[labA], df[labB])), model=f\"{A}Ã—{B}\",\n",
    "            notes=\"sklearn.mutual_info_score\" if HAVE_SKLEARN else \"sklearn not available\")\n",
    "\n",
    "# Save\n",
    "stats_df = pd.DataFrame(rows)\n",
    "stats_df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved comprehensive descriptive stats to:\\n  {OUT_CSV.resolve()}\\nRows: {len(stats_df):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c23c2",
   "metadata": {},
   "source": [
    "# Comprehesive summary stat table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b96b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV table:\n",
      "  F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\summary_descriptive_table.csv\n",
      "Saved LaTeX (booktabs) table:\n",
      "  F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\summary_descriptive_table.tex\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Paper-ready comprehensive descriptive statistics (single table)\n",
    "# ============================================================\n",
    "# Input : cbdc-dataset-predictions.csv (same folder as notebook)\n",
    "# Output: prediction_results/summary_descriptive_table.csv\n",
    "#         prediction_results/summary_descriptive_table.tex\n",
    "# ------------------------------------------------------------\n",
    "import os, math, textwrap\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional: SciPy for CramÃ©r's V; scikit-learn for Mutual Information\n",
    "try:\n",
    "    from scipy.stats import chi2_contingency\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import mutual_info_score\n",
    "    HAVE_SKLEARN = True\n",
    "except Exception:\n",
    "    HAVE_SKLEARN = False\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "INPUT_FILE  = \"cbdc-dataset-predictions.csv\"\n",
    "OUT_DIR     = Path(\"prediction_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CSV_OUT     = OUT_DIR / \"summary_descriptive_table.csv\"\n",
    "TEX_OUT     = OUT_DIR / \"summary_descriptive_table.tex\"\n",
    "\n",
    "SENT_COL    = \"cbdc_sentence\"\n",
    "DATE_COL    = \"date\"\n",
    "COUNTRY_COL = \"country\"\n",
    "AUTHOR_COL  = \"author\"\n",
    "URL_COL     = \"url\"\n",
    "\n",
    "MODELS = [\n",
    "    (\"Type\",      \"Type_Label\",      \"Type_Score\"),\n",
    "    (\"Stance\",    \"Stance_Label\",    \"Stance_Score\"),\n",
    "    (\"Sentiment\", \"Sentiment_Label\", \"Sentiment_Score\"),\n",
    "    (\"Discourse\", \"Discourse_Label\", \"Discourse_Score\"),\n",
    "]\n",
    "\n",
    "LOW_CONF_THRESHOLDS = [0.60, 0.80, 0.90]   # report shares @ these cutoffs\n",
    "TOP_K_LIST = 10                             # Top-K for countries/authors (for a compact single-cell summary)\n",
    "DECIMALS   = 3                              # numeric formatting for LaTeX\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def parse_dates(series):\n",
    "    d1 = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    mask = d1.isna()\n",
    "    if mask.any():\n",
    "        d2 = pd.to_datetime(series[mask], errors=\"coerce\", dayfirst=True)\n",
    "        d1.loc[mask] = d2\n",
    "    return d1\n",
    "\n",
    "def cramers_v_from_crosstab(ct):\n",
    "    if not HAVE_SCIPY:\n",
    "        return np.nan\n",
    "    chi2, _, _, _ = chi2_contingency(ct)\n",
    "    n = ct.values.sum()\n",
    "    r, k = ct.shape\n",
    "    denom = min(k-1, r-1)\n",
    "    if n == 0 or denom <= 0:\n",
    "        return np.nan\n",
    "    return float(math.sqrt((chi2 / n) / denom))\n",
    "\n",
    "def topk_series_to_cell(s, k=10):\n",
    "    \"\"\"Return 'Name1 (n1); Name2 (n2); ...'\"\"\"\n",
    "    s = s.dropna()\n",
    "    vc = s.astype(str).str.strip().value_counts().head(k)\n",
    "    return \"; \".join([f\"{idx} ({cnt:,d})\" for idx, cnt in vc.items()])\n",
    "\n",
    "def fmt_pct(x, decimals=1):\n",
    "    return f\"{100*x:.{decimals}f}%\"\n",
    "\n",
    "def wrap(s, width=60):\n",
    "    return \"\\n\".join(textwrap.wrap(str(s), width=width)) if isinstance(s, str) else s\n",
    "\n",
    "# -------------------------\n",
    "# Load data\n",
    "# -------------------------\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Ensure required columns exist\n",
    "assert SENT_COL in df.columns, f\"Missing '{SENT_COL}'.\"\n",
    "for _, lab, sc in MODELS:\n",
    "    assert lab in df.columns, f\"Missing '{lab}'.\"\n",
    "    assert sc in df.columns,  f\"Missing '{sc}'.\"\n",
    "\n",
    "# Coerce types / clean\n",
    "for _, lab, sc in MODELS:\n",
    "    df[lab] = df[lab].astype(str).str.strip()\n",
    "    df[sc]  = pd.to_numeric(df[sc], errors=\"coerce\")\n",
    "\n",
    "for col in [COUNTRY_COL, AUTHOR_COL, URL_COL]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Dates & time features\n",
    "dates = parse_dates(df.get(DATE_COL, pd.Series(index=df.index)))\n",
    "df[\"year\"]    = dates.dt.year\n",
    "df[\"quarter\"] = dates.dt.quarter\n",
    "df[\"month\"]   = dates.dt.month\n",
    "\n",
    "# Text lengths\n",
    "df[\"word_len\"] = df[SENT_COL].astype(str).str.split().str.len()\n",
    "df[\"char_len\"] = df[SENT_COL].astype(str).str.len()\n",
    "\n",
    "# Duplicates (sentence text)\n",
    "df[\"_dup_any\"]   = df[SENT_COL].duplicated(keep=False)\n",
    "df[\"_dup_first\"] = df[SENT_COL].duplicated(keep=\"first\")\n",
    "\n",
    "# =========================\n",
    "# Build the single summary table (Panels Aâ€“F)\n",
    "# =========================\n",
    "rows = []\n",
    "N = len(df)\n",
    "\n",
    "# ---------- Panel A: Dataset overview ----------\n",
    "date_min, date_max = (pd.to_datetime(dates).min(), pd.to_datetime(dates).max())\n",
    "yrs = df[\"year\"].dropna().astype(int)\n",
    "sent_per_url_mean = df.groupby(URL_COL, dropna=False)[SENT_COL].count().mean() if URL_COL in df.columns else np.nan\n",
    "sent_per_url_median = df.groupby(URL_COL, dropna=False)[SENT_COL].count().median() if URL_COL in df.columns else np.nan\n",
    "\n",
    "panel = \"Panel A. Dataset overview\"\n",
    "rows += [\n",
    "    [panel, \"Observations (rows)\",      N,                       \"\", \"\", \"\"],\n",
    "    [panel, \"Unique sentences\",         df[SENT_COL].nunique(),  \"\", \"\", \"\"],\n",
    "    [panel, \"Duplicate sentence rows\",  int(df[\"_dup_any\"].sum()), fmt_pct(df[\"_dup_any\"].mean()), \"\", \"\"],\n",
    "    [panel, \"Unique URLs\",              df[URL_COL].nunique(),   \"\", \"\", \"\"],\n",
    "    [panel, \"Unique authors\",           df[AUTHOR_COL].nunique(), \"\", \"\", \"\"],\n",
    "    [panel, \"Unique countries\",         df[COUNTRY_COL].nunique(), \"\", \"\", \"\"],\n",
    "    [panel, \"Date range\",               f\"{date_min.date() if pd.notna(date_min) else 'NA'} â€” {date_max.date() if pd.notna(date_max) else 'NA'}\", \"\", \"\", \"\"],\n",
    "    [panel, \"Years covered (distinct)\", int(yrs.nunique()) if len(yrs) else \"\", \"\", \"\", \"\"],\n",
    "    [panel, \"Median year [IQR]\",        f\"{int(yrs.median()) if len(yrs) else 'NA'} [{int(yrs.quantile(.25)) if len(yrs) else 'NA'}â€“{int(yrs.quantile(.75)) if len(yrs) else 'NA'}]\", \"\", \"\", \"\"],\n",
    "    [panel, \"Sentences per URL (mean)\", f\"{sent_per_url_mean:,.2f}\" if pd.notna(sent_per_url_mean) else \"NA\", \"\", \"\", \"\"],\n",
    "    [panel, \"Sentences per URL (median)\", f\"{sent_per_url_median:,.0f}\" if pd.notna(sent_per_url_median) else \"NA\", \"\", \"\", \"\"],\n",
    "    [panel, \"Word length (mean / median / sd)\", f\"{df['word_len'].mean():.1f} / {df['word_len'].median():.0f} / {df['word_len'].std(ddof=1):.1f}\", \"\", \"\", \"\"],\n",
    "    [panel, \"Char length (mean / median / sd)\", f\"{df['char_len'].mean():.1f} / {df['char_len'].median():.0f} / {df['char_len'].std(ddof=1):.1f}\", \"\", \"\", \"\"],\n",
    "]\n",
    "\n",
    "# ---------- Panel B: Label distributions (counts and shares) ----------\n",
    "panel = \"Panel B. Label distributions (counts; shares)\"\n",
    "for m, lab, _ in MODELS:\n",
    "    counts = df[lab].value_counts(dropna=False)\n",
    "    shares = counts / counts.sum()\n",
    "    # entropy / concentration\n",
    "    p = shares.values\n",
    "    entropy_bits = float(-(p[p>0] * np.log2(p[p>0])).sum())\n",
    "    hhi = float((p**2).sum())\n",
    "    rows.append([panel, f\"{m}: classes (K)\", int(counts.shape[0]), \"\", \"\", \"\"])\n",
    "    rows.append([panel, f\"{m}: entropy (bits)\", round(entropy_bits, DECIMALS), \"\", \"\", \"\"])\n",
    "    rows.append([panel, f\"{m}: HHI\", round(hhi, DECIMALS), \"\", \"\", \"\"])\n",
    "    for lbl, cnt in counts.items():\n",
    "        rows.append([panel, f\"{m}: {lbl}\", int(cnt), fmt_pct(shares[lbl]), \"\", \"\"])\n",
    "\n",
    "# ---------- Panel C: Score statistics & low-confidence shares ----------\n",
    "panel = \"Panel C. Score statistics & low-confidence\"\n",
    "for m, lab, sc in MODELS:\n",
    "    s = df[sc].dropna()\n",
    "    if len(s) == 0:\n",
    "        rows.append([panel, f\"{m}: score stats (no data)\", \"\", \"\", \"\", \"\"])\n",
    "        continue\n",
    "    q = s.quantile([.10,.25,.50,.75,.90])\n",
    "    rows += [\n",
    "        [panel, f\"{m}: mean / sd\", f\"{s.mean():.{DECIMALS}f} / {s.std(ddof=1):.{DECIMALS}f}\", \"\", \"\", \"\"],\n",
    "        [panel, f\"{m}: min / p10 / median / p90 / max\",\n",
    "         f\"{s.min():.{DECIMALS}f} / {q.loc[.10]:.{DECIMALS}f} / {q.loc[.50]:.{DECIMALS}f} / {q.loc[.90]:.{DECIMALS}f} / {s.max():.{DECIMALS}f}\",\n",
    "         \"\", \"\", \"\"],\n",
    "    ]\n",
    "    # by label (mean Â± sd)\n",
    "    for lbl in df[lab].value_counts().index:\n",
    "        ss = df.loc[df[lab]==lbl, sc].dropna()\n",
    "        if len(ss):\n",
    "            rows.append([panel, f\"{m}: score by {lbl} (mean Â± sd)\", f\"{ss.mean():.{DECIMALS}f} Â± {ss.std(ddof=1):.{DECIMALS}f}\", \"\", \"\", \"\"])\n",
    "    # low-confidence shares\n",
    "    for thr in LOW_CONF_THRESHOLDS:\n",
    "        share_low = float((df[sc] < thr).mean())\n",
    "        rows.append([panel, f\"{m}: share(score < {thr:.2f})\", fmt_pct(share_low, 1), \"\", \"\", \"\"])\n",
    "\n",
    "# ---------- Panel D: Time coverage summaries ----------\n",
    "panel = \"Panel D. Time coverage\"\n",
    "if len(yrs):\n",
    "    share_2020s = float((yrs >= 2020).mean())\n",
    "    share_2010s = float(((yrs >= 2010) & (yrs <= 2019)).mean())\n",
    "    share_pre2010 = float((yrs < 2010).mean())\n",
    "    rows += [\n",
    "        [panel, \"Share of observations in 2020s\", fmt_pct(share_2020s, 1), \"\", \"\", \"\"],\n",
    "        [panel, \"Share of observations in 2010s\", fmt_pct(share_2010s, 1), \"\", \"\", \"\"],\n",
    "        [panel, \"Share of observations pre-2010\", fmt_pct(share_pre2010, 1), \"\", \"\", \"\"],\n",
    "    ]\n",
    "# most recent year & share\n",
    "if len(yrs):\n",
    "    y_max = int(yrs.max())\n",
    "    share_max_year = float((yrs == y_max).mean())\n",
    "    rows.append([panel, f\"Most recent year in data\", y_max, \"\", \"\", \"\"])\n",
    "    rows.append([panel, f\"Share of observations in {y_max}\", fmt_pct(share_max_year, 1), \"\", \"\", \"\"])\n",
    "\n",
    "# ---------- Panel E: Cross-model association metrics ----------\n",
    "panel = \"Panel E. Cross-model associations\"\n",
    "pairs = [(\"Type\",\"Discourse\"), (\"Type\",\"Stance\"), (\"Stance\",\"Sentiment\"), (\"Discourse\",\"Sentiment\")]\n",
    "for A, B in pairs:\n",
    "    labA = next(l for m,l,s in MODELS if m==A)\n",
    "    labB = next(l for m,l,s in MODELS if m==B)\n",
    "    ct = pd.crosstab(df[labA], df[labB])\n",
    "    cv = cramers_v_from_crosstab(ct)\n",
    "    rows.append([panel, f\"CramÃ©râ€™s V: {A} Ã— {B}\", round(cv, DECIMALS) if pd.notna(cv) else \"NA\", \"\", \"\", \"\"])\n",
    "    if HAVE_SKLEARN:\n",
    "        mi = mutual_info_score(df[labA], df[labB])  # nats\n",
    "        rows.append([panel, f\"Mutual information (nats): {A} Ã— {B}\", round(float(mi), DECIMALS), \"\", \"\", \"\"])\n",
    "\n",
    "# Score-score Spearman correlations (pairwise across models)\n",
    "score_cols = {m: sc for m,_,sc in MODELS}\n",
    "scores_df = df[[c for c in score_cols.values() if c in df.columns]].copy()\n",
    "if scores_df.shape[1] >= 2:\n",
    "    corr = scores_df.corr(method=\"spearman\", min_periods=1)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[1]):\n",
    "            rows.append([panel,\n",
    "                         f\"Spearman Ï (scores): {list(score_cols.keys())[i]} Ã— {list(score_cols.keys())[j]}\",\n",
    "                         round(float(corr.iloc[i,j]), DECIMALS) if pd.notna(corr.iloc[i,j]) else \"NA\",\n",
    "                         \"\", \"\", \"\"])\n",
    "\n",
    "# ---------- Panel F: Countries & Authors (Top-10, compact) ----------\n",
    "panel = \"Panel F. Key entities (Top-10 compact)\"\n",
    "rows.append([panel, \"Top-10 countries (count)\", topk_series_to_cell(df[COUNTRY_COL], TOP_K_LIST), \"\", \"\", \"\"])\n",
    "rows.append([panel, \"Top-10 authors (count)\",  topk_series_to_cell(df[AUTHOR_COL], TOP_K_LIST), \"\", \"\", \"\"])\n",
    "\n",
    "# ---------- Panel G: Missingness (key columns) ----------\n",
    "panel = \"Panel G. Missingness\"\n",
    "key_cols = [DATE_COL, COUNTRY_COL, AUTHOR_COL, URL_COL, SENT_COL] + [lab for _,lab,_ in MODELS] + [sc for *_,sc in MODELS]\n",
    "for c in key_cols:\n",
    "    if c in df.columns:\n",
    "        miss_share = float(df[c].isna().mean())\n",
    "        rows.append([panel, f\"Missing share: {c}\", fmt_pct(miss_share, 1), \"\", \"\", \"\"])\n",
    "\n",
    "# -------------------------\n",
    "# Assemble final table\n",
    "# -------------------------\n",
    "table = pd.DataFrame(rows, columns=[\"Panel / Section\", \"Statistic\", \"Value\", \"Count\", \"Share\", \"Notes\"])\n",
    "\n",
    "# For CSV (analysis-friendly): keep raw numeric where we have it; string where appropriate\n",
    "table.to_csv(CSV_OUT, index=False)\n",
    "print(f\"Saved CSV table:\\n  {CSV_OUT.resolve()}\")\n",
    "\n",
    "# For LaTeX (paper-ready): light formatting\n",
    "#  - Wrap long cells, apply thousands separators where applicable\n",
    "latex_df = table.copy()\n",
    "\n",
    "# Apply gentle wrapping to extremely long â€œValueâ€ cells (Top-10 lists)\n",
    "latex_df[\"Value\"] = latex_df[\"Value\"].apply(lambda s: wrap(s, width=80))\n",
    "\n",
    "# Build LaTeX with booktabs\n",
    "latex_str = latex_df.to_latex(\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    longtable=True,\n",
    "    bold_rows=False,\n",
    "    na_rep=\"\",\n",
    "    caption=\"Comprehensive Descriptive Statistics for CBDC Sentence Predictions.\",\n",
    "    label=\"tab:descriptive_stats\",\n",
    "    column_format=\"p{3.2cm}p{7.8cm}p{4.5cm}p{1.6cm}p{1.6cm}p{2.0cm}\"\n",
    ").replace(\"\\\\toprule\", \"\\\\toprule\\\\addlinespace[0.25em]\") \\\n",
    " .replace(\"\\\\bottomrule\", \"\\\\addlinespace[0.25em]\\\\bottomrule\")\n",
    "\n",
    "with open(TEX_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_str)\n",
    "\n",
    "print(f\"Saved LaTeX (booktabs) table:\\n  {TEX_OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42b860",
   "metadata": {},
   "source": [
    "# Additional Heatmaps: Discourse Ã— Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e21bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\heatmap_discourse_by_sentiment_counts.svg\n",
      " - F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\heatmap_discourse_by_sentiment_rowshare.svg\n",
      " - F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\heatmap_discourse_by_sentiment_colshare.svg\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Heatmaps: Discourse Ã— Sentiment (counts & shares)\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"prediction_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----- configurable order to match your other figures -----\n",
    "ORDER_DISCOURSE = [\"Feature\", \"Process\", \"Risk-Benefit\"]\n",
    "ORDER_SENTIMENT = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# ----- build crosstab with fixed ordering -----\n",
    "disc = df[\"Discourse_Label\"].astype(str).str.strip()\n",
    "sent = df[\"Sentiment_Label\"].astype(str).str.strip()\n",
    "\n",
    "ct = pd.crosstab(disc, sent).reindex(index=ORDER_DISCOURSE, columns=ORDER_SENTIMENT, fill_value=0)\n",
    "\n",
    "# ----- small helper for consistent look -----\n",
    "def plot_heatmap(mat, title, fname, fmt=\"d\", cbar_label=\"Count\"):\n",
    "    fig, ax = plt.subplots(figsize=(6.2, 5.2))  # sized to pair nicely in Word two-column layout\n",
    "    im = ax.imshow(mat.values, aspect=\"auto\")\n",
    "    ax.set_xticks(range(mat.shape[1])); ax.set_xticklabels(mat.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(mat.shape[0])); ax.set_yticklabels(mat.index)\n",
    "    ax.set_title(title)\n",
    "    # annotate\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(mat.shape[1]):\n",
    "            val = mat.iat[i, j]\n",
    "            text = f\"{val:.1f}%\" if fmt == \"%\" else f\"{int(val):,}\"\n",
    "            # choose text color for contrast\n",
    "            r, g, b, _ = im.cmap(im.norm(mat.values[i, j]))\n",
    "            lum = 0.299*r + 0.587*g + 0.114*b\n",
    "            color = \"white\" if lum < 0.5 else \"black\"\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", fontsize=9, color=color)\n",
    "    cb = fig.colorbar(im, ax=ax)\n",
    "    cb.set_label(cbar_label)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUT_DIR / f\"{fname}.svg\", format=\"svg\", bbox_inches=\"tight\", dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ----- (1) Counts heatmap -----\n",
    "plot_heatmap(ct, \"Discourse Ã— Sentiment (counts)\", \"heatmap_discourse_by_sentiment_counts\", fmt=\"d\", cbar_label=\"Count\")\n",
    "\n",
    "# ----- (2) Row-normalized shares: within each Discourse label -----\n",
    "row_share = (ct.div(ct.sum(axis=1), axis=0) * 100).round(1)\n",
    "plot_heatmap(row_share, \"Discourse Ã— Sentiment (row share, % within Discourse)\", \n",
    "             \"heatmap_discourse_by_sentiment_rowshare\", fmt=\"%\", cbar_label=\"Share (%)\")\n",
    "\n",
    "# ----- (3) Column-normalized shares: within each Sentiment level -----\n",
    "col_share = (ct.div(ct.sum(axis=0), axis=1) * 100).round(1)\n",
    "plot_heatmap(col_share, \"Discourse Ã— Sentiment (column share, % within Sentiment)\", \n",
    "             \"heatmap_discourse_by_sentiment_colshare\", fmt=\"%\", cbar_label=\"Share (%)\")\n",
    "\n",
    "print(\"Saved:\",\n",
    "      (OUT_DIR / \"heatmap_discourse_by_sentiment_counts.svg\").resolve(),\n",
    "      (OUT_DIR / \"heatmap_discourse_by_sentiment_rowshare.svg\").resolve(),\n",
    "      (OUT_DIR / \"heatmap_discourse_by_sentiment_colshare.svg\").resolve(),\n",
    "      sep=\"\\n - \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e563c98",
   "metadata": {},
   "source": [
    "# Time-series line graphs for each classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5e79533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SVGs & CSVs to: F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CBDC classification --- time-series\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# ----------------------------- FILES -----------------------------\n",
    "CSV_PATH = \"cbdc-dataset-predictions.csv\"   # <- change if needed\n",
    "OUT_DIR  = Path(\"prediction_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------- TIME WINDOW -----------------------\n",
    "X_MIN = pd.Timestamp(\"2016-01-01\")\n",
    "X_MAX = pd.Timestamp(\"2024-12-31\")\n",
    "\n",
    "# ----------------------------- FREQUENCY / SMOOTHING -------------\n",
    "FREQ = \"Q\"                           # 'Y', 'Q', or 'M'\n",
    "ROLLING_WINDOW = {\"M\": 3, \"Q\": 2, \"Y\": 1}[FREQ]\n",
    "CENTERED = False                     # keep False so smoothing never leaks past 2024\n",
    "\n",
    "# ############## DISTANCE CONTROLS â€” ADJUST HERE ##################\n",
    "LEGEND_PANEL_FRAC = 0.2             # Legend panel width (fraction of Axes width)\n",
    "LEGEND_GAP_MODE   = \"inch\"           # \"inch\" (precise) or \"frac\" (fraction of Axes)\n",
    "LEGEND_GAP_INCH   = 0.001             # Gap between plot and legend (inches)\n",
    "LEGEND_GAP_FRAC   = 0.001             # If using \"frac\", set the fractional gap here\n",
    "# #################################################################\n",
    "\n",
    "# ----------------------------- LABEL ORDERS / WRAPS --------------\n",
    "LABEL_ORDERS = {\n",
    "    \"Type_Label\":      [\"Retail CBDC\", \"Wholesale CBDC\", \"General/Unspecified\"],\n",
    "    \"Stance_Label\":    [\"Pro-CBDC\", \"Wait-and-See\", \"Anti-CBDC\"],\n",
    "    \"Sentiment_Label\": [\"negative\", \"neutral\", \"positive\"],\n",
    "    \"Discourse_Label\": [\"Feature\", \"Process\", \"Risk-Benefit\"],\n",
    "}\n",
    "TYPE_LEGEND_WRAP = {\n",
    "    \"Retail CBDC\": \"Retail\\nCBDC\",\n",
    "    \"Wholesale CBDC\": \"Wholesale\\nCBDC\",\n",
    "    \"General/Unspecified\": \"General/\\nUnspecified\",\n",
    "}\n",
    "\n",
    "# ----------------------------- LOAD DATA -------------------------\n",
    "def parse_dates(series):\n",
    "    d1 = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    m = d1.isna()\n",
    "    if m.any():\n",
    "        d2 = pd.to_datetime(series[m], errors=\"coerce\", dayfirst=True)\n",
    "        d1.loc[m] = d2\n",
    "    return d1\n",
    "\n",
    "usecols = [\"date\",\"Type_Label\",\"Stance_Label\",\"Sentiment_Label\",\"Discourse_Label\"]\n",
    "df = pd.read_csv(CSV_PATH, usecols=usecols)\n",
    "df[\"__date\"] = parse_dates(df[\"date\"]).dt.floor(\"D\")\n",
    "df = df[(df[\"__date\"] >= X_MIN) & (df[\"__date\"] <= X_MAX)].reset_index(drop=True)\n",
    "\n",
    "# ----------------------------- HELPERS ---------------------------\n",
    "def period_range_clamped(freq):\n",
    "    return pd.period_range(pd.Period(X_MIN, freq), pd.Period(X_MAX, freq), freq=freq)\n",
    "\n",
    "def make_timeseries(dfin, label_col, freq=\"Q\", order=None, w=1, centered=False):\n",
    "    d = dfin.copy()\n",
    "    d[\"_p\"] = d[\"__date\"].dt.to_period(freq)\n",
    "    ct = d.groupby([\"_p\", label_col]).size().unstack(fill_value=0).sort_index()\n",
    "    if order is not None:\n",
    "        order = [c for c in order if c in ct.columns]\n",
    "        ct = ct.reindex(columns=order)\n",
    "    idx = period_range_clamped(freq)\n",
    "    ct = ct.reindex(idx, fill_value=0)\n",
    "    shares = ct.div(ct.sum(axis=1).replace(0, np.nan), axis=0)\n",
    "    if w and w > 1:\n",
    "        ct = ct.rolling(w, center=centered, min_periods=1).mean()\n",
    "        shares = shares.rolling(w, center=centered, min_periods=1).mean()\n",
    "    return ct, shares\n",
    "\n",
    "def _gap_frac_from_inches(fig, ax, gap_inch: float) -> float:\n",
    "    \"\"\"Convert an absolute gap (inches) into a fraction of the Axes width.\"\"\"\n",
    "    fig_w_in = fig.get_size_inches()[0]\n",
    "    ax_w_frac = ax.get_position().width  # fraction of figure width\n",
    "    ax_w_in = fig_w_in * ax_w_frac\n",
    "    return gap_inch / max(ax_w_in, 1e-9)\n",
    "\n",
    "def plot_lines(df_wide, title, ylabel, fname, legend_labels=None):\n",
    "    fig, ax = plt.subplots(figsize=(7.2, 4.6))\n",
    "\n",
    "    # x-values: end-of-period timestamps (floored to day)\n",
    "    x_all = df_wide.index.asfreq(FREQ, how=\"end\").to_timestamp().floor(\"D\")\n",
    "\n",
    "    # Hard clamp, remove margins, and set explicit year ticks 2016..2024\n",
    "    ax.set_xlim(X_MIN, X_MAX)\n",
    "    ax.set_xbound(X_MIN, X_MAX)\n",
    "    ax.set_xmargin(0.0); ax.margins(x=0.0)\n",
    "    year_ticks = pd.date_range(X_MIN.normalize(), X_MAX.normalize(), freq=\"YS\")\n",
    "    ax.set_xticks(year_ticks)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "\n",
    "    # Mask after smoothing; plot only within [X_MIN, X_MAX]\n",
    "    mask = (x_all >= X_MIN) & (x_all <= X_MAX)\n",
    "    lines = []\n",
    "    for col in df_wide.columns:\n",
    "        y = np.asarray(df_wide[col].values)\n",
    "        ln, = ax.plot(x_all[mask], y[mask], label=str(col), linewidth=2)\n",
    "        lines.append(ln)\n",
    "\n",
    "    # Labels/grid\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel({\"Y\":\"Year\", \"Q\":\"Quarter\", \"M\":\"Month\"}[FREQ])\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # --- compute effective gap (fraction) from chosen mode ---\n",
    "    if LEGEND_GAP_MODE.lower() == \"inch\":\n",
    "        gap_frac = _gap_frac_from_inches(fig, ax, LEGEND_GAP_INCH)\n",
    "    else:\n",
    "        gap_frac = LEGEND_GAP_FRAC\n",
    "\n",
    "    # Leave room for legend panel\n",
    "    fig.subplots_adjust(right=1.0 - (LEGEND_PANEL_FRAC + gap_frac))\n",
    "\n",
    "    # Right-side vertical legend panel (swatch above label)\n",
    "    leg_ax = inset_axes(\n",
    "        ax,\n",
    "        width=f\"{int(LEGEND_PANEL_FRAC*100)}%\", height=\"100%\",\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(1.0 + gap_frac, 0.0, LEGEND_PANEL_FRAC, 1.0),\n",
    "        bbox_transform=ax.transAxes,\n",
    "        borderpad=0,\n",
    "    )\n",
    "    leg_ax.set_xlim(0, 1); leg_ax.set_ylim(0, 1); leg_ax.axis(\"off\")\n",
    "\n",
    "    labels = legend_labels if legend_labels is not None else [ln.get_label() for ln in lines]\n",
    "    n = len(lines); top, bottom = 0.94, 0.06\n",
    "    step = 0 if n <= 1 else (top - bottom) / (n - 1)\n",
    "    for i, (ln, txt) in enumerate(zip(lines, labels)):\n",
    "        y = top - i * step\n",
    "        leg_ax.plot([0.25, 0.75], [y, y], color=ln.get_color(), linewidth=3, solid_capstyle=\"round\")\n",
    "        leg_ax.text(0.50, y - 0.06, txt, ha=\"center\", va=\"top\", fontsize=9)\n",
    "\n",
    "    fig.savefig(OUT_DIR / f\"{fname}.svg\", format=\"svg\", bbox_inches=\"tight\", dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ----------------------------- BUILD & PLOT ----------------------\n",
    "MODELS = [\n",
    "    (\"Type\",      \"Type_Label\"),\n",
    "    (\"Stance\",    \"Stance_Label\"),\n",
    "    (\"Sentiment\", \"Sentiment_Label\"),\n",
    "    (\"Discourse\", \"Discourse_Label\"),\n",
    "]\n",
    "\n",
    "for model_name, label_col in MODELS:\n",
    "    order = LABEL_ORDERS.get(label_col, None)\n",
    "    counts, shares = make_timeseries(df, label_col, freq=FREQ, order=order,\n",
    "                                     w=ROLLING_WINDOW, centered=CENTERED)\n",
    "\n",
    "    # Save the underlying time series\n",
    "    counts.to_csv(OUT_DIR / f\"{model_name.lower()}_{FREQ}_counts_timeseries.csv\")\n",
    "    shares.to_csv(OUT_DIR / f\"{model_name.lower()}_{FREQ}_shares_timeseries.csv\")\n",
    "\n",
    "    # Wrap Type labels in the legend only\n",
    "    legend_map = None\n",
    "    if label_col == \"Type_Label\":\n",
    "        legend_map = [TYPE_LEGEND_WRAP.get(c, c) for c in counts.columns]\n",
    "\n",
    "    ttl = f\"{model_name}: label counts over time ({FREQ.lower()})\"\n",
    "    plot_lines(counts, ttl, \"Count\", f\"{model_name.lower()}_{FREQ}_counts_line\", legend_labels=legend_map)\n",
    "\n",
    "    ttl = f\"{model_name}: label shares over time ({FREQ.lower()})\"\n",
    "    plot_lines(shares, ttl, \"Share of CBDC sentences\", f\"{model_name.lower()}_{FREQ}_shares_line\", legend_labels=legend_map)\n",
    "\n",
    "print(\"Saved SVGs & CSVs to:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bef6a",
   "metadata": {},
   "source": [
    "# CBDC mentions over time (Sentences and Speeches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd96be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# CBDC mentions over time â€” TWO SEPARATE GRAPHS\n",
    "#   â€¢ Graph 1: Sentences (row counts)\n",
    "#   â€¢ Graph 2: Speeches (unique URLs)\n",
    "# ==============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "CSV_PATH = \"cbdc-dataset-predictions.csv\"     # input file\n",
    "DATE_COL = \"date\"\n",
    "URL_COL  = \"url\"\n",
    "\n",
    "# Frequency selector: 'D' (daily), 'M' (monthly), 'Q' (quarterly), 'Y' (yearly)\n",
    "FREQ = \"Q\"                                    # â† change to 'D' / 'M' / 'Q' / 'Y'\n",
    "\n",
    "# Optional smoothing (rolling mean) per frequency (set to 1 to disable)\n",
    "ROLLING_WINDOW = {\"D\": 7, \"M\": 3, \"Q\": 2, \"Y\": 1}[FREQ]\n",
    "CENTERED = False                               # keep False to avoid bleeding past horizon\n",
    "\n",
    "# Study horizon (hard clamp)\n",
    "START = pd.Timestamp(\"2016-01-01\")\n",
    "END   = pd.Timestamp(\"2024-12-31\")\n",
    "\n",
    "OUT_DIR = Path(\"prediction_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- UTILITIES ----------\n",
    "def parse_dates(series: pd.Series) -> pd.Series:\n",
    "    d1 = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    m  = d1.isna()\n",
    "    if m.any():\n",
    "        d2 = pd.to_datetime(series[m], errors=\"coerce\", dayfirst=True)\n",
    "        d1.loc[m] = d2\n",
    "    return d1\n",
    "\n",
    "def full_period_range(start: pd.Timestamp, end: pd.Timestamp, freq: str) -> pd.PeriodIndex:\n",
    "    return pd.period_range(pd.Period(start, freq), pd.Period(end, freq), freq=freq)\n",
    "\n",
    "def nice_x_ticks(ax, freq: str, start: pd.Timestamp, end: pd.Timestamp):\n",
    "    if freq == \"Y\":\n",
    "        ticks = pd.date_range(start, end, freq=\"YS\")\n",
    "        ax.set_xticks(ticks)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "        ax.set_xlabel(\"Year\")\n",
    "    elif freq == \"Q\":\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "        ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[3,6,9,12]))\n",
    "        ax.set_xlabel(\"Quarter\")\n",
    "    elif freq == \"M\":\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "        ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "        ax.set_xlabel(\"Month\")\n",
    "    else:  # 'D'\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "        ax.set_xlabel(\"Day\")\n",
    "\n",
    "def freq_name(f):\n",
    "    return {\"D\":\"day\",\"M\":\"month\",\"Q\":\"quarter\",\"Y\":\"year\"}[f]\n",
    "\n",
    "# ---------- LOAD ----------\n",
    "usecols = [DATE_COL, URL_COL]\n",
    "df = pd.read_csv(CSV_PATH, usecols=usecols)\n",
    "df[\"__date\"] = parse_dates(df[DATE_COL]).dt.floor(\"D\")\n",
    "df = df[(df[\"__date\"] >= START) & (df[\"__date\"] <= END)].dropna(subset=[\"__date\"]).reset_index(drop=True)\n",
    "\n",
    "# ---------- BUILD PERIOD SERIES ----------\n",
    "df[\"_period\"] = df[\"__date\"].dt.to_period(FREQ)\n",
    "idx = full_period_range(START, END, FREQ)\n",
    "\n",
    "# Sentences = row count per period\n",
    "sentences = df.groupby(\"_period\").size().reindex(idx, fill_value=0)\n",
    "\n",
    "# Speeches = unique URLs per period\n",
    "speeches  = df.groupby(\"_period\")[URL_COL].nunique().reindex(idx, fill_value=0)\n",
    "\n",
    "# Optional smoothing\n",
    "if ROLLING_WINDOW and ROLLING_WINDOW > 1:\n",
    "    sentences_sm = sentences.rolling(ROLLING_WINDOW, center=CENTERED, min_periods=1).mean()\n",
    "    speeches_sm  = speeches.rolling(ROLLING_WINDOW, center=CENTERED, min_periods=1).mean()\n",
    "else:\n",
    "    sentences_sm = sentences\n",
    "    speeches_sm  = speeches\n",
    "\n",
    "# Save underlying data\n",
    "pd.DataFrame({\"Sentences\": sentences, \"Sentences_smoothed\": sentences_sm}).to_csv(\n",
    "    OUT_DIR / f\"cbdc_sentences_{FREQ}_timeseries.csv\", index_label=\"period\"\n",
    ")\n",
    "pd.DataFrame({\"Speeches\": speeches, \"Speeches_smoothed\": speeches_sm}).to_csv(\n",
    "    OUT_DIR / f\"cbdc_speeches_{FREQ}_timeseries.csv\", index_label=\"period\"\n",
    ")\n",
    "\n",
    "# ---------- PLOT: Sentences ----------\n",
    "fig1, ax1 = plt.subplots(figsize=(6.0, 5.5))   # sized for Word\n",
    "x = sentences_sm.index.asfreq(FREQ, how=\"end\").to_timestamp().floor(\"D\")\n",
    "ax1.set_xlim(START, END); ax1.set_xbound(START, END)\n",
    "ax1.set_xmargin(0.0); ax1.margins(x=0.0)\n",
    "nice_x_ticks(ax1, FREQ, START, END)\n",
    "ax1.plot(x, sentences_sm.values, linewidth=2)\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "ax1.set_title(f\"CBDC sentences over {freq_name(FREQ)}\")\n",
    "fig1.savefig(OUT_DIR / f\"cbdc_sentences_{FREQ}_line.svg\", format=\"svg\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.close(fig1)\n",
    "\n",
    "# ---------- PLOT: Speeches (unique URLs) ----------\n",
    "fig2, ax2 = plt.subplots(figsize=(6.0, 5.5))\n",
    "x2 = speeches_sm.index.asfreq(FREQ, how=\"end\").to_timestamp().floor(\"D\")\n",
    "ax2.set_xlim(START, END); ax2.set_xbound(START, END)\n",
    "ax2.set_xmargin(0.0); ax2.margins(x=0.0)\n",
    "nice_x_ticks(ax2, FREQ, START, END)\n",
    "ax2.plot(x2, speeches_sm.values, linewidth=2)\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "ax2.set_title(f\"CBDC speeches (unique URLs) over {freq_name(FREQ)}\")\n",
    "fig2.savefig(OUT_DIR / f\"cbdc_speeches_{FREQ}_line.svg\", format=\"svg\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.close(fig2)\n",
    "\n",
    "print(\"Saved to:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887cf135",
   "metadata": {},
   "source": [
    "# CBDC speech sample: scope and text statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9af291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Statistic                   Value Mean per year Min year (value) [year] Max year (value) [year]\n",
      "     No of speeches mention CBDC                     588         65.33        2016 (13) [2016]       2022 (110) [2022]\n",
      "            No of CBDC sentences                   5,376        597.33        2016 (48) [2016]     2022 (1,309) [2022]\n",
      "                  Unique authors                     157         37.67        2016 (11) [2016]        2021 (54) [2021]\n",
      "                Unique countries                      57         23.89         2016 (9) [2016]        2021 (33) [2021]\n",
      "                      Date range 2016-03-02 â€” 2024-12-14                                                              \n",
      "        Years covered (distinct)                       9                                                              \n",
      "               Median year [IQR]        2022 [2020â€“2023]                                                              \n",
      "     Sentences per speech (mean)                    9.14                                                              \n",
      "   Sentences per speech (median)                       3                                                              \n",
      "Word length (mean / median / sd)        28.6 / 25 / 16.5                                                              \n",
      "Char length (mean / median / sd)     175.2 / 153 / 104.7                                                              \n",
      "\n",
      "Saved: F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\table1_scope_text_stats.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# Table 1 with Min/Max columns showing year(s) again in brackets\n",
    "# e.g., \"2016, 2017 (41) [2016; 2017]\"\n",
    "# ==============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "CSV_PATH = \"cbdc-dataset-predictions.csv\"\n",
    "OUT_DIR  = Path(\"prediction_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def parse_dates(series: pd.Series) -> pd.Series:\n",
    "    d1 = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    m  = d1.isna()\n",
    "    if m.any():\n",
    "        d2 = pd.to_datetime(series[m], errors=\"coerce\", dayfirst=True)\n",
    "        d1.loc[m] = d2\n",
    "    return d1\n",
    "\n",
    "_word_re = re.compile(r\"\\w+\", flags=re.UNICODE)\n",
    "def word_count(text: str) -> int:\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(_word_re.findall(text))\n",
    "\n",
    "def fmt_int(x):   return f\"{int(x):,}\"\n",
    "def fmt_float(x, d=2): return f\"{float(x):.{d}f}\"\n",
    "def fmt_mmssd(mean, median, sd, d_mean=1, d_med=0, d_sd=1):\n",
    "    return f\"{mean:.{d_mean}f} / {median:.{d_med}f} / {sd:.{d_sd}f}\"\n",
    "def fmt_daterange(dmin, dmax):\n",
    "    return f\"{dmin.date().isoformat()} â€” {dmax.date().isoformat()}\"\n",
    "\n",
    "def year_min_max_strings(s: pd.Series):\n",
    "    \"\"\"\n",
    "    Given a per-year Series (index=year, values=counts), return:\n",
    "      mean,\n",
    "      'YYYY[, YYYYâ€¦] (value_min) [YYYY; YYYYâ€¦]',\n",
    "      'YYYY[, YYYYâ€¦] (value_max) [YYYY; YYYYâ€¦]'\n",
    "    \"\"\"\n",
    "    if s.empty:\n",
    "        return np.nan, \"\", \"\"\n",
    "    s = s.dropna()\n",
    "    mu = s.mean()\n",
    "    vmin, vmax = s.min(), s.max()\n",
    "    min_years = s.index[s == vmin].astype(int).tolist()\n",
    "    max_years = s.index[s == vmax].astype(int).tolist()\n",
    "\n",
    "    min_years_str = \", \".join(map(str, min_years))\n",
    "    max_years_str = \", \".join(map(str, max_years))\n",
    "    min_bracket   = \"; \".join(map(str, min_years))\n",
    "    max_bracket   = \"; \".join(map(str, max_years))\n",
    "\n",
    "    min_str = f\"{min_years_str} ({fmt_int(vmin)}) [{min_bracket}]\"\n",
    "    max_str = f\"{max_years_str} ({fmt_int(vmax)}) [{max_bracket}]\"\n",
    "    return mu, min_str, max_str\n",
    "\n",
    "# ---------- load ----------\n",
    "usecols = [\"url\",\"cbdc_sentence\",\"date\",\"author\",\"country\"]\n",
    "df = pd.read_csv(CSV_PATH, usecols=usecols)\n",
    "df[\"__date\"] = parse_dates(df[\"date\"]).dt.floor(\"D\")\n",
    "df = df[df[\"__date\"].notna()].copy()\n",
    "df[\"__year\"] = df[\"__date\"].dt.year.astype(int)\n",
    "\n",
    "# ---------- base counts ----------\n",
    "n_rows       = len(df)                       # CBDC sentences\n",
    "n_urls       = df[\"url\"].nunique()          # speeches (unique URLs)\n",
    "n_authors    = df[\"author\"].nunique()\n",
    "n_countries  = df[\"country\"].nunique()\n",
    "dmin, dmax   = df[\"__date\"].min(), df[\"__date\"].max()\n",
    "\n",
    "years_unique = np.sort(df[\"__year\"].unique())\n",
    "n_years      = len(years_unique)\n",
    "\n",
    "# Median year & IQR\n",
    "years = df[\"__year\"]\n",
    "y50 = int(np.round(years.median()))\n",
    "y25 = int(np.floor(np.percentile(years, 25)))\n",
    "y75 = int(np.ceil(np.percentile(years, 75)))\n",
    "median_year_iqr = f\"{y50} [{y25}â€“{y75}]\"\n",
    "\n",
    "# Sentences per speech\n",
    "per_url = df.groupby(\"url\").size()\n",
    "sentences_per_speech_mean   = per_url.mean()\n",
    "sentences_per_speech_median = per_url.median()\n",
    "\n",
    "# Text lengths\n",
    "df[\"__words\"] = df[\"cbdc_sentence\"].apply(word_count)\n",
    "df[\"__chars\"] = df[\"cbdc_sentence\"].fillna(\"\").astype(str).str.len()\n",
    "w_mean, w_med, w_sd = df[\"__words\"].mean(), df[\"__words\"].median(), df[\"__words\"].std(ddof=1)\n",
    "c_mean, c_med, c_sd = df[\"__chars\"].mean(), df[\"__chars\"].median(), df[\"__chars\"].std(ddof=1)\n",
    "\n",
    "# ---------- yearly aggregates for mean/min/max year reporting ----------\n",
    "sentences_by_year = df.groupby(\"__year\").size()\n",
    "speeches_by_year  = df.groupby(\"__year\")[\"url\"].nunique()\n",
    "authors_by_year   = df.groupby(\"__year\")[\"author\"].nunique()\n",
    "countries_by_year = df.groupby(\"__year\")[\"country\"].nunique()\n",
    "\n",
    "mpy_sent, miny_sent, maxy_sent = year_min_max_strings(sentences_by_year)\n",
    "mpy_spch, miny_spch, maxy_spch = year_min_max_strings(speeches_by_year)\n",
    "mpy_auth, miny_auth, maxy_auth = year_min_max_strings(authors_by_year)\n",
    "mpy_ctry, miny_ctry, maxy_ctry = year_min_max_strings(countries_by_year)\n",
    "\n",
    "# ---------- assemble table ----------\n",
    "rows = [\n",
    "    (\"No of speeches mention CBDC\", fmt_int(n_urls), fmt_float(mpy_spch, 2), miny_spch, maxy_spch),\n",
    "    (\"No of CBDC sentences\",        fmt_int(n_rows), fmt_float(mpy_sent, 2), miny_sent, maxy_sent),\n",
    "    (\"Unique authors\",              fmt_int(n_authors), fmt_float(mpy_auth, 2), miny_auth, maxy_auth),\n",
    "    (\"Unique countries\",            fmt_int(n_countries), fmt_float(mpy_ctry, 2), miny_ctry, maxy_ctry),\n",
    "    (\"Date range\", fmt_daterange(dmin, dmax), \"\", \"\", \"\"),\n",
    "    (\"Years covered (distinct)\", f\"{n_years}\", \"\", \"\", \"\"),\n",
    "    (\"Median year [IQR]\", median_year_iqr, \"\", \"\", \"\"),\n",
    "    (\"Sentences per speech (mean)\", fmt_float(sentences_per_speech_mean, 2), \"\", \"\", \"\"),\n",
    "    (\"Sentences per speech (median)\", fmt_float(sentences_per_speech_median, 0), \"\", \"\", \"\"),\n",
    "    (\"Word length (mean / median / sd)\", fmt_mmssd(w_mean, w_med, w_sd, d_mean=1, d_med=0, d_sd=1), \"\", \"\", \"\"),\n",
    "    (\"Char length (mean / median / sd)\", fmt_mmssd(c_mean, c_med, c_sd, d_mean=1, d_med=0, d_sd=1), \"\", \"\", \"\"),\n",
    "]\n",
    "\n",
    "table = pd.DataFrame(rows, columns=[\n",
    "    \"Statistic\", \"Value\", \"Mean per year\", \"Min year (value) [year]\", \"Max year (value) [year]\"\n",
    "])\n",
    "\n",
    "# save & preview\n",
    "out_csv = OUT_DIR / \"table1_scope_text_stats.csv\"\n",
    "table.to_csv(out_csv, index=False)\n",
    "print(table.to_string(index=False))\n",
    "print(\"\\nSaved:\", out_csv.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110d68e",
   "metadata": {},
   "source": [
    "# Stance and Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e04104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_15748\\1116017211.py:19: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", dayfirst=False, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Wrote speech_level_stance.csv (588 speeches)\n",
      "âœ“ Wrote country_quarter_stance.csv (379 country-quarters)\n",
      "âœ“ Wrote country_year_stance.csv (215 country-years)\n",
      "âœ“ Wrote global_quarter_stance.csv (36 quarters)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "freq not specified and cannot be inferred",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 175\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m    174\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m--> 175\u001b[0m x \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mPeriodIndex(global_quarter[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquarter\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto_timestamp()\n\u001b[0;32m    176\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x, global_quarter[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_stance\u001b[39m\u001b[38;5;124m\"\u001b[39m], linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    177\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal mean CBDC stance (speech-level, quarterly)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\period.py:314\u001b[0m, in \u001b[0;36mPeriodIndex.__new__\u001b[1;34m(cls, data, ordinal, freq, dtype, copy, name, **fields)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot pass both data and ordinal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't pass copy here, since we copy later.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         data \u001b[38;5;241m=\u001b[39m period_array(data\u001b[38;5;241m=\u001b[39mdata, freq\u001b[38;5;241m=\u001b[39mfreq)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m    317\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\period.py:1124\u001b[0m, in \u001b[0;36mperiod_array\u001b[1;34m(data, freq, copy)\u001b[0m\n\u001b[0;32m   1122\u001b[0m data \u001b[38;5;241m=\u001b[39m ensure_object(arrdata)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1124\u001b[0m     freq \u001b[38;5;241m=\u001b[39m libperiod\u001b[38;5;241m.\u001b[39mextract_freq(data)\n\u001b[0;32m   1125\u001b[0m dtype \u001b[38;5;241m=\u001b[39m PeriodDtype(freq)\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PeriodArray\u001b[38;5;241m.\u001b[39m_from_sequence(data, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mpandas/_libs/tslibs/period.pyx:1621\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.period.extract_freq\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: freq not specified and cannot be inferred"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CentralBank-BERT: stance aggregation from sentence -> speech -> countryâ€“time ---\n",
    "# Assumes a CSV with at least these columns:\n",
    "# url, cbdc_sentence, title, description, date, author, affiliation, position, country,\n",
    "# Type_Label, Type_Score, Stance_Label, Stance_Score, Sentiment_Label, Sentiment_Score,\n",
    "# Discourse_Label, Discourse_Score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "# 0) Config\n",
    "CSV_PATH = \"cbdc-dataset-predictions.csv\"   # <-- change to your file name\n",
    "OUTDIR = Path(\".\")                          # writes next to the notebook\n",
    "\n",
    "# 1) Load + clean\n",
    "def parse_date(s):\n",
    "    # robust parse for e.g., \"3/2/2016 0:00\", \"2016-03-02\", etc.\n",
    "    return pd.to_datetime(s, errors=\"coerce\", dayfirst=False, infer_datetime_format=True)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"date\" not in df.columns:\n",
    "    raise ValueError(\"Expected a 'date' column in the CSV.\")\n",
    "df[\"date\"] = df[\"date\"].apply(parse_date)\n",
    "if df[\"date\"].isna().all():\n",
    "    raise ValueError(\"All dates failed to parseâ€”check the 'date' column format.\")\n",
    "\n",
    "# 2) Canonicalize stance labels and probabilities\n",
    "# Expected values in Stance_Label: {\"Pro-CBDC\",\"Wait-and-See\",\"Anti-CBDC\"}\n",
    "canon_map = {\n",
    "    \"Pro-CBDC\": \"pro\",\n",
    "    \"Wait-and-See\": \"wait\",\n",
    "    \"Anti-CBDC\": \"anti\",\n",
    "    # also tolerate lower/variant forms just in case\n",
    "    \"pro\": \"pro\", \"wait\": \"wait\", \"anti\": \"anti\",\n",
    "    \"Pro\": \"pro\", \"Wait\": \"wait\", \"Anti\": \"anti\"\n",
    "}\n",
    "df[\"stance_canon\"] = df[\"Stance_Label\"].map(canon_map).fillna(df[\"Stance_Label\"].str.lower())\n",
    "\n",
    "# Basic sanity\n",
    "missing = df[\"stance_canon\"].isna().sum()\n",
    "if missing > 0:\n",
    "    print(f\"Warning: {missing} rows have unknown stance labels; they will be dropped.\")\n",
    "    df = df[~df[\"stance_canon\"].isna()].copy()\n",
    "\n",
    "# 3) Create a stable speech_id\n",
    "# Use (url, title, date) as a composite key; hash to keep it tidy\n",
    "def mk_speech_id(row):\n",
    "    key = f\"{row.get('url','')}\\u241F{row.get('title','')}\\u241F{row.get('date','')}\"\n",
    "    return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "df[\"speech_id\"] = df.apply(mk_speech_id, axis=1)\n",
    "\n",
    "# 4) Probability-weighted aggregation at speech level\n",
    "# For each sentence, contribute its predicted-class probability to the class bucket\n",
    "for lab in [\"pro\", \"wait\", \"anti\"]:\n",
    "    df[f\"{lab}_w\"] = np.where(df[\"stance_canon\"] == lab, df[\"Stance_Score\"].astype(float), 0.0)\n",
    "\n",
    "speech_w = (df.groupby(\"speech_id\")[[\"pro_w\",\"wait_w\",\"anti_w\"]]\n",
    "              .sum()\n",
    "              .assign(speech_stance_pred=lambda d: d[[\"anti_w\",\"wait_w\",\"pro_w\"]]\n",
    "                      .idxmax(axis=1).str.replace(\"_w\",\"\", regex=False)))\n",
    "\n",
    "# 4a) Tie-breaker: if two buckets are (near) equal within 1e-9, fall back to simple count majority\n",
    "tol = 1e-12\n",
    "def tiebreak(row):\n",
    "    v = row[[\"pro_w\",\"wait_w\",\"anti_w\"]].values\n",
    "    if (np.max(v) - np.sort(v)[-2]) <= tol:\n",
    "        # tie â†’ choose most frequent stance by counts; if still tie, choose 'wait'\n",
    "        counts = (df.loc[df[\"speech_id\"] == row.name, \"stance_canon\"]\n",
    "                    .value_counts())\n",
    "        if counts.empty:\n",
    "            return \"wait\"\n",
    "        top = counts.sort_values(ascending=False)\n",
    "        if len(top) > 1 and top.iloc[0] == top.iloc[1]:\n",
    "            return \"wait\"\n",
    "        return top.index[0]\n",
    "    return row[\"speech_stance_pred\"]\n",
    "\n",
    "speech_w[\"speech_stance_pred\"] = speech_w.apply(tiebreak, axis=1)\n",
    "\n",
    "# 5) Map to numeric {-1,0,+1} for comparability with BIS research series\n",
    "num_map = {\"anti\": -1, \"wait\": 0, \"pro\": 1}\n",
    "speech_w[\"speech_stance_num\"] = speech_w[\"speech_stance_pred\"].map(num_map)\n",
    "\n",
    "# 6) Attach speech metadata (first author/country/date/title/url per speech)\n",
    "meta_cols = [\"date\",\"country\",\"author\",\"affiliation\",\"title\",\"url\"]\n",
    "speech_meta = (df.groupby(\"speech_id\")[meta_cols]\n",
    "                 .agg({\n",
    "                     \"date\":\"min\",\n",
    "                     \"country\": lambda s: s.dropna().iloc[0] if len(s.dropna()) else np.nan,\n",
    "                     \"author\":  lambda s: s.dropna().iloc[0] if len(s.dropna()) else np.nan,\n",
    "                     \"affiliation\": lambda s: s.dropna().iloc[0] if len(s.dropna()) else np.nan,\n",
    "                     \"title\":   lambda s: s.dropna().iloc[0] if len(s.dropna()) else np.nan,\n",
    "                     \"url\":     lambda s: s.dropna().iloc[0] if len(s.dropna()) else np.nan\n",
    "                 })\n",
    "                 .reset_index())\n",
    "\n",
    "speech = (speech_w.reset_index()\n",
    "                   .merge(speech_meta, on=\"speech_id\", how=\"left\"))\n",
    "\n",
    "# 7) Time keys\n",
    "speech[\"quarter\"] = pd.PeriodIndex(pd.to_datetime(speech[\"date\"]), freq=\"Q\").astype(str)\n",
    "speech[\"year\"]    = pd.to_datetime(speech[\"date\"]).dt.year\n",
    "\n",
    "# 8) Countryâ€“time series (fixed)\n",
    "\n",
    "# base stats per country-quarter\n",
    "base_cq = (speech\n",
    "           .dropna(subset=[\"country\",\"quarter\"])\n",
    "           .groupby([\"country\",\"quarter\"])\n",
    "           .agg(mean_stance=(\"speech_stance_num\",\"mean\"),\n",
    "                n_speeches=(\"speech_id\",\"nunique\"))\n",
    "           .reset_index())\n",
    "\n",
    "# counts by stance per country-quarter\n",
    "tmp = (speech\n",
    "       .dropna(subset=[\"country\",\"quarter\"])\n",
    "       .groupby([\"country\",\"quarter\",\"speech_stance_num\"])\n",
    "       .size()\n",
    "       .rename(\"n\")\n",
    "       .reset_index())\n",
    "\n",
    "# convert counts to shares within each country-quarter\n",
    "tot = tmp.groupby([\"country\",\"quarter\"])[\"n\"].transform(\"sum\")\n",
    "tmp[\"share\"] = tmp[\"n\"] / tot\n",
    "\n",
    "# pivot stance shares to columns\n",
    "shares = (tmp.pivot_table(index=[\"country\",\"quarter\"],\n",
    "                          columns=\"speech_stance_num\",\n",
    "                          values=\"share\",\n",
    "                          fill_value=0.0)\n",
    "            .rename(columns={-1:\"anti_share\", 0:\"wait_share\", 1:\"pro_share\"})\n",
    "            .reset_index())\n",
    "\n",
    "# merge base stats + shares\n",
    "country_quarter = (base_cq.merge(shares, on=[\"country\",\"quarter\"], how=\"left\")\n",
    "                          .sort_values([\"country\",\"quarter\"]))\n",
    "\n",
    "# 9) Countryâ€“year series\n",
    "country_year = (speech\n",
    "                .dropna(subset=[\"country\",\"year\"])\n",
    "                .groupby([\"country\",\"year\"])\n",
    "                .agg(mean_stance=(\"speech_stance_num\",\"mean\"),\n",
    "                     n_speeches=(\"speech_id\",\"nunique\"))\n",
    "                .reset_index()\n",
    "                .sort_values([\"country\",\"year\"]))\n",
    "\n",
    "# 10) Global quarterly series\n",
    "global_quarter = (speech.groupby(\"quarter\")\n",
    "                        .agg(mean_stance=(\"speech_stance_num\",\"mean\"),\n",
    "                             n_speeches=(\"speech_id\",\"nunique\"))\n",
    "                        .reset_index())\n",
    "\n",
    "# save (same filenames as before)\n",
    "speech_out = OUTDIR / \"speech_level_stance.csv\"\n",
    "cq_out     = OUTDIR / \"country_quarter_stance.csv\"\n",
    "cy_out     = OUTDIR / \"country_year_stance.csv\"\n",
    "gq_out     = OUTDIR / \"global_quarter_stance.csv\"\n",
    "\n",
    "speech.to_csv(speech_out, index=False)\n",
    "country_quarter.to_csv(cq_out, index=False)\n",
    "country_year.to_csv(cy_out, index=False)\n",
    "global_quarter.to_csv(gq_out, index=False)\n",
    "\n",
    "print(f\"âœ“ Wrote {speech_out.name} ({len(speech)} speeches)\")\n",
    "print(f\"âœ“ Wrote {cq_out.name} ({len(country_quarter)} country-quarters)\")\n",
    "print(f\"âœ“ Wrote {cy_out.name} ({len(country_year)} country-years)\")\n",
    "print(f\"âœ“ Wrote {gq_out.name} ({len(global_quarter)} quarters)\")\n",
    "\n",
    "\n",
    "# 11) Minimal plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "x = pd.PeriodIndex(global_quarter[\"quarter\"]).to_timestamp()\n",
    "plt.plot(x, global_quarter[\"mean_stance\"], linewidth=2)\n",
    "plt.title(\"Global mean CBDC stance (speech-level, quarterly)\")\n",
    "plt.xlabel(\"Quarter\")\n",
    "plt.ylabel(\"Mean stance (-1, 0, +1)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60afc2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Wrote country_overall_stance.csv (57 countries)\n",
      "âœ“ Wrote country_overall_stance_2016_2024.csv (57 countries)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>mean_stance</th>\n",
       "      <th>n_speeches</th>\n",
       "      <th>first_date</th>\n",
       "      <th>last_date</th>\n",
       "      <th>anti_share</th>\n",
       "      <th>wait_share</th>\n",
       "      <th>pro_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bahamas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>North Macedonia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-07-07</td>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-06-28</td>\n",
       "      <td>2021-10-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barbados</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-10-03</td>\n",
       "      <td>2022-07-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cyprus</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Israel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bosnia and Herzegovina</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Eswatini</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-06-13</td>\n",
       "      <td>2024-06-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kuwait</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-27</td>\n",
       "      <td>2018-11-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Morocco</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  country  mean_stance  n_speeches first_date  last_date  \\\n",
       "0                 Bahamas          1.0           4 2018-09-17 2020-01-16   \n",
       "1         North Macedonia          1.0           3 2017-07-07 2021-12-09   \n",
       "2                Portugal          1.0           3 2021-06-28 2021-10-11   \n",
       "3                Barbados          1.0           2 2017-10-03 2022-07-26   \n",
       "4                  Cyprus          1.0           2 2023-12-07 2023-12-09   \n",
       "5                  Israel          1.0           2 2022-06-20 2022-11-16   \n",
       "6  Bosnia and Herzegovina          1.0           1 2018-11-01 2018-11-01   \n",
       "7                Eswatini          1.0           1 2024-06-13 2024-06-13   \n",
       "8                  Kuwait          1.0           1 2018-11-27 2018-11-27   \n",
       "9                 Morocco          1.0           1 2019-03-13 2019-03-13   \n",
       "\n",
       "   anti_share  wait_share  pro_share  \n",
       "0         0.0         0.0        1.0  \n",
       "1         0.0         0.0        1.0  \n",
       "2         0.0         0.0        1.0  \n",
       "3         0.0         0.0        1.0  \n",
       "4         0.0         0.0        1.0  \n",
       "5         0.0         0.0        1.0  \n",
       "6         0.0         0.0        1.0  \n",
       "7         0.0         0.0        1.0  \n",
       "8         0.0         0.0        1.0  \n",
       "9         0.0         0.0        1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>mean_stance</th>\n",
       "      <th>n_speeches</th>\n",
       "      <th>first_date</th>\n",
       "      <th>last_date</th>\n",
       "      <th>anti_share</th>\n",
       "      <th>wait_share</th>\n",
       "      <th>pro_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Euro Area</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>76</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>2024-11-28</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.381579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>France</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>63</td>\n",
       "      <td>2018-03-13</td>\n",
       "      <td>2024-11-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Germany</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>56</td>\n",
       "      <td>2016-06-21</td>\n",
       "      <td>2024-10-14</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.589286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>UK</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>34</td>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>2024-10-26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>US</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>32</td>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>2024-11-12</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>India</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>28</td>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>2024-12-14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Italy</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>27</td>\n",
       "      <td>2018-06-07</td>\n",
       "      <td>2024-09-24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.703704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>26</td>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>2024-11-27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Japan</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>19</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>2024-03-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>15</td>\n",
       "      <td>2016-11-16</td>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      country  mean_stance  n_speeches first_date  last_date  anti_share  \\\n",
       "41  Euro Area     0.342105          76 2016-04-25 2024-11-28    0.039474   \n",
       "19     France     0.761905          63 2018-03-13 2024-11-06    0.000000   \n",
       "27    Germany     0.571429          56 2016-06-21 2024-10-14    0.017857   \n",
       "45         UK     0.294118          34 2016-03-02 2024-10-26    0.000000   \n",
       "49         US     0.156250          32 2017-03-03 2024-11-12    0.093750   \n",
       "17      India     0.821429          28 2021-03-25 2024-12-14    0.000000   \n",
       "22      Italy     0.703704          27 2018-06-07 2024-09-24    0.000000   \n",
       "14  Hong Kong     0.923077          26 2018-09-21 2024-11-27    0.000000   \n",
       "40      Japan     0.368421          19 2016-05-12 2024-03-06    0.000000   \n",
       "21  Singapore     0.733333          15 2016-11-16 2024-11-04    0.000000   \n",
       "\n",
       "    wait_share  pro_share  \n",
       "41    0.578947   0.381579  \n",
       "19    0.238095   0.761905  \n",
       "27    0.392857   0.589286  \n",
       "45    0.705882   0.294118  \n",
       "49    0.656250   0.250000  \n",
       "17    0.178571   0.821429  \n",
       "22    0.296296   0.703704  \n",
       "14    0.076923   0.923077  \n",
       "40    0.631579   0.368421  \n",
       "21    0.266667   0.733333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Country-level stance aggregates (overall + optional filtered window) ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Safety check\n",
    "required_cols = {\"country\",\"date\",\"speech_id\",\"speech_stance_num\",\"speech_stance_pred\"}\n",
    "missing = required_cols - set(speech.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"speech is missing columns: {missing}\")\n",
    "\n",
    "# Helper to compute shares of {-1,0,+1}\n",
    "def stance_shares(df_grouped):\n",
    "    tmp = (df_grouped\n",
    "           .groupby([\"country\",\"speech_stance_num\"])\n",
    "           .size()\n",
    "           .rename(\"n\")\n",
    "           .reset_index())\n",
    "    tot = tmp.groupby(\"country\")[\"n\"].transform(\"sum\")\n",
    "    tmp[\"share\"] = tmp[\"n\"] / tot\n",
    "    shares = (tmp.pivot_table(index=\"country\",\n",
    "                              columns=\"speech_stance_num\",\n",
    "                              values=\"share\",\n",
    "                              fill_value=0.0)\n",
    "                .rename(columns={-1:\"anti_share\", 0:\"wait_share\", 1:\"pro_share\"})\n",
    "                .reset_index())\n",
    "    return shares\n",
    "\n",
    "# -------------------------\n",
    "# 1) Overall (full sample)\n",
    "# -------------------------\n",
    "base_country = (speech\n",
    "                .groupby(\"country\")\n",
    "                .agg(mean_stance=(\"speech_stance_num\",\"mean\"),\n",
    "                     n_speeches=(\"speech_id\",\"nunique\"),\n",
    "                     first_date=(\"date\",\"min\"),\n",
    "                     last_date=(\"date\",\"max\"))\n",
    "                .reset_index())\n",
    "\n",
    "shares_country = stance_shares(speech)\n",
    "\n",
    "country_overall = (base_country\n",
    "                   .merge(shares_country, on=\"country\", how=\"left\")\n",
    "                   .sort_values([\"mean_stance\",\"n_speeches\"], ascending=[False, False])\n",
    "                   .reset_index(drop=True))\n",
    "\n",
    "# Save\n",
    "country_overall_path = OUTDIR / \"country_overall_stance.csv\"\n",
    "country_overall.to_csv(country_overall_path, index=False)\n",
    "print(f\"âœ“ Wrote {country_overall_path.name} ({len(country_overall)} countries)\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2) Optional: Filtered window (e.g., 2016â€“2024) for robustness\n",
    "# ----------------------------------------------------------------\n",
    "START, END = pd.Timestamp(\"2016-01-01\"), pd.Timestamp(\"2024-12-31\")\n",
    "speech_win = speech[(speech[\"date\"] >= START) & (speech[\"date\"] <= END)].copy()\n",
    "\n",
    "base_country_win = (speech_win\n",
    "                    .groupby(\"country\")\n",
    "                    .agg(mean_stance=(\"speech_stance_num\",\"mean\"),\n",
    "                         n_speeches=(\"speech_id\",\"nunique\"),\n",
    "                         first_date=(\"date\",\"min\"),\n",
    "                         last_date=(\"date\",\"max\"))\n",
    "                    .reset_index())\n",
    "\n",
    "shares_country_win = stance_shares(speech_win)\n",
    "\n",
    "country_window = (base_country_win\n",
    "                  .merge(shares_country_win, on=\"country\", how=\"left\")\n",
    "                  .sort_values([\"mean_stance\",\"n_speeches\"], ascending=[False, False])\n",
    "                  .reset_index(drop=True))\n",
    "\n",
    "country_window_path = OUTDIR / \"country_overall_stance_2016_2024.csv\"\n",
    "country_window.to_csv(country_window_path, index=False)\n",
    "print(f\"âœ“ Wrote {country_window_path.name} ({len(country_window)} countries)\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) (Optional) Quick views\n",
    "# -------------------------\n",
    "# Top 10 by mean stance (overall)\n",
    "display(country_overall.head(10))\n",
    "\n",
    "# Most active 10 by speeches (overall)\n",
    "display(country_overall.sort_values(\"n_speeches\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8fb084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Wrote merged_cbdc_country_with_auer.csv with 57 rows.\n",
      "\n",
      "Countries from OUR table with no match in Auer (fix by extending manual_map and re-run):\n",
      "country_our                   key\n",
      "  Euro Area         euro area ecb\n",
      "      Korea korea the republic of\n",
      "Philippines       philippines the\n"
     ]
    }
   ],
   "source": [
    "# === Merge \"our\" country stance table with Auerâ€“Cornelliâ€“Frost country table ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# --------- INPUTS (edit paths if needed) ----------\n",
    "OUR_CSV  = \"country_overall_stance_2016_2024.csv\"   # from the earlier step\n",
    "AUER_CSV = \"work880_data_mar2024.csv\"               # your exported Auer CSV\n",
    "\n",
    "OUTFILE  = \"merged_cbdc_country_with_auer.csv\"\n",
    "\n",
    "# --------- LOAD ----------\n",
    "our  = pd.read_csv(OUR_CSV)   # expects columns: country, mean_stance, n_speeches, anti/wait/pro_share, first_date, last_date\n",
    "auer = pd.read_csv(AUER_CSV)  # expects columns like: ISO2,country_name,...,central_bankers_speech_stance_index\n",
    "\n",
    "# --------- NORMALIZE COLUMN NAMES ----------\n",
    "def norm_col(s):\n",
    "    s = str(s).strip().lower()\n",
    "    for ch in [\"(\",\")\",\"[\",\"]\",\"{\",\"}\",\"/\",\"\\\\\",\",\",\".\",\";\",\"'\",\"â€™\",\"-\"]:\n",
    "        s = s.replace(ch, \" \")\n",
    "    s = \" \".join(s.split())\n",
    "    return s.replace(\" \", \"_\")\n",
    "\n",
    "auer.columns = [norm_col(c) for c in auer.columns]\n",
    "\n",
    "# pick columns we want\n",
    "keep = [\"iso2\", \"country_name\",\n",
    "        \"project_score_overall\",\"project_score_retail\",\"project_score_wholesale\",\n",
    "        \"search_interest\",\"search_interest_normalized\",\n",
    "        \"central_bankers_speech_stance_index\",\"central_bankers_speech_stance_index_normalized\"]\n",
    "keep = [c for c in keep if c in auer.columns]\n",
    "auer = auer[keep].copy()\n",
    "\n",
    "# --------- COUNTRY KEY NORMALIZATION ----------\n",
    "def strip_accents(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    return \"\".join(ch for ch in unicodedata.normalize(\"NFKD\", str(s)) if not unicodedata.combining(ch))\n",
    "\n",
    "def norm_country(s):\n",
    "    s = strip_accents(s).lower().strip()\n",
    "    # drop \"(the)\" etc.\n",
    "    for tok in [\" (the)\", \"(the)\", \"(of the)\", \"(of)\"]:\n",
    "        s = s.replace(tok, \"\")\n",
    "    s = s.replace(\"&\", \"and\")\n",
    "    for ch in [\",\", \".\", \";\", \"â€™\", \"'\"]:\n",
    "        s = s.replace(ch, \" \")\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "our[\"key\"]  = our[\"country\"].map(norm_country)\n",
    "auer[\"key\"] = auer[\"country_name\"].map(norm_country)\n",
    "\n",
    "# manual mapping for names that differ between the two lists\n",
    "manual_map = {\n",
    "    # abbreviations / long forms\n",
    "    \"uae\": \"united arab emirates\",\n",
    "    \"us\": \"united states of america\",\n",
    "    \"uk\": \"united kingdom of great britain and northern ireland\",\n",
    "    # variants\n",
    "    \"russia\": \"russian federation\",\n",
    "    \"czech republic\": \"czechia\",\n",
    "    \"netherlands\": \"netherlands\",\n",
    "    \"euro area\": \"euro area ecb\",\n",
    "    \"hong kong\": \"hong kong sar\",\n",
    "    \"korea\": \"korea the republic of\",\n",
    "    \"curacao and sint maarten\": \"curacao and sint maarten\",\n",
    "    \"eswatini\": \"swaziland\",\n",
    "    \"pakistan\": \"islamic republic of pakistan\",\n",
    "    \"philippines\": \"philippines the\",\n",
    "}\n",
    "\n",
    "our[\"key\"] = our[\"key\"].replace(manual_map)\n",
    "\n",
    "# --------- MERGE ----------\n",
    "merged = (our.merge(auer, on=\"key\", how=\"left\", suffixes=(\"_our\",\"_auer\"))\n",
    "            .rename(columns={\n",
    "                \"country\":\"country_our\",\n",
    "                \"country_name\":\"country_auer\",\n",
    "                \"central_bankers_speech_stance_index\":\"auer_stance_raw\",\n",
    "                \"central_bankers_speech_stance_index_normalized\":\"auer_stance_norm\",\n",
    "            }))\n",
    "\n",
    "# stance scale for our series: -1..+1 -> 0..1\n",
    "merged[\"stance01_our\"] = (merged[\"mean_stance\"] + 1) / 2.0\n",
    "\n",
    "# reorder columns for readability\n",
    "cols = [\n",
    "    \"key\",\"country_our\",\"country_auer\",\"iso2\",\n",
    "    \"mean_stance\",\"stance01_our\",\"n_speeches\",\"anti_share\",\"wait_share\",\"pro_share\",\n",
    "    \"auer_stance_raw\",\"auer_stance_norm\",\n",
    "    \"project_score_overall\",\"project_score_retail\",\"project_score_wholesale\",\n",
    "    \"search_interest\",\"search_interest_normalized\",\n",
    "    \"first_date\",\"last_date\"\n",
    "]\n",
    "cols = [c for c in cols if c in merged.columns]\n",
    "merged = merged[cols].sort_values(\"country_our\")\n",
    "\n",
    "# write file\n",
    "Path(\".\").joinpath(OUTFILE).write_text(\"\")  # ensure path exists\n",
    "merged.to_csv(OUTFILE, index=False)\n",
    "print(f\"âœ“ Wrote {OUTFILE} with {len(merged)} rows.\")\n",
    "\n",
    "# quick diagnostics: who didn't match?\n",
    "unmatched = merged[merged[\"country_auer\"].isna()][[\"country_our\",\"key\"]]\n",
    "if len(unmatched):\n",
    "    print(\"\\nCountries from OUR table with no match in Auer (fix by extending manual_map and re-run):\")\n",
    "    print(unmatched.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nAll countries matched successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cb4e023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auer keys near targets:\n",
      "\n",
      "[euro] -> 1 hits\n",
      "   euro area (ecb)\n",
      "\n",
      "[korea] -> 2 hits\n",
      "   korea (the democratic people s republic of)\n",
      "   korea (the republic of)\n",
      "\n",
      "[philipp] -> 1 hits\n",
      "   philippines\n",
      "\n",
      "Remaining unmatched after patch:\n",
      "country_our                   key\n",
      "      Korea korea the republic of\n",
      "Philippines       philippines the\n",
      "  Euro Area         euro area ecb\n",
      "\n",
      "âœ“ Overwrote merged_cbdc_country_with_auer.csv with 57 rows\n"
     ]
    }
   ],
   "source": [
    "# --- Patch the 3 unmatched: inspect Auer keys, extend map, re-merge ---\n",
    "\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# 1) Inspect what's in Auer for these patterns (to see exact normalized keys)\n",
    "targets = [\"euro\", \"korea\", \"philipp\"]\n",
    "print(\"Auer keys near targets:\")\n",
    "for t in targets:\n",
    "    hits = sorted({k for k in auer[\"key\"].dropna().unique() if t in k})\n",
    "    print(f\"\\n[{t}] -> {len(hits)} hits\")\n",
    "    for h in hits[:20]:\n",
    "        print(\"  \", h)\n",
    "\n",
    "# 2) Extend manual map to the most likely canonical Auer keys\n",
    "#    (These are the normalized forms produced by our norm_country() on your Auer CSV.)\n",
    "more_map = {\n",
    "    \"euro area\": \"euro area ecb\",                  # Euro area (ECB)\n",
    "    \"korea\": \"korea the republic of\",              # Korea (the Republic of)\n",
    "    \"philippines\": \"philippines the\",              # Philippines (the)\n",
    "}\n",
    "\n",
    "# Apply on OUR side and re-merge cleanly\n",
    "our2 = our.copy()\n",
    "our2[\"key\"] = our2[\"key\"].replace(more_map)\n",
    "\n",
    "merged2 = (our2.merge(auer, on=\"key\", how=\"left\", suffixes=(\"_our\",\"_auer\"))\n",
    "                 .rename(columns={\n",
    "                     \"country\":\"country_our\",\n",
    "                     \"country_name\":\"country_auer\",\n",
    "                     \"central_bankers_speech_stance_index\":\"auer_stance_raw\",\n",
    "                     \"central_bankers_speech_stance_index_normalized\":\"auer_stance_norm\",\n",
    "                 }))\n",
    "\n",
    "# Report any remaining gaps\n",
    "still_unmatched = merged2[merged2[\"country_auer\"].isna()][[\"country_our\",\"key\"]]\n",
    "print(\"\\nRemaining unmatched after patch:\")\n",
    "print(still_unmatched.to_string(index=False) if len(still_unmatched) else \"None ðŸŽ‰\")\n",
    "\n",
    "# Rebuild stance01 and save the final merged file (overwrite)\n",
    "merged2[\"stance01_our\"] = (merged2[\"mean_stance\"] + 1) / 2.0\n",
    "cols = [\n",
    "    \"key\",\"country_our\",\"country_auer\",\"iso2\",\n",
    "    \"mean_stance\",\"stance01_our\",\"n_speeches\",\"anti_share\",\"wait_share\",\"pro_share\",\n",
    "    \"auer_stance_raw\",\"auer_stance_norm\",\n",
    "    \"project_score_overall\",\"project_score_retail\",\"project_score_wholesale\",\n",
    "    \"search_interest\",\"search_interest_normalized\",\n",
    "    \"first_date\",\"last_date\"\n",
    "]\n",
    "cols = [c for c in cols if c in merged2.columns]\n",
    "merged2 = merged2[cols].sort_values(\"country_our\")\n",
    "\n",
    "merged2.to_csv(OUTFILE, index=False)\n",
    "print(f\"\\nâœ“ Overwrote {OUTFILE} with {len(merged2)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f7b02da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap countries: 54\n",
      "Pearson  : 0.408\n",
      "Spearman : 0.331\n",
      "Kendall  : 0.307\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ranked differences to compare_out\\country_differences.csv\n",
      "\n",
      "Agreement matrix (Ï„=0.5 cut):\n",
      "our_bucket    cautious/opp  supportive\n",
      "auer_bucket                           \n",
      "cautious/opp             2          31\n",
      "supportive               0          21\n",
      "\n",
      "Robustness (n_speeches â‰¥ 10): N=14 â€” Pearson 0.842, Spearman 0.824, Kendall 0.648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Comparison of our stance vs Auerâ€“Cornelliâ€“Frost (country level) ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "INFILE = \"merged_cbdc_country_with_auer.csv\"\n",
    "OUTDIR = Path(\"compare_out\"); OUTDIR.mkdir(exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(INFILE)\n",
    "\n",
    "# keep overlap rows with both series\n",
    "comp = df.copy()\n",
    "# our stance in [0,1]\n",
    "if \"stance01_our\" not in comp.columns and \"mean_stance\" in comp.columns:\n",
    "    comp[\"stance01_our\"] = (comp[\"mean_stance\"] + 1) / 2.0\n",
    "\n",
    "# pick Auer stance: prefer raw 0..1; fallback to minâ€“max of normalized if needed\n",
    "if \"auer_stance_raw\" in comp.columns and comp[\"auer_stance_raw\"].notna().any():\n",
    "    comp[\"stance_auer_use\"] = pd.to_numeric(comp[\"auer_stance_raw\"], errors=\"coerce\")\n",
    "elif \"auer_stance_norm\" in comp.columns and comp[\"auer_stance_norm\"].notna().any():\n",
    "    tmp = pd.to_numeric(comp[\"auer_stance_norm\"], errors=\"coerce\")\n",
    "    comp[\"stance_auer_use\"] = (tmp - tmp.min()) / (tmp.max() - tmp.min())\n",
    "else:\n",
    "    raise ValueError(\"No usable Auer stance column found.\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "comp = comp[np.isfinite(comp[\"stance01_our\"]) & np.isfinite(comp[\"stance_auer_use\"])].copy()\n",
    "\n",
    "# Keep helpful labels\n",
    "comp[\"country\"] = comp[\"country_our\"].fillna(comp[\"country_auer\"])\n",
    "\n",
    "# --------- 1) Correlations ----------\n",
    "pear   = comp[\"stance01_our\"].corr(comp[\"stance_auer_use\"], method=\"pearson\")\n",
    "spear  = comp[\"stance01_our\"].corr(comp[\"stance_auer_use\"], method=\"spearman\")\n",
    "kend   = comp[\"stance01_our\"].corr(comp[\"stance_auer_use\"], method=\"kendall\")\n",
    "\n",
    "print(f\"Overlap countries: {comp['country'].nunique()}\")\n",
    "print(f\"Pearson  : {pear:.3f}\")\n",
    "print(f\"Spearman : {spear:.3f}\")\n",
    "print(f\"Kendall  : {kend:.3f}\")\n",
    "\n",
    "# Save a tiny summary\n",
    "(Path(OUTDIR) / \"summary.txt\").write_text(\n",
    "    f\"Overlap countries: {comp['country'].nunique()}\\n\"\n",
    "    f\"Pearson : {pear:.3f}\\n\"\n",
    "    f\"Spearman: {spear:.3f}\\n\"\n",
    "    f\"Kendall : {kend:.3f}\\n\"\n",
    ")\n",
    "\n",
    "# --------- 2) Scatter with 45Â° reference and labels ----------\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(comp[\"stance_auer_use\"], comp[\"stance01_our\"], s=28)\n",
    "mn = 0; mx = 1\n",
    "plt.plot([mn,mx],[mn,mx], linestyle=\"--\")  # 45Â° line\n",
    "plt.xlim(mn, mx); plt.ylim(mn, mx)\n",
    "plt.xlabel(\"Auer stance (0â€“1)\")\n",
    "plt.ylabel(\"Our stance (0â€“1)\")\n",
    "plt.title(f\"Our vs Auer stance â€” r={pear:.2f}, Ï={spear:.2f}\")\n",
    "# light labeling for most deviant points\n",
    "comp[\"abs_diff\"] = (comp[\"stance01_our\"] - comp[\"stance_auer_use\"]).abs()\n",
    "for _, r in comp.sort_values(\"abs_diff\", ascending=False).head(12).iterrows():\n",
    "    plt.annotate(r[\"country\"], (r[\"stance_auer_use\"], r[\"stance01_our\"]),\n",
    "                 xytext=(3,3), textcoords=\"offset points\", fontsize=8)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"scatter_our_vs_auer.png\", dpi=220)\n",
    "plt.show()\n",
    "\n",
    "# --------- 3) Rank countries by difference ----------\n",
    "rank = (comp[[\"country\",\"stance01_our\",\"stance_auer_use\",\"n_speeches\",\"pro_share\",\"wait_share\",\"anti_share\"]]\n",
    "        .assign(diff=lambda d: d[\"stance01_our\"] - d[\"stance_auer_use\"],\n",
    "                abs_diff=lambda d: d[\"diff\"].abs())\n",
    "        .sort_values(\"abs_diff\", ascending=False))\n",
    "\n",
    "rank.to_csv(OUTDIR / \"country_differences.csv\", index=False)\n",
    "print(f\"Saved ranked differences to {OUTDIR/'country_differences.csv'}\")\n",
    "\n",
    "# --------- 4) Simple agreement matrix (threshold can be tuned) ----------\n",
    "THR_OUR  = 0.5\n",
    "THR_AUER = 0.5\n",
    "\n",
    "comp[\"our_bucket\"]  = np.where(comp[\"stance01_our\"]  >= THR_OUR, \"supportive\", \"cautious/opp\")\n",
    "comp[\"auer_bucket\"] = np.where(comp[\"stance_auer_use\"]>= THR_AUER, \"supportive\", \"cautious/opp\")\n",
    "\n",
    "cmat = (comp.groupby([\"auer_bucket\",\"our_bucket\"]).size()\n",
    "          .rename(\"n\").reset_index()\n",
    "          .pivot(index=\"auer_bucket\", columns=\"our_bucket\", values=\"n\").fillna(0).astype(int))\n",
    "\n",
    "print(\"\\nAgreement matrix (Ï„=0.5 cut):\")\n",
    "print(cmat)\n",
    "\n",
    "cmat.to_csv(OUTDIR / \"agreement_matrix.csv\")\n",
    "\n",
    "# --------- 5) (Optional) Robustness: restrict to countries with >= K speeches ----------\n",
    "K = 10\n",
    "rob = comp[comp[\"n_speeches\"] >= K].copy()\n",
    "pear_k   = rob[\"stance01_our\"].corr(rob[\"stance_auer_use\"], method=\"pearson\") if len(rob) else np.nan\n",
    "spear_k  = rob[\"stance01_our\"].corr(rob[\"stance_auer_use\"], method=\"spearman\") if len(rob) else np.nan\n",
    "kend_k   = rob[\"stance01_our\"].corr(rob[\"stance_auer_use\"], method=\"kendall\") if len(rob) else np.nan\n",
    "print(f\"\\nRobustness (n_speeches â‰¥ {K}): N={len(rob)} â€” Pearson {pear_k:.3f}, Spearman {spear_k:.3f}, Kendall {kend_k:.3f}\")\n",
    "(Path(OUTDIR) / f\"robust_nge{K}.txt\").write_text(\n",
    "    f\"N={len(rob)}\\nPearson {pear_k:.3f}\\nSpearman {spear_k:.3f}\\nKendall {kend_k:.3f}\\n\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
