{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfe6f02",
   "metadata": {},
   "source": [
    "# Statistics for CBDC Sentence Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c100c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_17016\\3833511645.py:28: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[\"date\"]    = pd.to_datetime(df[\"date\"], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📒 Wrote Excel workbook → results\\cbdc_descriptives.xlsx\n",
      "✅ Saved: results\\top_authors.svg\n",
      "✅ Descriptives complete. Tables & figures saved in: results\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Descriptive Statistics for CBDC Sentence Dataset\n",
    "# ============================================================\n",
    "# Input schema expected: url, cbdc_sentence, title, description, date,\n",
    "#                        author, affiliation, position, country\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ========== TASK 0. Imports & Paths =========================================\n",
    "import os, re, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "IN_FILE  = \"cbdc-dataset-final.csv\"\n",
    "OUT_DIR  = \"results\"                     # <-- per request\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ========== TASK 1. Load & Basic Hygiene ====================================\n",
    "df = pd.read_csv(IN_FILE)\n",
    "\n",
    "# Standardize text columns\n",
    "for col in [\"cbdc_sentence\",\"title\",\"description\",\"author\",\"affiliation\",\"position\",\"country\",\"url\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip().replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "        df[col] = df[col].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "# Dates & calendar cuts\n",
    "df[\"date\"]    = pd.to_datetime(df[\"date\"], errors=\"coerce\", infer_datetime_format=True)\n",
    "df[\"year\"]    = df[\"date\"].dt.year\n",
    "df[\"month\"]   = df[\"date\"].dt.month\n",
    "df[\"quarter\"] = df[\"date\"].dt.to_period(\"Q\").astype(str)\n",
    "\n",
    "# Sentence length features\n",
    "df[\"tokens\"] = df[\"cbdc_sentence\"].fillna(\"\").str.split().str.len()\n",
    "df[\"chars\"]  = df[\"cbdc_sentence\"].fillna(\"\").str.len()\n",
    "\n",
    "# ========== TASK 2. Overview & Missingness ==================================\n",
    "overview = pd.DataFrame({\n",
    "    \"rows (sentences)\":            [len(df)],\n",
    "    \"unique URLs (speeches)\":      [df[\"url\"].nunique()],\n",
    "    \"date coverage (min→max)\":     [f\"{df['date'].min():%Y-%m-%d} → {df['date'].max():%Y-%m-%d}\"],\n",
    "    \"unique authors\":              [df[\"author\"].nunique(dropna=True)],\n",
    "    \"unique affiliations\":         [df[\"affiliation\"].nunique(dropna=True)],\n",
    "    \"unique positions\":            [df[\"position\"].nunique(dropna=True)],\n",
    "    \"unique countries\":            [df[\"country\"].nunique(dropna=True)],\n",
    "    \"median tokens / sentence\":    [int(df[\"tokens\"].median())],\n",
    "    \"p90 tokens / sentence\":       [int(df[\"tokens\"].quantile(0.90))],\n",
    "}).T.reset_index().rename(columns={\"index\": \"metric\", 0: \"value\"})\n",
    "\n",
    "missing = (df[[\"title\",\"description\",\"date\",\"author\",\"affiliation\",\"position\",\"country\"]]\n",
    "           .isna().mean().mul(100).round(1)\n",
    "           .rename(\"missing_percent\").reset_index()\n",
    "           .rename(columns={\"index\":\"column\"}))\n",
    "\n",
    "# ========== TASK 3. Time Series (Year / Quarter) ============================\n",
    "by_year = (df.groupby(\"year\")\n",
    "             .agg(sentences=(\"url\",\"count\"),\n",
    "                  speeches=(\"url\",\"nunique\"))\n",
    "             .reset_index()\n",
    "             .sort_values(\"year\"))\n",
    "\n",
    "by_quarter = (df.groupby(\"quarter\")\n",
    "                .agg(sentences=(\"url\",\"count\"),\n",
    "                     speeches=(\"url\",\"nunique\"))\n",
    "                .reset_index()\n",
    "                .sort_values(\"quarter\"))\n",
    "\n",
    "# ========== TASK 4. Entity Frequencies ======================================\n",
    "def freq_table(series, k=20, dropna=True, name=None):\n",
    "    s = series if not dropna else series.dropna()\n",
    "    counts = s.value_counts()\n",
    "    out = (pd.DataFrame({\"count\": counts, \"share_%\": (counts / counts.sum() * 100).round(2)})\n",
    "           .reset_index()\n",
    "           .rename(columns={\"index\": name or series.name}))\n",
    "    return out.head(k)\n",
    "\n",
    "top_authors = (df.groupby(\"author\")\n",
    "                 .agg(sentences=(\"url\",\"count\"),\n",
    "                      speeches=(\"url\",\"nunique\"),\n",
    "                      first_year=(\"year\",\"min\"),\n",
    "                      last_year=(\"year\",\"max\"),\n",
    "                      avg_tokens=(\"tokens\",\"mean\"))\n",
    "                 .reset_index()\n",
    "                 .sort_values([\"sentences\",\"speeches\"], ascending=False)\n",
    "                 .head(25))\n",
    "top_authors[\"avg_tokens\"] = top_authors[\"avg_tokens\"].round(1)\n",
    "\n",
    "top_affiliations = (df.groupby(\"affiliation\")\n",
    "                     .agg(sentences=(\"url\",\"count\"),\n",
    "                          speeches=(\"url\",\"nunique\"),\n",
    "                          countries=(\"country\", pd.Series.nunique))\n",
    "                     .reset_index()\n",
    "                     .sort_values([\"sentences\",\"speeches\"], ascending=False)\n",
    "                     .head(25))\n",
    "\n",
    "top_positions = freq_table(df[\"position\"], k=25, name=\"position\", dropna=True)\n",
    "top_countries = freq_table(df[\"country\"], k=25, name=\"country\", dropna=True)\n",
    "\n",
    "# ========== TASK 5. Affiliation × Country Crosstab (SAFE) ===================\n",
    "# Avoid KeyError from mis-using row labels as column names in sort_values\n",
    "tmp = df.copy()\n",
    "tmp[\"country\"] = tmp[\"country\"].fillna(\"Unknown\")\n",
    "ct = pd.crosstab(tmp[\"affiliation\"], tmp[\"country\"])\n",
    "\n",
    "# Sort rows by totals; sort columns by totals\n",
    "ct[\"__row_total__\"] = ct.sum(axis=1)\n",
    "ct = ct.sort_values(\"__row_total__\", ascending=False)\n",
    "col_order = ct.drop(columns=\"__row_total__\").sum(axis=0).sort_values(ascending=False).index\n",
    "ct = ct.loc[:, list(col_order) + [\"__row_total__\"]]\n",
    "\n",
    "N = 20\n",
    "aff_country_top = ct.head(N).reset_index().rename(columns={\"affiliation\":\"Affiliation\",\"__row_total__\":\"Total\"})\n",
    "\n",
    "# ========== TASK 6. Sentence-Length Distributions ===========================\n",
    "def describe_length(col):\n",
    "    return (df[col].describe(percentiles=[.1,.25,.5,.75,.9,.95,.99])\n",
    "              .round(2).to_frame(name=col).reset_index()\n",
    "              .rename(columns={\"index\":\"stat\"}))\n",
    "\n",
    "len_tokens = describe_length(\"tokens\")\n",
    "len_chars  = describe_length(\"chars\")\n",
    "\n",
    "# ========== TASK 7. N-gram Summaries (Uni/Bi/Tri) ===========================\n",
    "STOP = {\n",
    "    \"a\",\"an\",\"the\",\"and\",\"or\",\"for\",\"of\",\"to\",\"in\",\"on\",\"at\",\"with\",\"by\",\"from\",\"as\",\"that\",\"this\",\n",
    "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"it\",\"its\",\"into\",\"than\",\"then\",\"so\",\"such\",\"which\",\n",
    "    \"will\",\"would\",\"should\",\"could\",\"may\",\"might\",\"can\",\"cannot\",\"not\",\"no\",\"do\",\"does\",\"did\",\"doing\",\n",
    "    \"we\",\"our\",\"us\",\"you\",\"your\",\"they\",\"their\",\"them\",\"i\",\"he\",\"she\",\"his\",\"her\",\"him\",\"mr\",\"mrs\",\"ms\",\n",
    "    \"central\",\"bank\",\"digital\",\"currency\",\"cbdc\",\"banks\",\"monetary\",\"policy\",\"speech\",\"hong\",\"kong\",\n",
    "    \"china\",\"people\",\"banking\",\"system\",\"economy\",\"financial\"\n",
    "}\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return re.findall(r\"[A-Za-z']+\", str(s).lower())\n",
    "\n",
    "def top_ngrams(series, n=1, top=50):\n",
    "    counter = Counter()\n",
    "    for s in series.dropna():\n",
    "        toks = [t for t in tokenize(s) if t not in STOP]\n",
    "        if n == 1:\n",
    "            counter.update(toks)\n",
    "        else:\n",
    "            if len(toks) >= n:\n",
    "                grams = zip(*[toks[i:] for i in range(n)])\n",
    "                counter.update((\" \".join(g),) for g in grams)\n",
    "    dfc = (pd.DataFrame(counter.most_common(top), columns=[f\"{n}-gram\",\"count\"])\n",
    "             .assign(**{\"share_%\": lambda d: (d[\"count\"]/d[\"count\"].sum()*100).round(2)}))\n",
    "    return dfc\n",
    "\n",
    "top_unigrams = top_ngrams(df[\"cbdc_sentence\"], n=1, top=50)\n",
    "top_bigrams  = top_ngrams(df[\"cbdc_sentence\"], n=2, top=50)\n",
    "top_trigrams = top_ngrams(df[\"cbdc_sentence\"], n=3, top=50)\n",
    "\n",
    "# ========== TASK 8. Country × Year Matrix ===================================\n",
    "country_year = (pd.crosstab(df[\"country\"], df[\"year\"])\n",
    "                .loc[lambda t: t.sum(axis=1).sort_values(ascending=False).index]\n",
    "                .reset_index())\n",
    "\n",
    "# ========== TASK 9. Persist Tables (CSV + Excel) ============================\n",
    "tables = {\n",
    "    \"00_overview.csv\":                 overview,\n",
    "    \"01_missingness.csv\":              missing,\n",
    "    \"02_by_year.csv\":                  by_year,\n",
    "    \"03_by_quarter.csv\":               by_quarter,\n",
    "    \"04_top_authors.csv\":              top_authors,\n",
    "    \"05_top_affiliations.csv\":         top_affiliations,\n",
    "    \"06_top_positions.csv\":            top_positions,\n",
    "    \"07_top_countries.csv\":            top_countries,\n",
    "    \"08_affiliation_x_country.csv\":    aff_country_top,\n",
    "    \"09_len_tokens.csv\":               len_tokens,\n",
    "    \"10_len_chars.csv\":                len_chars,\n",
    "    \"11_top_unigrams.csv\":             top_unigrams,\n",
    "    \"12_top_bigrams.csv\":              top_bigrams,\n",
    "    \"13_top_trigrams.csv\":             top_trigrams,\n",
    "    \"14_country_year_matrix.csv\":      country_year,\n",
    "}\n",
    "\n",
    "for fname, frame in tables.items():\n",
    "    frame.to_csv(os.path.join(OUT_DIR, fname), index=False)\n",
    "\n",
    "# Also bundle as a single Excel workbook (if openpyxl available)\n",
    "try:\n",
    "    import openpyxl\n",
    "    with pd.ExcelWriter(os.path.join(OUT_DIR, \"cbdc_descriptives.xlsx\"), engine=\"openpyxl\") as xl:\n",
    "        for sheet, frame in {\n",
    "            \"Overview\": overview,\n",
    "            \"Missingness\": missing,\n",
    "            \"ByYear\": by_year,\n",
    "            \"ByQuarter\": by_quarter,\n",
    "            \"TopAuthors\": top_authors,\n",
    "            \"TopAffiliations\": top_affiliations,\n",
    "            \"TopPositions\": top_positions,\n",
    "            \"TopCountries\": top_countries,\n",
    "            \"Affiliations×Country\": aff_country_top,\n",
    "            \"LenTokens\": len_tokens,\n",
    "            \"LenChars\": len_chars,\n",
    "            \"Unigrams\": top_unigrams,\n",
    "            \"Bigrams\": top_bigrams,\n",
    "            \"Trigrams\": top_trigrams,\n",
    "            \"Country×Year\": country_year,\n",
    "        }.items():\n",
    "            frame.to_excel(xl, sheet_name=sheet, index=False)\n",
    "    print(f\"📒 Wrote Excel workbook → {os.path.join(OUT_DIR, 'cbdc_descriptives.xlsx')}\")\n",
    "except Exception as e:\n",
    "    print(\"Excel export skipped:\", e)\n",
    "\n",
    "# ============================\n",
    "# Top Authors (horizontal bar)\n",
    "# ============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUT_DIR = \"results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- build an author list that splits multi-author rows ---\n",
    "# assumes your full dataset is already loaded in `df`\n",
    "auth_col = \"author\"\n",
    "text_id  = \"url\"   # counts based on sentences; keep as 'url' if you want speeches use nunique later\n",
    "\n",
    "# split on semicolon, strip whitespace, drop empties, and explode\n",
    "auth_series = (df[auth_col]\n",
    "               .dropna()\n",
    "               .astype(str)\n",
    "               .str.split(\";\")\n",
    "               .explode()\n",
    "               .str.strip())\n",
    "auth_series = auth_series[auth_series.ne(\"\")]\n",
    "\n",
    "# count sentences per author\n",
    "topk = 15  # change to 20/25 if you want a longer bar chart\n",
    "author_counts = (auth_series\n",
    "                 .to_frame(name=\"author\")\n",
    "                 .assign(sentences=1)\n",
    "                 .groupby(\"author\", as_index=False)[\"sentences\"].sum()\n",
    "                 .sort_values(\"sentences\", ascending=False)\n",
    "                 .head(topk))\n",
    "# --- plot (horizontal bar) ---\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "plt.barh(author_counts[\"author\"][::-1], author_counts[\"sentences\"][::-1])\n",
    "plt.xlabel(\"Sentences\")\n",
    "plt.title(\"Top Authors by CBDC Mentions\")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = os.path.join(OUT_DIR, \"top_authors.svg\")\n",
    "plt.savefig(out_path, format=\"svg\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"✅ Saved: {out_path}\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Top Authors (horizontal bar) by speeches\n",
    "# ============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUT_DIR = \"results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- build an author list that splits multi-author rows ---\n",
    "# assumes your full dataset is already loaded in `df`\n",
    "auth_col = \"author\"\n",
    "text_id  = \"url\"   # counts based on sentences; keep as 'url' if you want speeches use nunique later\n",
    "\n",
    "# split on semicolon, strip whitespace, drop empties, and explode\n",
    "auth_series = (df[auth_col]\n",
    "               .dropna()\n",
    "               .astype(str)\n",
    "               .str.split(\";\")\n",
    "               .explode()\n",
    "               .str.strip())\n",
    "auth_series = auth_series[auth_series.ne(\"\")]\n",
    "\n",
    "#unique speeches per author\n",
    "tmp = (df.assign(author_list=df[\"author\"].fillna(\"\").astype(str).str.split(\";\"))\n",
    "          .explode(\"author_list\"))\n",
    "author_counts = (tmp.assign(author_list=tmp[\"author_list\"].str.strip())\n",
    "                      .query(\"author_list != ''\")\n",
    "                      .groupby(\"author_list\")[\"url\"].nunique()\n",
    "                      .reset_index(name=\"speeches\")\n",
    "                      .sort_values(\"speeches\", ascending=False)\n",
    "                      .head(topk)\n",
    "                 .rename(columns={\"author_list\":\"author\", \"speeches\":\"sentences\"}))\n",
    "\n",
    "# --- plot (horizontal bar) ---\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "plt.barh(author_counts[\"author\"][::-1], author_counts[\"sentences\"][::-1])\n",
    "plt.xlabel(\"Speeches\")\n",
    "plt.title(\"Top Authors by CBDC Mentions\")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = os.path.join(OUT_DIR, \"top_authors_by_speeches.svg\")\n",
    "plt.savefig(out_path, format=\"svg\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"✅ Saved: {out_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# ========== TASK 10. Figures (SVG) ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Time series (sentences & speeches per year)\n",
    "fig = plt.figure()\n",
    "plt.plot(by_year[\"year\"], by_year[\"sentences\"], label=\"Sentences\")\n",
    "plt.plot(by_year[\"year\"], by_year[\"speeches\"],  label=\"Speeches\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Count\"); plt.title(\"CBDC Mentions Over Time\"); plt.legend()\n",
    "plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"timeseries_year.svg\")); plt.close(fig)\n",
    "\n",
    "# Top affiliations (bar, top 15)\n",
    "ta15 = top_affiliations.head(15)\n",
    "fig = plt.figure()\n",
    "plt.barh(ta15[\"affiliation\"][::-1], ta15[\"sentences\"][::-1])\n",
    "plt.xlabel(\"Sentences\"); plt.title(\"Top Affiliations by CBDC Mentions\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"top_affiliations.svg\")); plt.close(fig)\n",
    "\n",
    "# Top countries (bar, top 15)\n",
    "tc15 = top_countries.head(15)\n",
    "fig = plt.figure()\n",
    "plt.barh(tc15[\"country\"][::-1], tc15[\"count\"][::-1])\n",
    "plt.xlabel(\"Sentences\"); plt.title(\"Top Countries by CBDC Mentions\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"top_countries.svg\")); plt.close(fig)\n",
    "\n",
    "# Affiliation × Country heatmap (top N rows; columns already sorted by totals)\n",
    "heat = ct.drop(columns=\"__row_total__\").head(N)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.imshow(heat.values, aspect=\"auto\")\n",
    "plt.yticks(range(heat.shape[0]), heat.index)\n",
    "plt.xticks(range(heat.shape[1]), heat.columns, rotation=45, ha=\"right\")\n",
    "plt.title(\"Affiliation × Country (Top 20 affiliations by CBDC mentions)\")\n",
    "plt.colorbar(label=\"Sentences\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"affiliation_country_heatmap.svg\"))\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"✅ Descriptives complete. Tables & figures saved in: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af7efc",
   "metadata": {},
   "source": [
    "# CBDC Sentence Classification Pipeline\n",
    "\n",
    "Applied four **domain-specialized BERT models** to classify BIS speech sentences on Central Bank Digital Currencies (CBDCs).\n",
    "The script runs in **Google Colab with GPU**, loads the input dataset, performs inference, and saves enriched results.\n",
    "\n",
    "## Models Used\n",
    "1. **CBDC-Type** → `Retail CBDC`, `Wholesale CBDC`, `General/Unspecified`\n",
    "2. **CBDC-Stance** → `Pro-CBDC`, `Wait-and-See`, `Anti-CBDC`\n",
    "3. **CBDC-Sentiment** → `Positive`, `Neutral`, `Negative`\n",
    "4. **CBDC-Discourse** → `Feature`, `Risk-Benefit`, `Process`\n",
    "\n",
    "Each model returns the **predicted label** and **confidence score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e045702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "HF Token): ··········\n",
      "Loaded 5376 sentences from: /content/drive/MyDrive/cbdc-analysis/cbdc-dataset-final.csv\n",
      "Using device: GPU\n",
      "Loading: Type -> bilalzafar/CBDC-Type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Stance -> bilalzafar/CBDC-Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Sentiment -> bilalzafar/CBDC-Sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Discourse -> bilalzafar/CBDC-Discourse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions for: Type\n",
      "Running predictions for: Stance\n",
      "Running predictions for: Sentiment\n",
      "Running predictions for: Discourse\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"print(f\\\"\\\\nSaved predictions to: {out_path}\\\")\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"cbdc_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"central bank digital currency: who can bank at the central bank?\",\n          \"and if a private-sector digital currency uses the technology to substitute for a third-party clearer, the central bank counterpart would do the opposite.\",\n          \"barrdear, j and kumhof, m (2016), \\\"the macroeconomics of central bank-issued digital currency\\\", bank of england working paper, forthcoming.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type_Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Wholesale CBDC\",\n          \"General/Unspecified\",\n          \"Retail CBDC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0501860603599214,\n        \"min\": 0.867669939994812,\n        \"max\": 0.998955249786377,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9972527623176575,\n          0.8904648423194885,\n          0.9979693293571472\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Stance_Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Wait-and-See\",\n          \"Anti-CBDC\",\n          \"Pro-CBDC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Stance_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03529067178971648,\n        \"min\": 0.902238667011261,\n        \"max\": 0.984924852848053,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.984924852848053,\n          0.902238667011261,\n          0.9796251058578491\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment_Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"neutral\",\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08391003511959477,\n        \"min\": 0.7376696467399597,\n        \"max\": 0.9889969825744629,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9889969825744629,\n          0.9107151031494141,\n          0.9795494675636292\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Discourse_Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Feature\",\n          \"Risk-Benefit\",\n          \"Process\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Discourse_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12501647359223872,\n        \"min\": 0.5994727611541748,\n        \"max\": 0.9992420673370361,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9991600513458252,\n          0.5994727611541748,\n          0.9992358684539795\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ad9286e3-cc2c-45c2-873b-0a1d758f8922\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbdc_sentence</th>\n",
       "      <th>Type_Label</th>\n",
       "      <th>Type_Score</th>\n",
       "      <th>Stance_Label</th>\n",
       "      <th>Stance_Score</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Discourse_Label</th>\n",
       "      <th>Discourse_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a \"central bank digital currency\" (cbdc) would...</td>\n",
       "      <td>Wholesale CBDC</td>\n",
       "      <td>0.996188</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.974996</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.977445</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.999242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and if a private-sector digital currency uses ...</td>\n",
       "      <td>General/Unspecified</td>\n",
       "      <td>0.890465</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.902239</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.910715</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.599473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and, as i tried to explain earlier, a cbdc tha...</td>\n",
       "      <td>Retail CBDC</td>\n",
       "      <td>0.998955</td>\n",
       "      <td>Anti-CBDC</td>\n",
       "      <td>0.948519</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.971240</td>\n",
       "      <td>Risk-Benefit</td>\n",
       "      <td>0.998474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>as far as its economic effects are concerned, ...</td>\n",
       "      <td>Retail CBDC</td>\n",
       "      <td>0.997053</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.983435</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.979650</td>\n",
       "      <td>Risk-Benefit</td>\n",
       "      <td>0.985957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as long as it's possible to hold something wit...</td>\n",
       "      <td>General/Unspecified</td>\n",
       "      <td>0.867670</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.917042</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.840400</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.998640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>barrdear, j and kumhof, m (2016), \"the macroec...</td>\n",
       "      <td>General/Unspecified</td>\n",
       "      <td>0.997969</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.979625</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.979549</td>\n",
       "      <td>Process</td>\n",
       "      <td>0.999236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ben broadbent: central banks and digital curre...</td>\n",
       "      <td>General/Unspecified</td>\n",
       "      <td>0.997384</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.975705</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.981172</td>\n",
       "      <td>Process</td>\n",
       "      <td>0.999203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>but as you widen that access, and the more clo...</td>\n",
       "      <td>Retail CBDC</td>\n",
       "      <td>0.998112</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.913568</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.987450</td>\n",
       "      <td>Risk-Benefit</td>\n",
       "      <td>0.938904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>central bank digital currency: who can bank at...</td>\n",
       "      <td>Retail CBDC</td>\n",
       "      <td>0.997253</td>\n",
       "      <td>Wait-and-See</td>\n",
       "      <td>0.984925</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.988997</td>\n",
       "      <td>Process</td>\n",
       "      <td>0.999160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>conversely, others see the distributed ledger ...</td>\n",
       "      <td>General/Unspecified</td>\n",
       "      <td>0.996235</td>\n",
       "      <td>Pro-CBDC</td>\n",
       "      <td>0.904902</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.737670</td>\n",
       "      <td>Risk-Benefit</td>\n",
       "      <td>0.994623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad9286e3-cc2c-45c2-873b-0a1d758f8922')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ad9286e3-cc2c-45c2-873b-0a1d758f8922 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ad9286e3-cc2c-45c2-873b-0a1d758f8922');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-8e75418a-9787-4740-9462-f3be2e3f0674\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8e75418a-9787-4740-9462-f3be2e3f0674')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-8e75418a-9787-4740-9462-f3be2e3f0674 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                       cbdc_sentence           Type_Label  \\\n",
       "0  a \"central bank digital currency\" (cbdc) would...       Wholesale CBDC   \n",
       "1  and if a private-sector digital currency uses ...  General/Unspecified   \n",
       "2  and, as i tried to explain earlier, a cbdc tha...          Retail CBDC   \n",
       "3  as far as its economic effects are concerned, ...          Retail CBDC   \n",
       "4  as long as it's possible to hold something wit...  General/Unspecified   \n",
       "5  barrdear, j and kumhof, m (2016), \"the macroec...  General/Unspecified   \n",
       "6  ben broadbent: central banks and digital curre...  General/Unspecified   \n",
       "7  but as you widen that access, and the more clo...          Retail CBDC   \n",
       "8  central bank digital currency: who can bank at...          Retail CBDC   \n",
       "9  conversely, others see the distributed ledger ...  General/Unspecified   \n",
       "\n",
       "   Type_Score  Stance_Label  Stance_Score Sentiment_Label  Sentiment_Score  \\\n",
       "0    0.996188  Wait-and-See      0.974996         neutral         0.977445   \n",
       "1    0.890465  Wait-and-See      0.902239         neutral         0.910715   \n",
       "2    0.998955     Anti-CBDC      0.948519        negative         0.971240   \n",
       "3    0.997053  Wait-and-See      0.983435         neutral         0.979650   \n",
       "4    0.867670  Wait-and-See      0.917042         neutral         0.840400   \n",
       "5    0.997969  Wait-and-See      0.979625         neutral         0.979549   \n",
       "6    0.997384  Wait-and-See      0.975705         neutral         0.981172   \n",
       "7    0.998112  Wait-and-See      0.913568        negative         0.987450   \n",
       "8    0.997253  Wait-and-See      0.984925         neutral         0.988997   \n",
       "9    0.996235      Pro-CBDC      0.904902        positive         0.737670   \n",
       "\n",
       "  Discourse_Label  Discourse_Score  \n",
       "0         Feature         0.999242  \n",
       "1         Feature         0.599473  \n",
       "2    Risk-Benefit         0.998474  \n",
       "3    Risk-Benefit         0.985957  \n",
       "4         Feature         0.998640  \n",
       "5         Process         0.999236  \n",
       "6         Process         0.999203  \n",
       "7    Risk-Benefit         0.938904  \n",
       "8         Process         0.999160  \n",
       "9    Risk-Benefit         0.994623  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved predictions to: /content/drive/MyDrive/cbdc-analysis/cbdc-dataset-predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Colab setup & dependencies\n",
    "# =========================\n",
    "# !pip -q install -U \"transformers>=4.41\" \"huggingface_hub>=0.23\" accelerate pandas\n",
    "\n",
    "import os, getpass, datetime as dt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline\n",
    "\n",
    "# -------------------------\n",
    "# 1) Mount Google Drive\n",
    "# -------------------------\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# -------------------------\n",
    "# 2) Paths & filenames\n",
    "# -------------------------\n",
    "DATA_DIR   = \"/content/drive/MyDrive/cbdc-analysis\"\n",
    "CSV_FILE   = \"cbdc-dataset-final.csv\"      # your file\n",
    "SENT_COL   = \"cbdc_sentence\"               # column containing sentences\n",
    "OUT_FILE   = f\"cbdc-dataset-predictions.csv\"\n",
    "\n",
    "in_path  = os.path.join(DATA_DIR, CSV_FILE)\n",
    "out_path = os.path.join(DATA_DIR, OUT_FILE)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Hugging Face login\n",
    "# -------------------------\n",
    "HF_TOKEN = getpass.getpass(\"HF Token): \")\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Load data (robust to comma or tab)\n",
    "# -------------------------\n",
    "def load_df(path):\n",
    "    try:\n",
    "        # Let pandas sniff the delimiter\n",
    "        df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "    except Exception:\n",
    "        # Common fallback: tab-separated\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "df = load_df(in_path)\n",
    "assert SENT_COL in df.columns, f\"Column '{SENT_COL}' not found. Available: {list(df.columns)}\"\n",
    "texts = df[SENT_COL].astype(str).fillna(\"\").tolist()\n",
    "print(f\"Loaded {len(texts)} sentences from: {in_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Models to run\n",
    "# -------------------------\n",
    "MODEL_IDS = {\n",
    "    \"Type\":      \"bilalzafar/CBDC-Type\",       # Retail / Wholesale / General\n",
    "    \"Stance\":    \"bilalzafar/CBDC-Stance\",     # Pro / Wait-and-See / Anti\n",
    "    \"Sentiment\": \"bilalzafar/CBDC-Sentiment\",  # negative / neutral / positive\n",
    "    \"Discourse\": \"bilalzafar/CBDC-Discourse\",  # Feature / Risk-Benefit / Process\n",
    "}\n",
    "\n",
    "# Prefer GPU if available\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Using device:\", \"GPU\" if DEVICE == 0 else \"CPU\")\n",
    "\n",
    "# Optional: half precision on GPU for speed\n",
    "model_kwargs = {\"torch_dtype\": torch.float16} if DEVICE == 0 else {}\n",
    "\n",
    "# -------------------------\n",
    "# 6) Build pipelines\n",
    "# -------------------------\n",
    "pipes = {}\n",
    "for name, repo in MODEL_IDS.items():\n",
    "    print(f\"Loading: {name} -> {repo}\")\n",
    "    pipes[name] = pipeline(\n",
    "        task=\"text-classification\",\n",
    "        model=repo,\n",
    "        device=DEVICE,\n",
    "        model_kwargs=model_kwargs\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# 7) Batched inference helper\n",
    "# -------------------------\n",
    "def predict_top(pipe, batch_texts, batch_size=64, max_length=320):\n",
    "    \"\"\"\n",
    "    Returns two lists (labels, scores) for a list of texts using a HF pipeline.\n",
    "    \"\"\"\n",
    "    labels, scores = [], []\n",
    "    for i in range(0, len(batch_texts), batch_size):\n",
    "        chunk = batch_texts[i:i+batch_size]\n",
    "        outputs = pipe(chunk, truncation=True, max_length=max_length, batch_size=batch_size)\n",
    "        # outputs is a list of dicts like {'label': 'Retail CBDC', 'score': 0.998}\n",
    "        for o in outputs:\n",
    "            labels.append(o[\"label\"])\n",
    "            scores.append(float(o[\"score\"]))\n",
    "    return labels, scores\n",
    "\n",
    "# -------------------------\n",
    "# 8) Run all four models\n",
    "# -------------------------\n",
    "results = {}\n",
    "for name, p in pipes.items():\n",
    "    print(f\"Running predictions for: {name}\")\n",
    "    lbls, scs = predict_top(p, texts, batch_size=64, max_length=320)\n",
    "    results[f\"{name}_Label\"] = lbls\n",
    "    results[f\"{name}_Score\"] = scs\n",
    "\n",
    "# -------------------------\n",
    "# 9) Merge & save\n",
    "# -------------------------\n",
    "pred_df = pd.DataFrame(results)\n",
    "out_df = pd.concat([df.reset_index(drop=True), pred_df], axis=1)\n",
    "\n",
    "# Preview a few rows\n",
    "display(out_df[[SENT_COL,\n",
    "                \"Type_Label\",\"Type_Score\",\n",
    "                \"Stance_Label\",\"Stance_Score\",\n",
    "                \"Sentiment_Label\",\"Sentiment_Score\",\n",
    "                \"Discourse_Label\",\"Discourse_Score\"]].head(10))\n",
    "\n",
    "# Save to Drive\n",
    "out_df.to_csv(out_path, index=False)\n",
    "print(f\"\\nSaved predictions to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfe510",
   "metadata": {},
   "source": [
    "# CBDC Predictions: Analysis & Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb95ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV: prediction_results\\type_label_counts.csv\n",
      "Saved CSV: prediction_results\\type_label_proportions.csv\n",
      "Saved SVG: prediction_results\\type_label_distribution_bar.svg\n",
      "Saved CSV: prediction_results\\stance_label_counts.csv\n",
      "Saved CSV: prediction_results\\stance_label_proportions.csv\n",
      "Saved SVG: prediction_results\\stance_label_distribution_bar.svg\n",
      "Saved CSV: prediction_results\\sentiment_label_counts.csv\n",
      "Saved CSV: prediction_results\\sentiment_label_proportions.csv\n",
      "Saved SVG: prediction_results\\sentiment_label_distribution_bar.svg\n",
      "Saved CSV: prediction_results\\discourse_label_counts.csv\n",
      "Saved CSV: prediction_results\\discourse_label_proportions.csv\n",
      "Saved SVG: prediction_results\\discourse_label_distribution_bar.svg\n",
      "Saved SVG: prediction_results\\type_score_hist.svg\n",
      "Saved SVG: prediction_results\\stance_score_hist.svg\n",
      "Saved SVG: prediction_results\\sentiment_score_hist.svg\n",
      "Saved SVG: prediction_results\\discourse_score_hist.svg\n",
      "Saved CSV: prediction_results\\low_confidence_type_thr_60.csv\n",
      "Saved CSV: prediction_results\\low_confidence_stance_thr_60.csv\n",
      "Saved CSV: prediction_results\\low_confidence_sentiment_thr_60.csv\n",
      "Saved CSV: prediction_results\\low_confidence_discourse_thr_60.csv\n",
      "Saved CSV: prediction_results\\low_confidence_summary.csv\n",
      "Saved CSV: prediction_results\\exemplars_top10_per_label_type.csv\n",
      "Saved CSV: prediction_results\\exemplars_top10_per_label_stance.csv\n",
      "Saved CSV: prediction_results\\exemplars_top10_per_label_sentiment.csv\n",
      "Saved CSV: prediction_results\\exemplars_top10_per_label_discourse.csv\n",
      "Saved CSV: prediction_results\\crosstab_type_by_discourse.csv\n",
      "Saved SVG: prediction_results\\heatmap_type_by_discourse.svg\n",
      "Saved CSV: prediction_results\\crosstab_stance_by_sentiment.csv\n",
      "Saved SVG: prediction_results\\heatmap_stance_by_sentiment.svg\n",
      "Saved CSV: prediction_results\\crosstab_type_by_stance.csv\n",
      "Saved SVG: prediction_results\\heatmap_type_by_stance.svg\n",
      "Saved CSV: prediction_results\\yearly_counts_by_type.csv\n",
      "Saved SVG: prediction_results\\yearly_counts_by_type_stacked.svg\n",
      "Saved CSV: prediction_results\\yearly_counts_by_discourse.csv\n",
      "Saved SVG: prediction_results\\yearly_counts_by_discourse_stacked.svg\n",
      "Saved CSV: prediction_results\\top_countries_overall.csv\n",
      "Saved SVG: prediction_results\\top_countries_overall_bar.svg\n",
      "Saved CSV: prediction_results\\country_by_type_topN.csv\n",
      "Saved SVG: prediction_results\\country_by_type_topN_stacked.svg\n",
      "Saved CSV: prediction_results\\top_authors_overall.csv\n",
      "Saved SVG: prediction_results\\top_authors_overall_bar.svg\n",
      "Saved CSV: prediction_results\\ngrams_top_by_type.csv\n",
      "Saved CSV: prediction_results\\ngrams_top_by_discourse.csv\n",
      "Saved CSV: prediction_results\\ngrams_top_by_stance.csv\n",
      "Saved CSV: prediction_results\\ngrams_top_by_sentiment.csv\n",
      "Saved CSV: prediction_results\\summary_label_counts_shares.csv\n",
      "Saved CSV: prediction_results\\sample_rows_for_appendix_head200.csv\n",
      "\n",
      "Artifacts written to: F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\n",
      "Files:\n",
      " - country_by_type_topN.csv\n",
      " - country_by_type_topN_stacked.svg\n",
      " - crosstab_stance_by_sentiment.csv\n",
      " - crosstab_type_by_discourse.csv\n",
      " - crosstab_type_by_stance.csv\n",
      " - discourse_label_counts.csv\n",
      " - discourse_label_distribution_bar.svg\n",
      " - discourse_label_proportions.csv\n",
      " - discourse_score_hist.svg\n",
      " - exemplars_top10_per_label_discourse.csv\n",
      " - exemplars_top10_per_label_sentiment.csv\n",
      " - exemplars_top10_per_label_stance.csv\n",
      " - exemplars_top10_per_label_type.csv\n",
      " - heatmap_stance_by_sentiment.svg\n",
      " - heatmap_type_by_discourse.svg\n",
      " - heatmap_type_by_stance.svg\n",
      " - low_confidence_discourse_thr_60.csv\n",
      " - low_confidence_sentiment_thr_60.csv\n",
      " - low_confidence_stance_thr_60.csv\n",
      " - low_confidence_summary.csv\n",
      " - low_confidence_type_thr_60.csv\n",
      " - ngrams_top_by_discourse.csv\n",
      " - ngrams_top_by_sentiment.csv\n",
      " - ngrams_top_by_stance.csv\n",
      " - ngrams_top_by_type.csv\n",
      " - sample_rows_for_appendix_head200.csv\n",
      " - sentiment_label_counts.csv\n",
      " - sentiment_label_distribution_bar.svg\n",
      " - sentiment_label_proportions.csv\n",
      " - sentiment_score_hist.svg\n",
      " - stance_label_counts.csv\n",
      " - stance_label_distribution_bar.svg\n",
      " - stance_label_proportions.csv\n",
      " - stance_score_hist.svg\n",
      " - summary_label_counts_shares.csv\n",
      " - top_authors_overall.csv\n",
      " - top_authors_overall_bar.svg\n",
      " - top_countries_overall.csv\n",
      " - top_countries_overall_bar.svg\n",
      " - type_label_counts.csv\n",
      " - type_label_distribution_bar.svg\n",
      " - type_label_proportions.csv\n",
      " - type_score_hist.svg\n",
      " - yearly_counts_by_discourse.csv\n",
      " - yearly_counts_by_discourse_stacked.svg\n",
      " - yearly_counts_by_type.csv\n",
      " - yearly_counts_by_type_stacked.svg\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CBDC Predictions: Analysis & Exports\n",
    "# =========================\n",
    "# Requirements: pandas, numpy, matplotlib, scikit-learn (for n-grams)\n",
    "# pip install -U pandas numpy matplotlib scikit-learn\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "INPUT_FILE   = \"cbdc-dataset-predictions.csv\" \n",
    "OUTPUT_DIR   = Path(\"prediction_results\")\n",
    "SENT_COL     = \"cbdc_sentence\"\n",
    "DATE_COL     = \"date\"\n",
    "COUNTRY_COL  = \"country\"\n",
    "AUTHOR_COL   = \"author\"\n",
    "AFFIL_COL    = \"affiliation\"\n",
    "TOP_N        = 20             # top items in bar charts / tables\n",
    "LOW_CONF_THR = 0.60           # flag low-confidence predictions\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def save_csv(df, name):\n",
    "    path = OUTPUT_DIR / f\"{name}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved CSV: {path}\")\n",
    "\n",
    "def save_fig(fig, name):\n",
    "    path = OUTPUT_DIR / f\"{name}.svg\"\n",
    "    fig.savefig(path, format=\"svg\", bbox_inches=\"tight\", dpi=150)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved SVG: {path}\")\n",
    "\n",
    "def parse_dates(series):\n",
    "    # robust parse without deprecated infer_datetime_format\n",
    "    d1 = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    mask = d1.isna()\n",
    "    if mask.any():\n",
    "        d2 = pd.to_datetime(series[mask], errors=\"coerce\", dayfirst=True)\n",
    "        d1.loc[mask] = d2\n",
    "    return d1\n",
    "\n",
    "def value_counts_df(series, normalize=False, top=None):\n",
    "    \"\"\"\n",
    "    Return value counts as a DataFrame with consistent column names.\n",
    "    \"\"\"\n",
    "    vc = series.value_counts(normalize=normalize, dropna=False)\n",
    "    if top:\n",
    "        vc = vc.head(top)\n",
    "    colname = \"count\" if not normalize else \"proportion\"\n",
    "    return vc.rename_axis(series.name or \"value\").reset_index(name=colname)\n",
    "\n",
    "def barplot_counts(series, title, fname, top=TOP_N):\n",
    "    counts = series.value_counts().head(top)\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    counts.sort_values().plot(kind=\"barh\", ax=ax)\n",
    "    ax.set_xlabel(\"Count\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_title(title)\n",
    "    for i, v in enumerate(counts.sort_values().values):\n",
    "        ax.text(v, i, f\" {v}\", va=\"center\")\n",
    "    save_fig(fig, fname)\n",
    "\n",
    "def heatmap_table(ct, title, fname):\n",
    "    fig, ax = plt.subplots(figsize=(1.1*ct.shape[1]+4, 1.1*ct.shape[0]+3))\n",
    "    im = ax.imshow(ct.values, aspect=\"auto\")\n",
    "    ax.set_xticks(range(ct.shape[1])); ax.set_xticklabels(ct.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(ct.shape[0])); ax.set_yticklabels(ct.index)\n",
    "    ax.set_title(title)\n",
    "    for i in range(ct.shape[0]):\n",
    "        for j in range(ct.shape[1]):\n",
    "            ax.text(j, i, f\"{ct.values[i, j]:,.0f}\", ha=\"center\", va=\"center\", fontsize=9, color=\"white\" if im.cmap(im.norm(ct.values[i,j]))[0:3] < (0.5,0.5,0.5) else \"black\")\n",
    "    fig.colorbar(im, ax=ax, label=\"Count\")\n",
    "    save_fig(fig, fname)\n",
    "\n",
    "def score_hist(scores, title, fname, bins=25):\n",
    "    fig, ax = plt.subplots(figsize=(7,4))\n",
    "    ax.hist(scores.dropna(), bins=bins)\n",
    "    ax.set_xlabel(\"Confidence score\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(title)\n",
    "    save_fig(fig, fname)\n",
    "\n",
    "def stacked_bar(pivot, title, fname):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    pivot.plot(kind=\"bar\", stacked=True, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(pivot.index.name or \"\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend(title=pivot.columns.name or \"\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    save_fig(fig, fname)\n",
    "\n",
    "def top_ngrams_per_label(df, text_col, label_col, ngram_range=(1,2), top_k=20):\n",
    "    # use a domain stoplist to avoid trivial tokens\n",
    "    stop_words = set(CountVectorizer(stop_words=\"english\").get_stop_words() or [])\n",
    "    stop_words |= {\"cbdc\",\"cbdcs\",\"central\",\"bank\",\"banks\",\"digital\",\"currency\",\"currencies\",\n",
    "                   \"centralbank\",\"central-bank\",\"centralbanks\",\"euro\",\"dollar\",\"banking\"}\n",
    "    rows = []\n",
    "    for label, d in df.groupby(label_col):\n",
    "        corpus = d[text_col].dropna().astype(str).tolist()\n",
    "        if not corpus:\n",
    "            continue\n",
    "        vec = CountVectorizer(stop_words=list(stop_words), ngram_range=ngram_range, min_df=2)\n",
    "        X = vec.fit_transform(corpus)\n",
    "        sums = np.asarray(X.sum(axis=0)).ravel()\n",
    "        terms = np.array(vec.get_feature_names_out())\n",
    "        order = np.argsort(-sums)[:top_k]\n",
    "        for t, c in zip(terms[order], sums[order]):\n",
    "            rows.append({label_col: label, \"ngram\": t, \"count\": int(c)})\n",
    "    out = pd.DataFrame(rows).sort_values([label_col, \"count\"], ascending=[True, False])\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Load data\n",
    "# -------------------------\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "required_cols = [\n",
    "    SENT_COL,\n",
    "    \"Type_Label\",\"Type_Score\",\n",
    "    \"Stance_Label\",\"Stance_Score\",\n",
    "    \"Sentiment_Label\",\"Sentiment_Score\",\n",
    "    \"Discourse_Label\",\"Discourse_Score\"\n",
    "]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "# Optional metadata\n",
    "for col in [DATE_COL, COUNTRY_COL, AUTHOR_COL, AFFIL_COL]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Parse dates & add time features\n",
    "dates = parse_dates(df[DATE_COL])\n",
    "df[\"year\"]   = dates.dt.year\n",
    "df[\"month\"]  = dates.dt.month\n",
    "df[\"quarter\"] = dates.dt.quarter\n",
    "\n",
    "# Strip label whitespace just in case\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    df[f\"{p}_Label\"] = df[f\"{p}_Label\"].astype(str).str.strip()\n",
    "\n",
    "# -------------------------\n",
    "# 1) Global distributions\n",
    "# -------------------------\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    counts = value_counts_df(df[f\"{p}_Label\"])\n",
    "    props  = value_counts_df(df[f\"{p}_Label\"], normalize=True)\n",
    "    save_csv(counts, f\"{p.lower()}_label_counts\")\n",
    "    save_csv(props,  f\"{p.lower()}_label_proportions\")\n",
    "    barplot_counts(df[f\"{p}_Label\"], f\"{p} — label distribution\", f\"{p.lower()}_label_distribution_bar\")\n",
    "\n",
    "# Score histograms\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    score_hist(df[f\"{p}_Score\"], f\"{p} — score distribution\", f\"{p.lower()}_score_hist\")\n",
    "\n",
    "# Low-confidence triage (per model)\n",
    "low_conf_tables = []\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    low = df[df[f\"{p}_Score\"] < LOW_CONF_THR].copy()\n",
    "    low = low[[SENT_COL, f\"{p}_Label\", f\"{p}_Score\", \"url\", \"title\", DATE_COL, COUNTRY_COL, AUTHOR_COL]]\n",
    "    save_csv(low, f\"low_confidence_{p.lower()}_thr_{int(LOW_CONF_THR*100)}\")\n",
    "    low_conf_tables.append({\"model\": p, \"low_conf_count\": len(low), \"share\": len(low)/len(df)})\n",
    "\n",
    "save_csv(pd.DataFrame(low_conf_tables), \"low_confidence_summary\")\n",
    "\n",
    "# High-confidence exemplars per class (top 10 each)\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    exemplars = []\n",
    "    for label in df[f\"{p}_Label\"].dropna().unique():\n",
    "        block = (\n",
    "            df[df[f\"{p}_Label\"] == label]\n",
    "            .sort_values(f\"{p}_Score\", ascending=False)\n",
    "            .head(10)\n",
    "            [[SENT_COL, f\"{p}_Label\", f\"{p}_Score\", \"url\", \"title\", DATE_COL, COUNTRY_COL, AUTHOR_COL]]\n",
    "        )\n",
    "        exemplars.append(block.assign(_label_group=label))\n",
    "    if exemplars:\n",
    "        out = pd.concat(exemplars, ignore_index=True)\n",
    "        save_csv(out, f\"exemplars_top10_per_label_{p.lower()}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Cross-model consistency\n",
    "# -------------------------\n",
    "# Type vs Discourse (e.g., which discourse categories dominate Retail vs Wholesale vs General)\n",
    "ct_type_disc = pd.crosstab(df[\"Type_Label\"], df[\"Discourse_Label\"])\n",
    "save_csv(ct_type_disc.reset_index(), \"crosstab_type_by_discourse\")\n",
    "heatmap_table(ct_type_disc, \"Type × Discourse (counts)\", \"heatmap_type_by_discourse\")\n",
    "\n",
    "# Stance vs Sentiment (correlation of stance and tone)\n",
    "ct_stance_sent = pd.crosstab(df[\"Stance_Label\"], df[\"Sentiment_Label\"])\n",
    "save_csv(ct_stance_sent.reset_index(), \"crosstab_stance_by_sentiment\")\n",
    "heatmap_table(ct_stance_sent, \"Stance × Sentiment (counts)\", \"heatmap_stance_by_sentiment\")\n",
    "\n",
    "# Type vs Stance\n",
    "ct_type_stance = pd.crosstab(df[\"Type_Label\"], df[\"Stance_Label\"])\n",
    "save_csv(ct_type_stance.reset_index(), \"crosstab_type_by_stance\")\n",
    "heatmap_table(ct_type_stance, \"Type × Stance (counts)\", \"heatmap_type_by_stance\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Time trends\n",
    "# -------------------------\n",
    "# By year: Type distribution\n",
    "by_year_type = df.pivot_table(index=\"year\", columns=\"Type_Label\", values=SENT_COL, aggfunc=\"count\").fillna(0).astype(int)\n",
    "by_year_type = by_year_type.sort_index()\n",
    "save_csv(by_year_type.reset_index(), \"yearly_counts_by_type\")\n",
    "stacked_bar(by_year_type, \"Yearly counts by Type\", \"yearly_counts_by_type_stacked\")\n",
    "\n",
    "# By year: Discourse distribution\n",
    "by_year_disc = df.pivot_table(index=\"year\", columns=\"Discourse_Label\", values=SENT_COL, aggfunc=\"count\").fillna(0).astype(int)\n",
    "by_year_disc = by_year_disc.sort_index()\n",
    "save_csv(by_year_disc.reset_index(), \"yearly_counts_by_discourse\")\n",
    "stacked_bar(by_year_disc, \"Yearly counts by Discourse\", \"yearly_counts_by_discourse_stacked\")\n",
    "\n",
    "# -------------------------\n",
    "# 4) Geography & authorship\n",
    "# -------------------------\n",
    "# Top countries overall and by Type\n",
    "top_countries = (\n",
    "    df[COUNTRY_COL].dropna().astype(str).str.strip().value_counts().head(TOP_N).rename_axis(COUNTRY_COL).reset_index(name=\"count\")\n",
    ")\n",
    "save_csv(top_countries, \"top_countries_overall\")\n",
    "barplot_counts(df[COUNTRY_COL].dropna().astype(str).str.strip(), \"Top countries (overall)\", \"top_countries_overall_bar\")\n",
    "\n",
    "type_country = (\n",
    "    df.groupby([COUNTRY_COL, \"Type_Label\"], dropna=True)[SENT_COL].count().reset_index(name=\"count\")\n",
    ")\n",
    "# keep only top countries for a readable pivot/plot\n",
    "type_country = type_country[type_country[COUNTRY_COL].isin(top_countries[COUNTRY_COL])]\n",
    "pivot_tc = type_country.pivot(index=COUNTRY_COL, columns=\"Type_Label\", values=\"count\").fillna(0).astype(int)\n",
    "save_csv(pivot_tc.reset_index(), \"country_by_type_topN\")\n",
    "stacked_bar(pivot_tc, \"Country × Type (Top countries)\", \"country_by_type_topN_stacked\")\n",
    "\n",
    "# Top authors\n",
    "if AUTHOR_COL in df.columns:\n",
    "    top_authors = (\n",
    "        df[AUTHOR_COL].dropna().astype(str).str.strip().value_counts().head(TOP_N).rename_axis(AUTHOR_COL).reset_index(name=\"count\")\n",
    "    )\n",
    "    save_csv(top_authors, \"top_authors_overall\")\n",
    "    barplot_counts(df[AUTHOR_COL].dropna().astype(str).str.strip(), \"Top authors (overall)\", \"top_authors_overall_bar\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Language signals (n-grams) per label\n",
    "# -------------------------\n",
    "# Type n-grams\n",
    "type_ngrams = top_ngrams_per_label(df, SENT_COL, \"Type_Label\", ngram_range=(1,2), top_k=25)\n",
    "save_csv(type_ngrams, \"ngrams_top_by_type\")\n",
    "\n",
    "# Discourse n-grams\n",
    "disc_ngrams = top_ngrams_per_label(df, SENT_COL, \"Discourse_Label\", ngram_range=(1,2), top_k=25)\n",
    "save_csv(disc_ngrams, \"ngrams_top_by_discourse\")\n",
    "\n",
    "# Stance n-grams\n",
    "stance_ngrams = top_ngrams_per_label(df, SENT_COL, \"Stance_Label\", ngram_range=(1,2), top_k=25)\n",
    "save_csv(stance_ngrams, \"ngrams_top_by_stance\")\n",
    "\n",
    "# Sentiment n-grams\n",
    "sent_ngrams = top_ngrams_per_label(df, SENT_COL, \"Sentiment_Label\", ngram_range=(1,2), top_k=25)\n",
    "save_csv(sent_ngrams, \"ngrams_top_by_sentiment\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Quick sanity tables for paper appendix\n",
    "# -------------------------\n",
    "summary_rows = []\n",
    "for p in [\"Type\",\"Stance\",\"Sentiment\",\"Discourse\"]:\n",
    "    total = len(df)\n",
    "    counts = df[f\"{p}_Label\"].value_counts()\n",
    "    for label, cnt in counts.items():\n",
    "        summary_rows.append({\"model\": p, \"label\": label, \"count\": int(cnt), \"share\": cnt/total})\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values([\"model\",\"count\"], ascending=[True, False])\n",
    "save_csv(summary_df, \"summary_label_counts_shares\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) List of columns kept for reproducibility\n",
    "# -------------------------\n",
    "keep_cols = [\n",
    "    \"url\",\"title\",DATE_COL,COUNTRY_COL,AUTHOR_COL,AFFIL_COL,\n",
    "    SENT_COL,\n",
    "    \"Type_Label\",\"Type_Score\",\n",
    "    \"Stance_Label\",\"Stance_Score\",\n",
    "    \"Sentiment_Label\",\"Sentiment_Score\",\n",
    "    \"Discourse_Label\",\"Discourse_Score\",\n",
    "    \"year\",\"quarter\",\"month\"\n",
    "]\n",
    "keep_cols = [c for c in keep_cols if c in df.columns]\n",
    "meta_export = df[keep_cols]\n",
    "save_csv(meta_export.head(200), \"sample_rows_for_appendix_head200\")\n",
    "\n",
    "# Done\n",
    "print(\"\\nArtifacts written to:\", OUTPUT_DIR.resolve())\n",
    "print(\"Files:\", *sorted(os.listdir(OUTPUT_DIR)), sep=\"\\n - \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2fe66",
   "metadata": {},
   "source": [
    "# Comprehensive stats for CBDC predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84a11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comprehensive descriptive stats to:\n",
      "  F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\cbdc_descriptive_stats.csv\n",
      "Rows: 2,098\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Comprehensive stats for CBDC predictions\n",
    "# ============================================\n",
    "# Input  : cbdc-dataset-predictions.csv \n",
    "# Output : prediction_results/cbdc_descriptive_stats.csv\n",
    "# --------------------------------------------\n",
    "import os, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Optional metrics (graceful fallback if not installed)\n",
    "try:\n",
    "    from scipy.stats import chi2_contingency, spearmanr\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import mutual_info_score\n",
    "    HAVE_SKLEARN = True\n",
    "except Exception:\n",
    "    HAVE_SKLEARN = False\n",
    "\n",
    "INPUT_FILE  = \"cbdc-dataset-predictions.csv\"\n",
    "OUT_DIR     = Path(\"prediction_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CSV     = OUT_DIR / \"cbdc_descriptive_stats.csv\"\n",
    "\n",
    "# Columns\n",
    "SENT_COL    = \"cbdc_sentence\"\n",
    "DATE_COL    = \"date\"\n",
    "COUNTRY_COL = \"country\"\n",
    "AUTHOR_COL  = \"author\"\n",
    "AFFIL_COL   = \"affiliation\"\n",
    "\n",
    "MODELS = [\n",
    "    (\"Type\",      \"Type_Label\",      \"Type_Score\"),\n",
    "    (\"Stance\",    \"Stance_Label\",    \"Stance_Score\"),\n",
    "    (\"Sentiment\", \"Sentiment_Label\", \"Sentiment_Score\"),\n",
    "    (\"Discourse\", \"Discourse_Label\", \"Discourse_Score\"),\n",
    "]\n",
    "THRESHOLDS = [0.50, 0.60, 0.70, 0.80, 0.90]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def parse_dates(series):\n",
    "    d1 = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    mask = d1.isna()\n",
    "    if mask.any():\n",
    "        d2 = pd.to_datetime(series[mask], errors=\"coerce\", dayfirst=True)\n",
    "        d1.loc[mask] = d2\n",
    "    return d1\n",
    "\n",
    "def entropy(p):\n",
    "    p = np.array(p, dtype=float)\n",
    "    p = p[p > 0]\n",
    "    if p.size == 0: return np.nan\n",
    "    return float(-(p * np.log2(p)).sum())\n",
    "\n",
    "def hhi(p):\n",
    "    p = np.array(p, dtype=float)\n",
    "    return float((p**2).sum())\n",
    "\n",
    "def cramers_v(ct):\n",
    "    if not HAVE_SCIPY: return np.nan\n",
    "    chi2, _, _, _ = chi2_contingency(ct)\n",
    "    n = ct.values.sum()\n",
    "    r, k = ct.shape\n",
    "    denom = min(k-1, r-1)\n",
    "    return math.sqrt((chi2 / n) / denom) if (n > 0 and denom > 0) else np.nan\n",
    "\n",
    "def mutual_info(a, b):\n",
    "    if not HAVE_SKLEARN: return np.nan\n",
    "    return float(mutual_info_score(a, b))\n",
    "\n",
    "def add_row(rows, section, metric, value,\n",
    "            model=None, label=None,\n",
    "            subgroup_type=None, subgroup=None,\n",
    "            n=None, denom=None, notes=None):\n",
    "    rows.append({\n",
    "        \"section\": section,\n",
    "        \"metric\": metric,\n",
    "        \"value\": value,\n",
    "        \"model\": model,\n",
    "        \"label\": label,\n",
    "        \"subgroup_type\": subgroup_type,\n",
    "        \"subgroup\": subgroup,\n",
    "        \"n\": n,\n",
    "        \"denominator\": denom,\n",
    "        \"notes\": notes\n",
    "    })\n",
    "\n",
    "# ---------- load data ----------\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "for col in [SENT_COL]:\n",
    "    assert col in df.columns, f\"Missing required column: {col}\"\n",
    "for col in [DATE_COL, COUNTRY_COL, AUTHOR_COL, AFFIL_COL]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Prepare text/labels/scores\n",
    "for m, lab, sc in MODELS:\n",
    "    if lab in df.columns:\n",
    "        df[lab] = df[lab].astype(str).str.strip()\n",
    "    if sc in df.columns:\n",
    "        df[sc] = pd.to_numeric(df[sc], errors=\"coerce\")\n",
    "\n",
    "# Dates & time-features\n",
    "dates = parse_dates(df[DATE_COL])\n",
    "df[\"year\"]    = dates.dt.year\n",
    "df[\"quarter\"] = dates.dt.quarter\n",
    "df[\"month\"]   = dates.dt.month\n",
    "\n",
    "# Text lengths\n",
    "df[\"char_len\"] = df[SENT_COL].astype(str).str.len()\n",
    "df[\"word_len\"] = df[SENT_COL].astype(str).str.split().str.len()\n",
    "\n",
    "# Duplicates on sentence text\n",
    "df[\"_dup_flag\"] = df[SENT_COL].duplicated(keep=False)\n",
    "df[\"_dup_exact\"] = df[SENT_COL].duplicated(keep=\"first\")\n",
    "\n",
    "# ---------- compile stats ----------\n",
    "rows = []\n",
    "\n",
    "# Dataset-level\n",
    "N = len(df)\n",
    "add_row(rows, \"dataset\", \"rows_total\", N)\n",
    "add_row(rows, \"dataset\", \"unique_sentences\", int(df[SENT_COL].nunique(dropna=True)), n=N)\n",
    "add_row(rows, \"dataset\", \"duplicate_sentence_rows\", int(df[\"_dup_flag\"].sum()), n=N)\n",
    "add_row(rows, \"dataset\", \"unique_urls\", int(df[\"url\"].nunique(dropna=True)) if \"url\" in df.columns else np.nan)\n",
    "add_row(rows, \"dataset\", \"unique_authors\", int(df[AUTHOR_COL].nunique(dropna=True)))\n",
    "add_row(rows, \"dataset\", \"unique_countries\", int(df[COUNTRY_COL].nunique(dropna=True)))\n",
    "add_row(rows, \"dataset\", \"date_min\", str(pd.to_datetime(dates).min()))\n",
    "add_row(rows, \"dataset\", \"date_max\", str(pd.to_datetime(dates).max()))\n",
    "add_row(rows, \"text\", \"char_len_mean\", float(df[\"char_len\"].mean()), n=N)\n",
    "add_row(rows, \"text\", \"char_len_median\", float(df[\"char_len\"].median()), n=N)\n",
    "add_row(rows, \"text\", \"char_len_std\", float(df[\"char_len\"].std(ddof=1)), n=N)\n",
    "add_row(rows, \"text\", \"word_len_mean\", float(df[\"word_len\"].mean()), n=N)\n",
    "add_row(rows, \"text\", \"word_len_median\", float(df[\"word_len\"].median()), n=N)\n",
    "add_row(rows, \"text\", \"word_len_std\", float(df[\"word_len\"].std(ddof=1)), n=N)\n",
    "\n",
    "# Per-model: label distributions, score stats, uncertainty, length effects\n",
    "for m, lab, sc in MODELS:\n",
    "    if lab not in df.columns: \n",
    "        add_row(rows, \"warning\", f\"missing_{m}_columns\", 1, model=m, notes=f\"{lab} not found\")\n",
    "        continue\n",
    "\n",
    "    # label counts/shares\n",
    "    lab_counts = df[lab].value_counts(dropna=False)\n",
    "    lab_shares = lab_counts / lab_counts.sum()\n",
    "    for lbl, cnt in lab_counts.items():\n",
    "        add_row(rows, \"labels\", \"count\", int(cnt), model=m, label=str(lbl), n=N)\n",
    "        add_row(rows, \"labels\", \"share\", float(lab_shares[lbl]), model=m, label=str(lbl), n=int(cnt), denom=N)\n",
    "\n",
    "    # concentration/entropy\n",
    "    p = lab_shares.values\n",
    "    add_row(rows, \"labels\", \"entropy_bits\", entropy(p), model=m)\n",
    "    add_row(rows, \"labels\", \"hhi\", hhi(p), model=m)\n",
    "    add_row(rows, \"labels\", \"num_classes\", int(lab_counts.shape[0]), model=m)\n",
    "    add_row(rows, \"labels\", \"majority_label\", lab_counts.idxmax(), model=m)\n",
    "    add_row(rows, \"labels\", \"majority_share\", float(lab_shares.max()), model=m)\n",
    "\n",
    "    # score summary overall\n",
    "    if sc in df.columns:\n",
    "        s = df[sc].dropna()\n",
    "        if len(s):\n",
    "            q = s.quantile([0.1,0.25,0.5,0.75,0.9])\n",
    "            add_row(rows, \"scores\", \"mean\", float(s.mean()), model=m, n=len(s))\n",
    "            add_row(rows, \"scores\", \"std\", float(s.std(ddof=1)), model=m, n=len(s))\n",
    "            add_row(rows, \"scores\", \"min\", float(s.min()), model=m)\n",
    "            add_row(rows, \"scores\", \"p10\", float(q.loc[0.10]), model=m)\n",
    "            add_row(rows, \"scores\", \"p25\", float(q.loc[0.25]), model=m)\n",
    "            add_row(rows, \"scores\", \"median\", float(q.loc[0.50]), model=m)\n",
    "            add_row(rows, \"scores\", \"p75\", float(q.loc[0.75]), model=m)\n",
    "            add_row(rows, \"scores\", \"p90\", float(q.loc[0.90]), model=m)\n",
    "            add_row(rows, \"scores\", \"max\", float(s.max()), model=m)\n",
    "\n",
    "            # score by label\n",
    "            for lbl in lab_counts.index:\n",
    "                ss = df.loc[df[lab] == lbl, sc].dropna()\n",
    "                if len(ss):\n",
    "                    add_row(rows, \"scores_by_label\", \"mean\", float(ss.mean()), model=m, label=str(lbl), n=len(ss))\n",
    "                    add_row(rows, \"scores_by_label\", \"median\", float(ss.median()), model=m, label=str(lbl), n=len(ss))\n",
    "                    add_row(rows, \"scores_by_label\", \"std\", float(ss.std(ddof=1)), model=m, label=str(lbl), n=len(ss))\n",
    "\n",
    "            # uncertainty buckets\n",
    "            for thr in THRESHOLDS:\n",
    "                low = (df[sc] < thr).sum()\n",
    "                add_row(rows, \"uncertainty\", f\"low_conf_count@{thr:.2f}\", int(low), model=m, n=N)\n",
    "                add_row(rows, \"uncertainty\", f\"low_conf_share@{thr:.2f}\", float(low / N), model=m, n=low, denom=N)\n",
    "\n",
    "            # length vs score\n",
    "            if HAVE_SCIPY:\n",
    "                rho, pval = spearmanr(df[\"word_len\"], df[sc], nan_policy=\"omit\")\n",
    "                add_row(rows, \"scores_vs_length\", \"spearman_rho_wordlen\", float(rho), model=m, n=int(df[[sc,\"word_len\"]].dropna().shape[0]), notes=f\"p={pval:.3g}\")\n",
    "                rho, pval = spearmanr(df[\"char_len\"], df[sc], nan_policy=\"omit\")\n",
    "                add_row(rows, \"scores_vs_length\", \"spearman_rho_charlen\", float(rho), model=m, n=int(df[[sc,\"char_len\"]].dropna().shape[0]), notes=f\"p={pval:.3g}\")\n",
    "\n",
    "    # time: by year label counts & shares\n",
    "    for y, g in df.groupby(\"year\", dropna=True):\n",
    "        if pd.isna(y): continue\n",
    "        yy = int(y)\n",
    "        total_y = g.shape[0]\n",
    "        c = g[lab].value_counts()\n",
    "        for lbl, cnt in c.items():\n",
    "            add_row(rows, \"time_year\", \"count\", int(cnt), model=m, label=str(lbl), subgroup_type=\"year\", subgroup=yy, n=total_y)\n",
    "            add_row(rows, \"time_year\", \"share\", float(cnt/total_y), model=m, label=str(lbl), subgroup_type=\"year\", subgroup=yy, n=int(cnt), denom=total_y)\n",
    "\n",
    "    # YoY change in shares (per label)\n",
    "    shares_by_year = (\n",
    "        df.groupby(\"year\")[lab]\n",
    "          .value_counts(normalize=True)\n",
    "          .rename(\"share\").reset_index()\n",
    "          .pivot(index=\"year\", columns=lab, values=\"share\")\n",
    "          .sort_index()\n",
    "    )\n",
    "    if shares_by_year.shape[0] >= 2:\n",
    "        yoy = shares_by_year.diff().dropna()\n",
    "        for yy, row in yoy.iterrows():\n",
    "            for lbl, delta in row.dropna().items():\n",
    "                add_row(rows, \"time_year\", \"yoy_share_delta\", float(delta), model=m, label=str(lbl), subgroup_type=\"year\", subgroup=int(yy))\n",
    "\n",
    "# Geography: per-country counts and shares (overall)\n",
    "if COUNTRY_COL in df.columns:\n",
    "    total = df.shape[0]\n",
    "    cn_counts = df[COUNTRY_COL].astype(str).str.strip().value_counts(dropna=True)\n",
    "    for c, cnt in cn_counts.items():\n",
    "        add_row(rows, \"geography\", \"count_overall\", int(cnt), subgroup_type=\"country\", subgroup=str(c), n=total)\n",
    "        add_row(rows, \"geography\", \"share_overall\", float(cnt/total), subgroup_type=\"country\", subgroup=str(c), n=int(cnt), denom=total)\n",
    "\n",
    "    # country × model label shares (within-country)\n",
    "    for m, lab, _ in MODELS:\n",
    "        if lab not in df.columns: continue\n",
    "        grp = df.groupby([COUNTRY_COL, lab], dropna=True)[SENT_COL].count().rename(\"count\").reset_index()\n",
    "        for (c, lbl), sub in grp.groupby([COUNTRY_COL, lab]):\n",
    "            c_total = int(df.loc[df[COUNTRY_COL]==c, SENT_COL].shape[0])\n",
    "            cnt = int(sub[\"count\"].sum())\n",
    "            add_row(rows, \"geography_by_model\", \"count\", cnt, model=m, label=str(lbl), subgroup_type=\"country\", subgroup=str(c), n=c_total)\n",
    "            add_row(rows, \"geography_by_model\", \"within_country_share\", float(cnt / c_total if c_total else np.nan),\n",
    "                    model=m, label=str(lbl), subgroup_type=\"country\", subgroup=str(c), n=cnt, denom=c_total)\n",
    "\n",
    "# Authorship: per-author counts (overall)\n",
    "if AUTHOR_COL in df.columns:\n",
    "    au_counts = df[AUTHOR_COL].astype(str).str.strip().value_counts(dropna=True)\n",
    "    total = df.shape[0]\n",
    "    for a, cnt in au_counts.items():\n",
    "        add_row(rows, \"authorship\", \"count_overall\", int(cnt), subgroup_type=\"author\", subgroup=str(a), n=total)\n",
    "        add_row(rows, \"authorship\", \"share_overall\", float(cnt/total), subgroup_type=\"author\", subgroup=str(a), n=int(cnt), denom=total)\n",
    "\n",
    "# Cross-model crosstabs + association metrics\n",
    "PAIRWISE = [\n",
    "    (\"Type\",\"Discourse\"),\n",
    "    (\"Type\",\"Stance\"),\n",
    "    (\"Stance\",\"Sentiment\"),\n",
    "    (\"Discourse\",\"Sentiment\"),\n",
    "]\n",
    "for A, B in PAIRWISE:\n",
    "    labA = next(l for m,l,s in MODELS if m==A)\n",
    "    labB = next(l for m,l,s in MODELS if m==B)\n",
    "    if labA not in df.columns or labB not in df.columns: continue\n",
    "\n",
    "    ct = pd.crosstab(df[labA], df[labB])\n",
    "    total = ct.values.sum()\n",
    "\n",
    "    # cell counts and shares\n",
    "    for i in ct.index:\n",
    "        for j in ct.columns:\n",
    "            cnt = int(ct.loc[i, j])\n",
    "            add_row(rows, \"cross_model\", \"count\", cnt, model=f\"{A}×{B}\", label=f\"{i} | {j}\", n=total)\n",
    "            add_row(rows, \"cross_model\", \"total_share\", float(cnt/total if total else np.nan), model=f\"{A}×{B}\", label=f\"{i} | {j}\", n=cnt, denom=total)\n",
    "\n",
    "    # association measures\n",
    "    add_row(rows, \"cross_model_assoc\", \"cramers_v\", float(cramers_v(ct)), model=f\"{A}×{B}\")\n",
    "    add_row(rows, \"cross_model_assoc\", \"mutual_info\", float(mutual_info(df[labA], df[labB])), model=f\"{A}×{B}\",\n",
    "            notes=\"sklearn.mutual_info_score\" if HAVE_SKLEARN else \"sklearn not available\")\n",
    "\n",
    "# Save\n",
    "stats_df = pd.DataFrame(rows)\n",
    "stats_df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved comprehensive descriptive stats to:\\n  {OUT_CSV.resolve()}\\nRows: {len(stats_df):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c23c2",
   "metadata": {},
   "source": [
    "# Comprehesive summary stat table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b96b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV table:\n",
      "  F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\summary_descriptive_table.csv\n",
      "Saved LaTeX (booktabs) table:\n",
      "  F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\summary_descriptive_table.tex\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Paper-ready comprehensive descriptive statistics (single table)\n",
    "# ============================================================\n",
    "# Input : cbdc-dataset-predictions.csv (same folder as notebook)\n",
    "# Output: prediction_results/summary_descriptive_table.csv\n",
    "#         prediction_results/summary_descriptive_table.tex\n",
    "# ------------------------------------------------------------\n",
    "import os, math, textwrap\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional: SciPy for Cramér's V; scikit-learn for Mutual Information\n",
    "try:\n",
    "    from scipy.stats import chi2_contingency\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import mutual_info_score\n",
    "    HAVE_SKLEARN = True\n",
    "except Exception:\n",
    "    HAVE_SKLEARN = False\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "INPUT_FILE  = \"cbdc-dataset-predictions.csv\"\n",
    "OUT_DIR     = Path(\"prediction_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CSV_OUT     = OUT_DIR / \"summary_descriptive_table.csv\"\n",
    "TEX_OUT     = OUT_DIR / \"summary_descriptive_table.tex\"\n",
    "\n",
    "SENT_COL    = \"cbdc_sentence\"\n",
    "DATE_COL    = \"date\"\n",
    "COUNTRY_COL = \"country\"\n",
    "AUTHOR_COL  = \"author\"\n",
    "URL_COL     = \"url\"\n",
    "\n",
    "MODELS = [\n",
    "    (\"Type\",      \"Type_Label\",      \"Type_Score\"),\n",
    "    (\"Stance\",    \"Stance_Label\",    \"Stance_Score\"),\n",
    "    (\"Sentiment\", \"Sentiment_Label\", \"Sentiment_Score\"),\n",
    "    (\"Discourse\", \"Discourse_Label\", \"Discourse_Score\"),\n",
    "]\n",
    "\n",
    "LOW_CONF_THRESHOLDS = [0.60, 0.80, 0.90]   # report shares @ these cutoffs\n",
    "TOP_K_LIST = 10                             # Top-K for countries/authors (for a compact single-cell summary)\n",
    "DECIMALS   = 3                              # numeric formatting for LaTeX\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def parse_dates(series):\n",
    "    d1 = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    mask = d1.isna()\n",
    "    if mask.any():\n",
    "        d2 = pd.to_datetime(series[mask], errors=\"coerce\", dayfirst=True)\n",
    "        d1.loc[mask] = d2\n",
    "    return d1\n",
    "\n",
    "def cramers_v_from_crosstab(ct):\n",
    "    if not HAVE_SCIPY:\n",
    "        return np.nan\n",
    "    chi2, _, _, _ = chi2_contingency(ct)\n",
    "    n = ct.values.sum()\n",
    "    r, k = ct.shape\n",
    "    denom = min(k-1, r-1)\n",
    "    if n == 0 or denom <= 0:\n",
    "        return np.nan\n",
    "    return float(math.sqrt((chi2 / n) / denom))\n",
    "\n",
    "def topk_series_to_cell(s, k=10):\n",
    "    \"\"\"Return 'Name1 (n1); Name2 (n2); ...'\"\"\"\n",
    "    s = s.dropna()\n",
    "    vc = s.astype(str).str.strip().value_counts().head(k)\n",
    "    return \"; \".join([f\"{idx} ({cnt:,d})\" for idx, cnt in vc.items()])\n",
    "\n",
    "def fmt_pct(x, decimals=1):\n",
    "    return f\"{100*x:.{decimals}f}%\"\n",
    "\n",
    "def wrap(s, width=60):\n",
    "    return \"\\n\".join(textwrap.wrap(str(s), width=width)) if isinstance(s, str) else s\n",
    "\n",
    "# -------------------------\n",
    "# Load data\n",
    "# -------------------------\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Ensure required columns exist\n",
    "assert SENT_COL in df.columns, f\"Missing '{SENT_COL}'.\"\n",
    "for _, lab, sc in MODELS:\n",
    "    assert lab in df.columns, f\"Missing '{lab}'.\"\n",
    "    assert sc in df.columns,  f\"Missing '{sc}'.\"\n",
    "\n",
    "# Coerce types / clean\n",
    "for _, lab, sc in MODELS:\n",
    "    df[lab] = df[lab].astype(str).str.strip()\n",
    "    df[sc]  = pd.to_numeric(df[sc], errors=\"coerce\")\n",
    "\n",
    "for col in [COUNTRY_COL, AUTHOR_COL, URL_COL]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Dates & time features\n",
    "dates = parse_dates(df.get(DATE_COL, pd.Series(index=df.index)))\n",
    "df[\"year\"]    = dates.dt.year\n",
    "df[\"quarter\"] = dates.dt.quarter\n",
    "df[\"month\"]   = dates.dt.month\n",
    "\n",
    "# Text lengths\n",
    "df[\"word_len\"] = df[SENT_COL].astype(str).str.split().str.len()\n",
    "df[\"char_len\"] = df[SENT_COL].astype(str).str.len()\n",
    "\n",
    "# Duplicates (sentence text)\n",
    "df[\"_dup_any\"]   = df[SENT_COL].duplicated(keep=False)\n",
    "df[\"_dup_first\"] = df[SENT_COL].duplicated(keep=\"first\")\n",
    "\n",
    "# =========================\n",
    "# Build the single summary table (Panels A–F)\n",
    "# =========================\n",
    "rows = []\n",
    "N = len(df)\n",
    "\n",
    "# ---------- Panel A: Dataset overview ----------\n",
    "date_min, date_max = (pd.to_datetime(dates).min(), pd.to_datetime(dates).max())\n",
    "yrs = df[\"year\"].dropna().astype(int)\n",
    "sent_per_url_mean = df.groupby(URL_COL, dropna=False)[SENT_COL].count().mean() if URL_COL in df.columns else np.nan\n",
    "sent_per_url_median = df.groupby(URL_COL, dropna=False)[SENT_COL].count().median() if URL_COL in df.columns else np.nan\n",
    "\n",
    "panel = \"Panel A. Dataset overview\"\n",
    "rows += [\n",
    "    [panel, \"Observations (rows)\",      N,                       \"\", \"\", \"\"],\n",
    "    [panel, \"Unique sentences\",         df[SENT_COL].nunique(),  \"\", \"\", \"\"],\n",
    "    [panel, \"Duplicate sentence rows\",  int(df[\"_dup_any\"].sum()), fmt_pct(df[\"_dup_any\"].mean()), \"\", \"\"],\n",
    "    [panel, \"Unique URLs\",              df[URL_COL].nunique(),   \"\", \"\", \"\"],\n",
    "    [panel, \"Unique authors\",           df[AUTHOR_COL].nunique(), \"\", \"\", \"\"],\n",
    "    [panel, \"Unique countries\",         df[COUNTRY_COL].nunique(), \"\", \"\", \"\"],\n",
    "    [panel, \"Date range\",               f\"{date_min.date() if pd.notna(date_min) else 'NA'} — {date_max.date() if pd.notna(date_max) else 'NA'}\", \"\", \"\", \"\"],\n",
    "    [panel, \"Years covered (distinct)\", int(yrs.nunique()) if len(yrs) else \"\", \"\", \"\", \"\"],\n",
    "    [panel, \"Median year [IQR]\",        f\"{int(yrs.median()) if len(yrs) else 'NA'} [{int(yrs.quantile(.25)) if len(yrs) else 'NA'}–{int(yrs.quantile(.75)) if len(yrs) else 'NA'}]\", \"\", \"\", \"\"],\n",
    "    [panel, \"Sentences per URL (mean)\", f\"{sent_per_url_mean:,.2f}\" if pd.notna(sent_per_url_mean) else \"NA\", \"\", \"\", \"\"],\n",
    "    [panel, \"Sentences per URL (median)\", f\"{sent_per_url_median:,.0f}\" if pd.notna(sent_per_url_median) else \"NA\", \"\", \"\", \"\"],\n",
    "    [panel, \"Word length (mean / median / sd)\", f\"{df['word_len'].mean():.1f} / {df['word_len'].median():.0f} / {df['word_len'].std(ddof=1):.1f}\", \"\", \"\", \"\"],\n",
    "    [panel, \"Char length (mean / median / sd)\", f\"{df['char_len'].mean():.1f} / {df['char_len'].median():.0f} / {df['char_len'].std(ddof=1):.1f}\", \"\", \"\", \"\"],\n",
    "]\n",
    "\n",
    "# ---------- Panel B: Label distributions (counts and shares) ----------\n",
    "panel = \"Panel B. Label distributions (counts; shares)\"\n",
    "for m, lab, _ in MODELS:\n",
    "    counts = df[lab].value_counts(dropna=False)\n",
    "    shares = counts / counts.sum()\n",
    "    # entropy / concentration\n",
    "    p = shares.values\n",
    "    entropy_bits = float(-(p[p>0] * np.log2(p[p>0])).sum())\n",
    "    hhi = float((p**2).sum())\n",
    "    rows.append([panel, f\"{m}: classes (K)\", int(counts.shape[0]), \"\", \"\", \"\"])\n",
    "    rows.append([panel, f\"{m}: entropy (bits)\", round(entropy_bits, DECIMALS), \"\", \"\", \"\"])\n",
    "    rows.append([panel, f\"{m}: HHI\", round(hhi, DECIMALS), \"\", \"\", \"\"])\n",
    "    for lbl, cnt in counts.items():\n",
    "        rows.append([panel, f\"{m}: {lbl}\", int(cnt), fmt_pct(shares[lbl]), \"\", \"\"])\n",
    "\n",
    "# ---------- Panel C: Score statistics & low-confidence shares ----------\n",
    "panel = \"Panel C. Score statistics & low-confidence\"\n",
    "for m, lab, sc in MODELS:\n",
    "    s = df[sc].dropna()\n",
    "    if len(s) == 0:\n",
    "        rows.append([panel, f\"{m}: score stats (no data)\", \"\", \"\", \"\", \"\"])\n",
    "        continue\n",
    "    q = s.quantile([.10,.25,.50,.75,.90])\n",
    "    rows += [\n",
    "        [panel, f\"{m}: mean / sd\", f\"{s.mean():.{DECIMALS}f} / {s.std(ddof=1):.{DECIMALS}f}\", \"\", \"\", \"\"],\n",
    "        [panel, f\"{m}: min / p10 / median / p90 / max\",\n",
    "         f\"{s.min():.{DECIMALS}f} / {q.loc[.10]:.{DECIMALS}f} / {q.loc[.50]:.{DECIMALS}f} / {q.loc[.90]:.{DECIMALS}f} / {s.max():.{DECIMALS}f}\",\n",
    "         \"\", \"\", \"\"],\n",
    "    ]\n",
    "    # by label (mean ± sd)\n",
    "    for lbl in df[lab].value_counts().index:\n",
    "        ss = df.loc[df[lab]==lbl, sc].dropna()\n",
    "        if len(ss):\n",
    "            rows.append([panel, f\"{m}: score by {lbl} (mean ± sd)\", f\"{ss.mean():.{DECIMALS}f} ± {ss.std(ddof=1):.{DECIMALS}f}\", \"\", \"\", \"\"])\n",
    "    # low-confidence shares\n",
    "    for thr in LOW_CONF_THRESHOLDS:\n",
    "        share_low = float((df[sc] < thr).mean())\n",
    "        rows.append([panel, f\"{m}: share(score < {thr:.2f})\", fmt_pct(share_low, 1), \"\", \"\", \"\"])\n",
    "\n",
    "# ---------- Panel D: Time coverage summaries ----------\n",
    "panel = \"Panel D. Time coverage\"\n",
    "if len(yrs):\n",
    "    share_2020s = float((yrs >= 2020).mean())\n",
    "    share_2010s = float(((yrs >= 2010) & (yrs <= 2019)).mean())\n",
    "    share_pre2010 = float((yrs < 2010).mean())\n",
    "    rows += [\n",
    "        [panel, \"Share of observations in 2020s\", fmt_pct(share_2020s, 1), \"\", \"\", \"\"],\n",
    "        [panel, \"Share of observations in 2010s\", fmt_pct(share_2010s, 1), \"\", \"\", \"\"],\n",
    "        [panel, \"Share of observations pre-2010\", fmt_pct(share_pre2010, 1), \"\", \"\", \"\"],\n",
    "    ]\n",
    "# most recent year & share\n",
    "if len(yrs):\n",
    "    y_max = int(yrs.max())\n",
    "    share_max_year = float((yrs == y_max).mean())\n",
    "    rows.append([panel, f\"Most recent year in data\", y_max, \"\", \"\", \"\"])\n",
    "    rows.append([panel, f\"Share of observations in {y_max}\", fmt_pct(share_max_year, 1), \"\", \"\", \"\"])\n",
    "\n",
    "# ---------- Panel E: Cross-model association metrics ----------\n",
    "panel = \"Panel E. Cross-model associations\"\n",
    "pairs = [(\"Type\",\"Discourse\"), (\"Type\",\"Stance\"), (\"Stance\",\"Sentiment\"), (\"Discourse\",\"Sentiment\")]\n",
    "for A, B in pairs:\n",
    "    labA = next(l for m,l,s in MODELS if m==A)\n",
    "    labB = next(l for m,l,s in MODELS if m==B)\n",
    "    ct = pd.crosstab(df[labA], df[labB])\n",
    "    cv = cramers_v_from_crosstab(ct)\n",
    "    rows.append([panel, f\"Cramér’s V: {A} × {B}\", round(cv, DECIMALS) if pd.notna(cv) else \"NA\", \"\", \"\", \"\"])\n",
    "    if HAVE_SKLEARN:\n",
    "        mi = mutual_info_score(df[labA], df[labB])  # nats\n",
    "        rows.append([panel, f\"Mutual information (nats): {A} × {B}\", round(float(mi), DECIMALS), \"\", \"\", \"\"])\n",
    "\n",
    "# Score-score Spearman correlations (pairwise across models)\n",
    "score_cols = {m: sc for m,_,sc in MODELS}\n",
    "scores_df = df[[c for c in score_cols.values() if c in df.columns]].copy()\n",
    "if scores_df.shape[1] >= 2:\n",
    "    corr = scores_df.corr(method=\"spearman\", min_periods=1)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[1]):\n",
    "            rows.append([panel,\n",
    "                         f\"Spearman ρ (scores): {list(score_cols.keys())[i]} × {list(score_cols.keys())[j]}\",\n",
    "                         round(float(corr.iloc[i,j]), DECIMALS) if pd.notna(corr.iloc[i,j]) else \"NA\",\n",
    "                         \"\", \"\", \"\"])\n",
    "\n",
    "# ---------- Panel F: Countries & Authors (Top-10, compact) ----------\n",
    "panel = \"Panel F. Key entities (Top-10 compact)\"\n",
    "rows.append([panel, \"Top-10 countries (count)\", topk_series_to_cell(df[COUNTRY_COL], TOP_K_LIST), \"\", \"\", \"\"])\n",
    "rows.append([panel, \"Top-10 authors (count)\",  topk_series_to_cell(df[AUTHOR_COL], TOP_K_LIST), \"\", \"\", \"\"])\n",
    "\n",
    "# ---------- Panel G: Missingness (key columns) ----------\n",
    "panel = \"Panel G. Missingness\"\n",
    "key_cols = [DATE_COL, COUNTRY_COL, AUTHOR_COL, URL_COL, SENT_COL] + [lab for _,lab,_ in MODELS] + [sc for *_,sc in MODELS]\n",
    "for c in key_cols:\n",
    "    if c in df.columns:\n",
    "        miss_share = float(df[c].isna().mean())\n",
    "        rows.append([panel, f\"Missing share: {c}\", fmt_pct(miss_share, 1), \"\", \"\", \"\"])\n",
    "\n",
    "# -------------------------\n",
    "# Assemble final table\n",
    "# -------------------------\n",
    "table = pd.DataFrame(rows, columns=[\"Panel / Section\", \"Statistic\", \"Value\", \"Count\", \"Share\", \"Notes\"])\n",
    "\n",
    "# For CSV (analysis-friendly): keep raw numeric where we have it; string where appropriate\n",
    "table.to_csv(CSV_OUT, index=False)\n",
    "print(f\"Saved CSV table:\\n  {CSV_OUT.resolve()}\")\n",
    "\n",
    "# For LaTeX (paper-ready): light formatting\n",
    "#  - Wrap long cells, apply thousands separators where applicable\n",
    "latex_df = table.copy()\n",
    "\n",
    "# Apply gentle wrapping to extremely long “Value” cells (Top-10 lists)\n",
    "latex_df[\"Value\"] = latex_df[\"Value\"].apply(lambda s: wrap(s, width=80))\n",
    "\n",
    "# Build LaTeX with booktabs\n",
    "latex_str = latex_df.to_latex(\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    longtable=True,\n",
    "    bold_rows=False,\n",
    "    na_rep=\"\",\n",
    "    caption=\"Comprehensive Descriptive Statistics for CBDC Sentence Predictions.\",\n",
    "    label=\"tab:descriptive_stats\",\n",
    "    column_format=\"p{3.2cm}p{7.8cm}p{4.5cm}p{1.6cm}p{1.6cm}p{2.0cm}\"\n",
    ").replace(\"\\\\toprule\", \"\\\\toprule\\\\addlinespace[0.25em]\") \\\n",
    " .replace(\"\\\\bottomrule\", \"\\\\addlinespace[0.25em]\\\\bottomrule\")\n",
    "\n",
    "with open(TEX_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_str)\n",
    "\n",
    "print(f\"Saved LaTeX (booktabs) table:\\n  {TEX_OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42b860",
   "metadata": {},
   "source": [
    "# Additional Heatmaps: Discourse × Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e21bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\heatmap_discourse_by_sentiment_counts.svg\n",
      " - F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\heatmap_discourse_by_sentiment_rowshare.svg\n",
      " - F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\\heatmap_discourse_by_sentiment_colshare.svg\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Heatmaps: Discourse × Sentiment (counts & shares)\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"prediction_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----- configurable order to match your other figures -----\n",
    "ORDER_DISCOURSE = [\"Feature\", \"Process\", \"Risk-Benefit\"]\n",
    "ORDER_SENTIMENT = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# ----- build crosstab with fixed ordering -----\n",
    "disc = df[\"Discourse_Label\"].astype(str).str.strip()\n",
    "sent = df[\"Sentiment_Label\"].astype(str).str.strip()\n",
    "\n",
    "ct = pd.crosstab(disc, sent).reindex(index=ORDER_DISCOURSE, columns=ORDER_SENTIMENT, fill_value=0)\n",
    "\n",
    "# ----- small helper for consistent look -----\n",
    "def plot_heatmap(mat, title, fname, fmt=\"d\", cbar_label=\"Count\"):\n",
    "    fig, ax = plt.subplots(figsize=(6.2, 5.2))  # sized to pair nicely in Word two-column layout\n",
    "    im = ax.imshow(mat.values, aspect=\"auto\")\n",
    "    ax.set_xticks(range(mat.shape[1])); ax.set_xticklabels(mat.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(mat.shape[0])); ax.set_yticklabels(mat.index)\n",
    "    ax.set_title(title)\n",
    "    # annotate\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(mat.shape[1]):\n",
    "            val = mat.iat[i, j]\n",
    "            text = f\"{val:.1f}%\" if fmt == \"%\" else f\"{int(val):,}\"\n",
    "            # choose text color for contrast\n",
    "            r, g, b, _ = im.cmap(im.norm(mat.values[i, j]))\n",
    "            lum = 0.299*r + 0.587*g + 0.114*b\n",
    "            color = \"white\" if lum < 0.5 else \"black\"\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", fontsize=9, color=color)\n",
    "    cb = fig.colorbar(im, ax=ax)\n",
    "    cb.set_label(cbar_label)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUT_DIR / f\"{fname}.svg\", format=\"svg\", bbox_inches=\"tight\", dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ----- (1) Counts heatmap -----\n",
    "plot_heatmap(ct, \"Discourse × Sentiment (counts)\", \"heatmap_discourse_by_sentiment_counts\", fmt=\"d\", cbar_label=\"Count\")\n",
    "\n",
    "# ----- (2) Row-normalized shares: within each Discourse label -----\n",
    "row_share = (ct.div(ct.sum(axis=1), axis=0) * 100).round(1)\n",
    "plot_heatmap(row_share, \"Discourse × Sentiment (row share, % within Discourse)\", \n",
    "             \"heatmap_discourse_by_sentiment_rowshare\", fmt=\"%\", cbar_label=\"Share (%)\")\n",
    "\n",
    "# ----- (3) Column-normalized shares: within each Sentiment level -----\n",
    "col_share = (ct.div(ct.sum(axis=0), axis=1) * 100).round(1)\n",
    "plot_heatmap(col_share, \"Discourse × Sentiment (column share, % within Sentiment)\", \n",
    "             \"heatmap_discourse_by_sentiment_colshare\", fmt=\"%\", cbar_label=\"Share (%)\")\n",
    "\n",
    "print(\"Saved:\",\n",
    "      (OUT_DIR / \"heatmap_discourse_by_sentiment_counts.svg\").resolve(),\n",
    "      (OUT_DIR / \"heatmap_discourse_by_sentiment_rowshare.svg\").resolve(),\n",
    "      (OUT_DIR / \"heatmap_discourse_by_sentiment_colshare.svg\").resolve(),\n",
    "      sep=\"\\n - \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e563c98",
   "metadata": {},
   "source": [
    "# Time-series line graphs for each classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf3ec6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote SVGs and CSVs to: F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Time-series line graphs for each classification (counts & shares)\n",
    "# ============================================\n",
    "# Prereqs: df must contain columns:\n",
    "#   date, Type_Label, Stance_Label, Sentiment_Label, Discourse_Label\n",
    "# And already run parse_dates() earlier in the notebook.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes  # <-- for compact right-side legend\n",
    "\n",
    "# --------------- CONFIG ----------------\n",
    "DATE_COL = \"date\"          # input date column name\n",
    "OUT_DIR  = Path(\"prediction_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose temporal frequency: 'Y' (yearly), 'Q' (quarterly), or 'M' (monthly)\n",
    "FREQ = \"Q\"\n",
    "\n",
    "# Optional smoothing with a centered moving average (for visual stability)\n",
    "ROLLING_WINDOW = {\"M\": 3, \"Q\": 2, \"Y\": 1}[FREQ]   # feel free to change\n",
    "CENTERED = True\n",
    "\n",
    "# Optional label orders (purely for legend consistency; remove a label if not present in your data)\n",
    "LABEL_ORDERS = {\n",
    "    \"Type_Label\":      [\"Retail CBDC\", \"Wholesale CBDC\", \"General/Unspecified\"],\n",
    "    \"Stance_Label\":    [\"Pro-CBDC\", \"Wait-and-See\", \"Anti-CBDC\"],\n",
    "    \"Sentiment_Label\": [\"negative\", \"neutral\", \"positive\"],\n",
    "    \"Discourse_Label\": [\"Feature\", \"Process\", \"Risk-Benefit\"],\n",
    "}\n",
    "\n",
    "# Legend panel width as fraction of axes width (tune if needed)\n",
    "LEGEND_PANEL_FRAC = 0.18\n",
    "\n",
    "# --------------- HELPERS ----------------\n",
    "def parse_dates(series):\n",
    "    d1 = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    mask = d1.isna()\n",
    "    if mask.any():\n",
    "        d2 = pd.to_datetime(series[mask], errors=\"coerce\", dayfirst=True)\n",
    "        d1.loc[mask] = d2\n",
    "    return d1\n",
    "\n",
    "def full_period_index(dmin, dmax, freq):\n",
    "    # Build continuous PeriodIndex from min to max for clean lines\n",
    "    if freq == \"Y\":\n",
    "        start = pd.Period(dmin, \"Y\")\n",
    "        end   = pd.Period(dmax, \"Y\")\n",
    "    elif freq == \"Q\":\n",
    "        start = pd.Period(dmin, \"Q\")\n",
    "        end   = pd.Period(dmax, \"Q\")\n",
    "    else:\n",
    "        start = pd.Period(dmin, \"M\")\n",
    "        end   = pd.Period(dmax, \"M\")\n",
    "    return pd.period_range(start, end, freq=freq)\n",
    "\n",
    "def plot_lines(df_wide, title, ylabel, fname):\n",
    "    # Slightly compact figure; we’ll reserve space on the right for the legend panel\n",
    "    fig, ax = plt.subplots(figsize=(7.2, 4.6))\n",
    "\n",
    "    # Convert PeriodIndex to Timestamp (period end) for prettier x ticks\n",
    "    x = df_wide.index.to_timestamp()\n",
    "\n",
    "    # Plot and keep handles for colors/labels\n",
    "    lines = []\n",
    "    for col in df_wide.columns:\n",
    "        ln, = ax.plot(x, df_wide[col], label=str(col), linewidth=2)\n",
    "        lines.append(ln)\n",
    "\n",
    "    # Axes labels/titles\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel({\"Y\":\"Year\", \"Q\":\"Quarter\", \"M\":\"Month\"}[FREQ])\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # Leave room on the right for a custom vertical legend panel\n",
    "    fig.subplots_adjust(right=1.0 - LEGEND_PANEL_FRAC - 0.02)\n",
    "\n",
    "    # -------- Custom legend panel: vertical, color swatch ABOVE the label --------\n",
    "    # Create an inset axes that serves as the legend box\n",
    "    leg_ax = inset_axes(ax,\n",
    "                        width=f\"{int(LEGEND_PANEL_FRAC*100)}%\", height=\"100%\",\n",
    "                        loc=\"upper right\", bbox_to_anchor=(1.0, 0.0, LEGEND_PANEL_FRAC, 1.0),\n",
    "                        bbox_transform=ax.transAxes, borderpad=0)\n",
    "    leg_ax.set_xlim(0, 1); leg_ax.set_ylim(0, 1); leg_ax.axis(\"off\")\n",
    "\n",
    "    n = len(lines)\n",
    "    top, bottom = 0.94, 0.06\n",
    "    step = 0 if n <= 1 else (top - bottom) / (n - 1)\n",
    "    for i, ln in enumerate(lines):\n",
    "        y = top - i * step\n",
    "        # short colored line (swatch)\n",
    "        leg_ax.plot([0.25, 0.75], [y, y], color=ln.get_color(), linewidth=3)\n",
    "        # label directly below the swatch (centered)\n",
    "        leg_ax.text(0.5, y - 0.06, ln.get_label(), ha=\"center\", va=\"top\", fontsize=9)\n",
    "\n",
    "    # Save\n",
    "    fig.savefig(OUT_DIR / f\"{fname}.svg\", format=\"svg\", bbox_inches=\"tight\", dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "def make_timeseries(df_in, label_col, freq=\"Q\", order=None, rolling_window=1, centered=True):\n",
    "    # Group to counts by period × label\n",
    "    d = df_in.copy()\n",
    "    d[\"_period\"] = d[\"__date\"].dt.to_period(freq)\n",
    "    ct = (\n",
    "        d.groupby([\"_period\", label_col])\n",
    "         .size()\n",
    "         .unstack(fill_value=0)\n",
    "         .sort_index()\n",
    "    )\n",
    "    # Reindex columns to requested order (keep only those present)\n",
    "    if order is not None:\n",
    "        order = [c for c in order if c in ct.columns]\n",
    "        ct = ct.reindex(columns=order)\n",
    "    # Reindex rows to full continuous period range for clean lines\n",
    "    full_idx = full_period_index(d[\"__date\"].min().to_period(freq), d[\"__date\"].max().to_period(freq), freq)\n",
    "    ct = ct.reindex(full_idx, fill_value=0)\n",
    "    # Shares\n",
    "    shares = ct.div(ct.sum(axis=1).replace(0, np.nan), axis=0)\n",
    "    # Optional smoothing\n",
    "    if rolling_window and rolling_window > 1:\n",
    "        ct = ct.rolling(rolling_window, center=centered, min_periods=1).mean()\n",
    "        shares = shares.rolling(rolling_window, center=centered, min_periods=1).mean()\n",
    "    return ct, shares\n",
    "\n",
    "# --------------- MAIN ----------------\n",
    "# Ensure/parse dates\n",
    "df = df.copy()\n",
    "df[\"__date\"] = parse_dates(df[DATE_COL])\n",
    "df = df[df[\"__date\"].notna()].reset_index(drop=True)\n",
    "\n",
    "MODELS = [\n",
    "    (\"Type\",      \"Type_Label\"),\n",
    "    (\"Stance\",    \"Stance_Label\"),\n",
    "    (\"Sentiment\", \"Sentiment_Label\"),\n",
    "    (\"Discourse\", \"Discourse_Label\"),\n",
    "]\n",
    "\n",
    "for model_name, label_col in MODELS:\n",
    "    order = LABEL_ORDERS.get(label_col, None)\n",
    "    counts, shares = make_timeseries(\n",
    "        df_in=df,\n",
    "        label_col=label_col,\n",
    "        freq=FREQ,\n",
    "        order=order,\n",
    "        rolling_window=ROLLING_WINDOW,\n",
    "        centered=CENTERED,\n",
    "    )\n",
    "\n",
    "    # Save underlying data (CSV)\n",
    "    counts.to_csv(OUT_DIR / f\"{model_name.lower()}_{FREQ}_counts_timeseries.csv\")\n",
    "    shares.to_csv(OUT_DIR / f\"{model_name.lower()}_{FREQ}_shares_timeseries.csv\")\n",
    "\n",
    "    # Plot lines with compact vertical legend on the right\n",
    "    ttl = f\"{model_name}: label counts over time ({FREQ.lower()})\"\n",
    "    plot_lines(counts, ttl, \"Count\", f\"{model_name.lower()}_{FREQ}_counts_line\")\n",
    "\n",
    "    ttl = f\"{model_name}: label shares over time ({FREQ.lower()})\"\n",
    "    plot_lines(shares, ttl, \"Share of CBDC sentences\", f\"{model_name.lower()}_{FREQ}_shares_line\")\n",
    "\n",
    "print(\"Wrote SVGs and CSVs to:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e79533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SVGs & CSVs to: F:\\1-Working Papers\\3-In progress\\63-CBDC--BIS\\8-CBDC-Analysis and Results\\prediction_results\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CBDC classification --- time-series\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# ----------------------------- FILES -----------------------------\n",
    "CSV_PATH = \"cbdc-dataset-predictions.csv\"   # <- change if needed\n",
    "OUT_DIR  = Path(\"prediction_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------- TIME WINDOW -----------------------\n",
    "X_MIN = pd.Timestamp(\"2016-01-01\")\n",
    "X_MAX = pd.Timestamp(\"2024-12-31\")\n",
    "\n",
    "# ----------------------------- FREQUENCY / SMOOTHING -------------\n",
    "FREQ = \"Q\"                           # 'Y', 'Q', or 'M'\n",
    "ROLLING_WINDOW = {\"M\": 3, \"Q\": 2, \"Y\": 1}[FREQ]\n",
    "CENTERED = False                     # keep False so smoothing never leaks past 2024\n",
    "\n",
    "# ############## DISTANCE CONTROLS — ADJUST HERE ##################\n",
    "LEGEND_PANEL_FRAC = 0.18             # Legend panel width (fraction of Axes width)\n",
    "LEGEND_GAP_MODE   = \"inch\"           # \"inch\" (precise) or \"frac\" (fraction of Axes)\n",
    "LEGEND_GAP_INCH   = 0.01             # Gap between plot and legend (inches)\n",
    "LEGEND_GAP_FRAC   = 0.02             # If using \"frac\", set the fractional gap here\n",
    "# #################################################################\n",
    "\n",
    "# ----------------------------- LABEL ORDERS / WRAPS --------------\n",
    "LABEL_ORDERS = {\n",
    "    \"Type_Label\":      [\"Retail CBDC\", \"Wholesale CBDC\", \"General/Unspecified\"],\n",
    "    \"Stance_Label\":    [\"Pro-CBDC\", \"Wait-and-See\", \"Anti-CBDC\"],\n",
    "    \"Sentiment_Label\": [\"negative\", \"neutral\", \"positive\"],\n",
    "    \"Discourse_Label\": [\"Feature\", \"Process\", \"Risk-Benefit\"],\n",
    "}\n",
    "TYPE_LEGEND_WRAP = {\n",
    "    \"Retail CBDC\": \"Retail\\nCBDC\",\n",
    "    \"Wholesale CBDC\": \"Wholesale\\nCBDC\",\n",
    "    \"General/Unspecified\": \"General/\\nUnspecified\",\n",
    "}\n",
    "\n",
    "# ----------------------------- LOAD DATA -------------------------\n",
    "def parse_dates(series):\n",
    "    d1 = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    m = d1.isna()\n",
    "    if m.any():\n",
    "        d2 = pd.to_datetime(series[m], errors=\"coerce\", dayfirst=True)\n",
    "        d1.loc[m] = d2\n",
    "    return d1\n",
    "\n",
    "usecols = [\"date\",\"Type_Label\",\"Stance_Label\",\"Sentiment_Label\",\"Discourse_Label\"]\n",
    "df = pd.read_csv(CSV_PATH, usecols=usecols)\n",
    "df[\"__date\"] = parse_dates(df[\"date\"]).dt.floor(\"D\")\n",
    "df = df[(df[\"__date\"] >= X_MIN) & (df[\"__date\"] <= X_MAX)].reset_index(drop=True)\n",
    "\n",
    "# ----------------------------- HELPERS ---------------------------\n",
    "def period_range_clamped(freq):\n",
    "    return pd.period_range(pd.Period(X_MIN, freq), pd.Period(X_MAX, freq), freq=freq)\n",
    "\n",
    "def make_timeseries(dfin, label_col, freq=\"Q\", order=None, w=1, centered=False):\n",
    "    d = dfin.copy()\n",
    "    d[\"_p\"] = d[\"__date\"].dt.to_period(freq)\n",
    "    ct = d.groupby([\"_p\", label_col]).size().unstack(fill_value=0).sort_index()\n",
    "    if order is not None:\n",
    "        order = [c for c in order if c in ct.columns]\n",
    "        ct = ct.reindex(columns=order)\n",
    "    idx = period_range_clamped(freq)\n",
    "    ct = ct.reindex(idx, fill_value=0)\n",
    "    shares = ct.div(ct.sum(axis=1).replace(0, np.nan), axis=0)\n",
    "    if w and w > 1:\n",
    "        ct = ct.rolling(w, center=centered, min_periods=1).mean()\n",
    "        shares = shares.rolling(w, center=centered, min_periods=1).mean()\n",
    "    return ct, shares\n",
    "\n",
    "def _gap_frac_from_inches(fig, ax, gap_inch: float) -> float:\n",
    "    \"\"\"Convert an absolute gap (inches) into a fraction of the Axes width.\"\"\"\n",
    "    fig_w_in = fig.get_size_inches()[0]\n",
    "    ax_w_frac = ax.get_position().width  # fraction of figure width\n",
    "    ax_w_in = fig_w_in * ax_w_frac\n",
    "    return gap_inch / max(ax_w_in, 1e-9)\n",
    "\n",
    "def plot_lines(df_wide, title, ylabel, fname, legend_labels=None):\n",
    "    fig, ax = plt.subplots(figsize=(7.2, 4.6))\n",
    "\n",
    "    # x-values: end-of-period timestamps (floored to day)\n",
    "    x_all = df_wide.index.asfreq(FREQ, how=\"end\").to_timestamp().floor(\"D\")\n",
    "\n",
    "    # Hard clamp, remove margins, and set explicit year ticks 2016..2024\n",
    "    ax.set_xlim(X_MIN, X_MAX)\n",
    "    ax.set_xbound(X_MIN, X_MAX)\n",
    "    ax.set_xmargin(0.0); ax.margins(x=0.0)\n",
    "    year_ticks = pd.date_range(X_MIN.normalize(), X_MAX.normalize(), freq=\"YS\")\n",
    "    ax.set_xticks(year_ticks)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "\n",
    "    # Mask after smoothing; plot only within [X_MIN, X_MAX]\n",
    "    mask = (x_all >= X_MIN) & (x_all <= X_MAX)\n",
    "    lines = []\n",
    "    for col in df_wide.columns:\n",
    "        y = np.asarray(df_wide[col].values)\n",
    "        ln, = ax.plot(x_all[mask], y[mask], label=str(col), linewidth=2)\n",
    "        lines.append(ln)\n",
    "\n",
    "    # Labels/grid\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel({\"Y\":\"Year\", \"Q\":\"Quarter\", \"M\":\"Month\"}[FREQ])\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # --- compute effective gap (fraction) from chosen mode ---\n",
    "    if LEGEND_GAP_MODE.lower() == \"inch\":\n",
    "        gap_frac = _gap_frac_from_inches(fig, ax, LEGEND_GAP_INCH)\n",
    "    else:\n",
    "        gap_frac = LEGEND_GAP_FRAC\n",
    "\n",
    "    # Leave room for legend panel\n",
    "    fig.subplots_adjust(right=1.0 - (LEGEND_PANEL_FRAC + gap_frac))\n",
    "\n",
    "    # Right-side vertical legend panel (swatch above label)\n",
    "    leg_ax = inset_axes(\n",
    "        ax,\n",
    "        width=f\"{int(LEGEND_PANEL_FRAC*100)}%\", height=\"100%\",\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(1.0 + gap_frac, 0.0, LEGEND_PANEL_FRAC, 1.0),\n",
    "        bbox_transform=ax.transAxes,\n",
    "        borderpad=0,\n",
    "    )\n",
    "    leg_ax.set_xlim(0, 1); leg_ax.set_ylim(0, 1); leg_ax.axis(\"off\")\n",
    "\n",
    "    labels = legend_labels if legend_labels is not None else [ln.get_label() for ln in lines]\n",
    "    n = len(lines); top, bottom = 0.94, 0.06\n",
    "    step = 0 if n <= 1 else (top - bottom) / (n - 1)\n",
    "    for i, (ln, txt) in enumerate(zip(lines, labels)):\n",
    "        y = top - i * step\n",
    "        leg_ax.plot([0.25, 0.75], [y, y], color=ln.get_color(), linewidth=3, solid_capstyle=\"round\")\n",
    "        leg_ax.text(0.50, y - 0.06, txt, ha=\"center\", va=\"top\", fontsize=9)\n",
    "\n",
    "    fig.savefig(OUT_DIR / f\"{fname}.svg\", format=\"svg\", bbox_inches=\"tight\", dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ----------------------------- BUILD & PLOT ----------------------\n",
    "MODELS = [\n",
    "    (\"Type\",      \"Type_Label\"),\n",
    "    (\"Stance\",    \"Stance_Label\"),\n",
    "    (\"Sentiment\", \"Sentiment_Label\"),\n",
    "    (\"Discourse\", \"Discourse_Label\"),\n",
    "]\n",
    "\n",
    "for model_name, label_col in MODELS:\n",
    "    order = LABEL_ORDERS.get(label_col, None)\n",
    "    counts, shares = make_timeseries(df, label_col, freq=FREQ, order=order,\n",
    "                                     w=ROLLING_WINDOW, centered=CENTERED)\n",
    "\n",
    "    # Save the underlying time series\n",
    "    counts.to_csv(OUT_DIR / f\"{model_name.lower()}_{FREQ}_counts_timeseries.csv\")\n",
    "    shares.to_csv(OUT_DIR / f\"{model_name.lower()}_{FREQ}_shares_timeseries.csv\")\n",
    "\n",
    "    # Wrap Type labels in the legend only\n",
    "    legend_map = None\n",
    "    if label_col == \"Type_Label\":\n",
    "        legend_map = [TYPE_LEGEND_WRAP.get(c, c) for c in counts.columns]\n",
    "\n",
    "    ttl = f\"{model_name}: label counts over time ({FREQ.lower()})\"\n",
    "    plot_lines(counts, ttl, \"Count\", f\"{model_name.lower()}_{FREQ}_counts_line\", legend_labels=legend_map)\n",
    "\n",
    "    ttl = f\"{model_name}: label shares over time ({FREQ.lower()})\"\n",
    "    plot_lines(shares, ttl, \"Share of CBDC sentences\", f\"{model_name.lower()}_{FREQ}_shares_line\", legend_labels=legend_map)\n",
    "\n",
    "print(\"Saved SVGs & CSVs to:\", OUT_DIR.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
